![Image](img/Â∞ÅÈù¢.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">bilibili</a> ¬∑
  <a href="https://www.youtube.com/@comfyui-LLM-party">youtube</a> ¬∑
  <a href="how_to_use_nodes_ZH.md">Tutoriel √©crit</a> ¬∑
  <a href="workflow_tutorial/">Tutoriel de workflow</a> ¬∑
  <a href="https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu">Lien Baidu</a> ¬∑
  <a href="img/QÁæ§.jpg">Groupe QQ</a> ¬∑
  <a href="https://discord.gg/hbMQDH7J">Discord</a> ¬∑
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">√Ä propos de nous</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/–†—É—Å—Å–∫–∏–π-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Fran√ßais-d9d9d9"></a> 
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/ÿßŸÑÿπÿ±ÿ®Ÿäÿ©-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Espa√±ol-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Portugu√™s-d9d9d9"></a>
</div>

####

Comfyui_llm_party aspire √† d√©velopper une biblioth√®que compl√®te de n≈ìuds pour la construction de workflows LLM, bas√©e sur l'interface utilisateur extr√™mement simplifi√©e de [comfyui](https://github.com/comfyanonymous/ComfyUI). Cela permettra aux utilisateurs de construire plus facilement et rapidement leurs propres workflows LLM et de les int√©grer de mani√®re plus pratique dans leurs workflows d'images.

## D√©monstration des effets
https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6

## Aper√ßu du projet
ComfyUI LLM Party permet de construire rapidement votre propre assistant AI personnalis√©, allant des appels multi-outils de LLM aux configurations de r√¥le, en passant par la gestion localis√©e des bases de connaissances du secteur gr√¢ce aux vecteurs de mots RAG et GraphRAG. De la cha√Æne d'agents intelligents unique √† la construction de modes d'interaction complexes entre agents intelligents, y compris les modes d'interaction radiaux et circulaires ; des besoins des utilisateurs individuels qui souhaitent int√©grer leurs applications sociales (QQ, Feishu, Discord) √† un flux de travail tout-en-un LLM+TTS+ComfyUI pour les travailleurs des m√©dias en continu ; des premi√®res applications LLM simples pour les √©tudiants ordinaires aux diverses interfaces de r√©glage de param√®tres utilis√©es par les chercheurs, ainsi que l'adaptation des mod√®les. Tout cela peut √™tre d√©couvert lors de ComfyUI LLM Party.

## D√©marrage rapide
1. Ajout de l'outil [searxng](https://github.com/searxng/searxng), qui peut agr√©ger les recherches sur l'ensemble du web. Perplexica repose √©galement sur cet outil d'agr√©gation de recherche, vous pouvez donc configurer un Perplexica lors de votre f√™te. Vous pouvez d√©ployer l'image publique searxng/searxng dans Docker, puis le d√©marrer en utilisant `docker run -d -p 8080:8080 searxng/searxng`, et y acc√©der en utilisant `http://localhost:8080`. Vous pouvez remplir cette URL `http://localhost:8080` dans l'outil searxng de la f√™te, et ensuite vous pouvez utiliser searxng comme un outil pour LLM.
1. Faites glisser les workflows suivants dans votre comfyui, puis utilisez [comfyui-Manager](https://github.com/ltdrdata/ComfyUI-Manager) pour installer les n≈ìuds manquants.
  - Utilisez l'API pour appeler LLM : [start_with_LLM_api](workflow/start_with_LLM_api.json)
  - G√©rez les LLM locaux avec ollama : [start_with_Ollama](workflow/ollama.json)
  - Utilisez des LLM locaux au format distribu√© : [start_with_LLM_local](workflow/start_with_LLM_local.json)
  - Utilisez des LLM locaux au format GGUF : [start_with_LLM_GGUF](workflow/start_with_GGUF.json)
  - Utilisez des VLM locaux au format distribu√© : [start_with_VLM_local](https://github.com/heshengtao/comfyui_LLM_party/blob/main/workflow_tutorial/LLM_Party%20for%20Llama3.2%20-Vision%EF%BC%88%E5%B8%A6%E8%AE%B0%E5%BF%86%EF%BC%89.json) (en test, actuellement uniquement [Llama-3.2-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct) pris en charge)
  - Utilisez des VLM locaux au format GGUF : [start_with_VLM_GGUF](workflow/start_with_llava.json)
2. Si vous utilisez l'API, remplissez votre `base_url` (cela peut √™tre une API relais, assurez-vous qu'elle se termine par `/v1/`) et `api_key` dans le n≈ìud de chargement de l'API LLM. Exemple : `https://api.openai.com/v1/`
3. Si vous utilisez ollama, activez l'option `is_ollama` dans le n≈ìud de chargement de l'API LLM, sans remplir `base_url` et `api_key`.
4. Si vous utilisez un mod√®le local, remplissez le chemin de votre mod√®le dans le n≈ìud de chargement du mod√®le local, par exemple : `E:\model\Llama-3.2-1B-Instruct`. Vous pouvez √©galement remplir l'ID du d√©p√¥t du mod√®le Huggingface dans le n≈ìud de chargement du mod√®le local, par exemple : `lllyasviel/omost-llama-3-8b-4bits`.
5. En raison du seuil d'utilisation √©lev√© de ce projet, m√™me si vous choisissez le d√©marrage rapide, j'esp√®re que vous pourrez lire attentivement la page d'accueil du projet.

## Derni√®res mises √† jour
1. Le n≈ìud de liste de noms de mod√®les automatiques a √©t√© supprim√© et remplac√© par un n≈ìud de chargeur API LLM simplifi√©, qui r√©cup√®re automatiquement la liste des noms de mod√®les √† partir de la configuration de votre fichier config.ini. Il vous suffit de choisir un nom pour charger le mod√®le. De plus, les n≈ìuds de chargeur simple LLM, chargeur simple LLM-GGUF, chargeur simple VLM, chargeur simple VLM-GGUF et chargeur simple LLM lora ont √©t√© mis √† jour. Ils lisent tous automatiquement les chemins des mod√®les √† partir du dossier model dans le dossier party, facilitant ainsi le chargement de divers mod√®les locaux.
1. Les LLM peuvent maintenant charger dynamiquement des lora comme SD et FLUX. Vous pouvez encha√Æner plusieurs lora pour charger plus de lora sur le m√™me LLM. Exemple de flux de travail : [start_with_LLM_LORA](workflow/LLM_lora.json).
2. Ajout de l'outil [searxng](https://github.com/searxng/searxng), qui peut agr√©ger les recherches sur tout le web. Perplexica repose √©galement sur cet outil de recherche agr√©g√©e, ce qui signifie que vous pouvez configurer un Perplexica lors d'une f√™te. Vous pouvez d√©ployer l'image publique searxng/searxng dans Docker, puis utiliser la commande `docker run -d -p 8080:8080 searxng/searxng` pour le d√©marrer, puis utiliser `http://localhost:8080` pour y acc√©der. Vous pouvez entrer l'URL `http://localhost:8080` dans l'outil searxng dans party, et alors searxng peut √™tre utilis√© comme un outil pour LLM.
1. **Mise √† jour majeure !!!** Vous pouvez maintenant encapsuler n'importe quel flux de travail ComfyUI dans un n≈ìud d'outil LLM. Vous pouvez faire en sorte que votre LLM contr√¥le plusieurs flux de travail ComfyUI simultan√©ment. Lorsque vous souhaitez qu'il accomplisse certaines t√¢ches, il peut choisir le flux de travail ComfyUI appropri√© en fonction de votre prompt, accomplir votre t√¢che et vous renvoyer le r√©sultat. Exemple de flux de travail : [comfyui_workflows_tool](workflow/Êää‰ªªÊÑèworkflowÂΩì‰ΩúLLM_tool.json). Les √©tapes sp√©cifiques sont les suivantes :
   - Tout d'abord, connectez l'interface d'entr√©e de texte du flux de travail que vous souhaitez encapsuler en tant qu'outil √† la sortie "user_prompt" du n≈ìud "D√©marrer le flux de travail". C'est l'endroit o√π le prompt est pass√© lorsque le LLM appelle l'outil.
   - Connectez les positions o√π vous souhaitez sortir du texte et des images aux positions d'entr√©e correspondantes du n≈ìud "Terminer le flux de travail".
   - Enregistrez ce flux de travail sous forme d'API (vous devez activer le mode d√©veloppeur dans les param√®tres pour voir ce bouton).
   - Enregistrez ce flux de travail dans le dossier workflow_api de ce projet.
   - Red√©marrez ComfyUI et cr√©ez un flux de travail LLM simple, par exemple : [start_with_LLM_api](workflow/start_with_LLM_api.json).
   - Ajoutez un n≈ìud "Outil de flux de travail" √† ce n≈ìud LLM et connectez-le √† l'entr√©e de l'outil du n≈ìud LLM.
   - Dans le n≈ìud "Outil de flux de travail", √©crivez le nom du fichier de flux de travail que vous souhaitez appeler dans la premi√®re case de saisie, par exemple : draw.json. Vous pouvez √©crire plusieurs noms de fichiers de flux de travail. Dans la deuxi√®me case de saisie, √©crivez la fonction de chaque flux de travail afin que le LLM comprenne comment utiliser ces flux de travail.
   - Ex√©cutez-le pour voir le LLM appeler votre flux de travail encapsul√© et vous renvoyer le r√©sultat. Si le r√©sultat est une image, connectez le n≈ìud "Aper√ßu de l'image" √† la sortie d'image du n≈ìud LLM pour voir l'image g√©n√©r√©e. Attention ! Cette m√©thode appelle un nouveau ComfyUI sur votre port 8190, veuillez ne pas occuper ce port. Un nouveau terminal sera ouvert sur les syst√®mes Windows et Mac, veuillez ne pas le fermer. Le syst√®me Linux utilise le processus screen pour r√©aliser cela, lorsque vous n'avez pas besoin de l'utiliser, fermez ce processus screen, sinon il occupera toujours votre port.

![workflow_tool](img/workflowtool.png)

## Instructions d'utilisation
1. Pour les instructions d'utilisation des n≈ìuds, veuillez consulter : [ÊÄé‰πà‰ΩøÁî®ËäÇÁÇπ](how_to_use_nodes.md)

2. Si vous rencontrez des probl√®mes avec le plugin ou si vous avez d'autres questions, n'h√©sitez pas √† rejoindre le groupe QQ : [931057213](img/QÁæ§.jpg) |discordÔºö[discord](https://discord.gg/hbMQDH7J).
3. Pour le tutoriel sur les flux de travail, veuillez consulter : [Tutoriel sur les flux de travail](workflow_tutorial/), merci pour la contribution de [HuangYuChuh](https://github.com/HuangYuChuh) !

4. Compte pour les fonctionnalit√©s avanc√©es des flux de travail : [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

4. Pour plus de flux de travail, veuillez consulter le dossier [workflow](workflow).

## Tutoriels vid√©o
<a href="https://space.bilibili.com/26978344">
  <img src="img/B.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://www.youtube.com/@comfyui-LLM-party">
  <img src="img/YT.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>

## Support du mod√®le
1. Prise en charge de tous les appels API au format OpenAI (en combinaison avec [oneapi](https://github.com/songquanpeng/one-api), il est possible d'appeler presque toutes les API LLM, ainsi que toutes les API de transit). Pour le choix de base_url, veuillez vous r√©f√©rer √† [config.ini.example](config.ini.example). Actuellement, les API test√©es incluent :
* [openai](https://platform.openai.com/docs/api-reference/chat/create) (Parfaitement compatible avec tous les mod√®les OpenAI, y compris les s√©ries 4o et o1!)
* [ollama](https://github.com/ollama/ollama) (Recommand√©! Si vous appelez localement, il est fortement recommand√© d'utiliser la m√©thode ollama pour h√©berger votre mod√®le local!)
* [Azure OpenAI](https://azure.microsoft.com/zh-cn/products/ai-services/openai-service/)
* [llama.cpp](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#web-server) (Recommand√©! Si vous souhaitez utiliser le mod√®le au format gguf local, vous pouvez utiliser l'API du projet llama.cpp pour acc√©der √† ce projet!)
* [ÈÄö‰πâÂçÉÈóÆ/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [Êô∫Ë∞±Ê∏ÖË®Ä/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [Ë±ÜÂåÖ](https://www.volcengine.com/docs/82379/1263482)

2. Prise en charge des appels API au format Gemini :
* [Gemini](https://aistudio.google.com/app/prompts/new_chat)

3. Compatible avec la plupart des mod√®les locaux dans la biblioth√®que transformer (le type de mod√®le sur le n≈ìud de cha√Æne de mod√®les LLM local a √©t√© chang√© en LLM, VLM-GGUF et LLM-GGUF, correspondant au chargement direct des mod√®les LLM, au chargement des mod√®les VLM et au chargement des mod√®les LLM au format GGUF). Si votre mod√®le LLM au format VLM ou GGUF signale une erreur, veuillez t√©l√©charger la derni√®re version de llama-cpp-python depuis [llama-cpp-python](https://github.com/abetlen/llama-cpp-python/releases). Les mod√®les actuellement test√©s incluent :
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (recommand√© ! Mod√®le de jeu de r√¥le)
* [lllyasviel/omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (recommand√© ! Mod√®le avec des invites riches)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)
* [lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main)
* [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)

4. T√©l√©chargement des mod√®les :
* [Adresse Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), code d'extraction : qyhu

## T√©l√©chargement
Installez en utilisant l'une des m√©thodes suivantes
### M√©thode 1 :
1. Recherchez `comfyui_LLM_party` dans le [gestionnaire ComfyUI](https://github.com/ltdrdata/ComfyUI-Manager) et installez-le d'un seul clic
2. Red√©marrez ComfyUI
### M√©thode deux :
1. Naviguez vers le sous-dossier `custom_nodes` dans le dossier racine de ComfyUI.
2. Clonez ce d√©p√¥t. `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### M√©thode trois :
1. Cliquez sur `CODE` en haut √† droite.
2. Cliquez sur `t√©l√©charger zip`.
3. D√©compressez le fichier zip t√©l√©charg√© dans le sous-dossier `custom_nodes` du dossier racine de ComfyUI.

## D√©ploiement de l'environnement
1. Naviguez vers le dossier du projet `comfyui_LLM_party`.
2. Dans le terminal, saisissez `pip install -r requirements.txt` pour d√©ployer les biblioth√®ques tierces n√©cessaires √† ce projet dans l'environnement de comfyui. Veuillez v√©rifier que vous installez dans l'environnement de comfyui et faites attention aux erreurs `pip` dans le terminal.
3. Si vous utilisez le lanceur comfyui, vous devez entrer dans le terminal `chemin dans la configuration du lanceur\python_embeded\python.exe -m pip install -r requirements.txt` pour proc√©der √† l'installation. Le dossier `python_embeded` est g√©n√©ralement au m√™me niveau que votre dossier `ComfyUI`.
4. Si vous rencontrez des probl√®mes de configuration de l'environnement, vous pouvez essayer d'utiliser les d√©pendances dans `requirements_fixed.txt`.
## Configuration
* Vous pouvez configurer la langue dans le fichier `config.ini`, actuellement seules deux langues sont disponibles : chinois (zh_CN) et anglais (en_US), la langue par d√©faut √©tant celle de votre syst√®me.
* Vous pouvez configurer l'APIKEY en utilisant l'une des m√©thodes suivantes :
### M√©thode 1 :
1. Ouvrez le fichier `config.ini` dans le dossier du projet `comfyui_LLM_party`.
2. Dans `config.ini`, saisissez votre `openai_api_key` et `base_url`.
3. Si vous utilisez le mod√®le ollama, entrez `http://127.0.0.1:11434/v1/` dans `base_url`, `ollama` dans `openai_api_key`, et le nom de votre mod√®le dans `model_name`, par exemple : llama3.
4. Si vous souhaitez utiliser les outils de recherche Google ou Bing, saisissez votre `google_api_key`, `cse_id` ou `bing_api_key` dans `config.ini`.
5. Si vous d√©sirez utiliser des entr√©es d'images pour LLM, il est recommand√© d'utiliser le service d'h√©bergement d'images imgbb, et d'indiquer votre `imgbb_api` dans `config.ini`.
6. Chaque mod√®le peut √™tre configur√© individuellement dans le fichier `config.ini`, vous pouvez vous r√©f√©rer au fichier `config.ini.example` pour vous aider. Une fois que vous avez termin√© la configuration, il vous suffit de saisir `model_name` dans le n≈ìud.

### M√©thode 2 :
1. Ouvrez l'interface comfyui.
2. Cr√©ez un n≈ìud de grand mod√®le de langage (LLM) et saisissez directement votre `openai_api_key` et `base_url` dans le n≈ìud.
3. Si vous utilisez le mod√®le ollama, veuillez utiliser le n≈ìud LLM_api, entrer `http://127.0.0.1:11434/v1/` dans `base_url`, `ollama` dans `api_key`, et le nom de votre mod√®le dans `model_name`, par exemple : llama3.
4. Si vous souhaitez utiliser des entr√©es d'images pour LLM, il est recommand√© d'utiliser le service d'h√©bergement d'images imgbb et d'indiquer votre `imgbb_api_key` dans le n≈ìud.
## Journal des mises √† jour
1. Vous pouvez cliquer avec le bouton droit dans l'interface comfyui, s√©lectionner `llm` dans le menu contextuel, et trouver le n≈ìud de ce projet. [Comment utiliser les n≈ìuds](how_to_use_nodes_ZH.md)
2. Prise en charge de l'int√©gration API ou de l'int√©gration de grands mod√®les locaux. Mise en ≈ìuvre modulaire de la fonctionnalit√© d'appel d'outils. Lors de la saisie de base_url, veuillez entrer une URL se terminant par `/v1/`. Vous pouvez utiliser [ollama](https://github.com/ollama/ollama) pour g√©rer vos mod√®les, puis entrer `http://127.0.0.1:11434/v1/` dans base_url, ollama dans api_key, et le nom de votre mod√®le, par exemple : llama3.
   - Exemple de flux de travail d'int√©gration API : [start_with_LLM_api](workflow/start_with_LLM_api.json)
   - Exemple de flux de travail d'int√©gration de mod√®le local : [start_with_LLM_local](workflow/start_with_LLM_local.json)
   - Exemple de flux de travail d'int√©gration ollama : [ollama](workflow/ollama.json)
3. Int√©gration d'une base de connaissances locale, prise en charge de RAG. Exemple de flux de travail : [Recherche RAG de la base de connaissances.json](workflow/Áü•ËØÜÂ∫ìRAGÊêúÁ¥¢.json)
4. Possibilit√© d'appeler un interpr√©teur de code.
5. Possibilit√© de rechercher en ligne, prise en charge de la recherche Google. Exemple de flux de travail : [Flux de recherche de films](workflow/ÁîµÂΩ±Êü•ËØ¢Â∑•‰ΩúÊµÅ.json)
6. Possibilit√© de mettre en ≈ìuvre des instructions conditionnelles dans comfyui, permettant de classer les questions des utilisateurs avant de r√©pondre de mani√®re cibl√©e. Exemple de flux de travail : [Service client intelligent](workflow/Êô∫ËÉΩÂÆ¢Êúç.json)
7. Prise en charge des liens en boucle pour les grands mod√®les, permettant √† deux grands mod√®les de d√©battre. Exemple de flux de travail : [D√©bat sur le dilemme du tramway](workflow/ÁîµËΩ¶ÈöæÈ¢òËæ©ËÆ∫Ëµõ.json)
8. Prise en charge de l'attachement de n'importe quel masque de personnalit√©, possibilit√© de personnaliser les mod√®les d'invite.
9. Prise en charge de divers appels d'outils, actuellement d√©velopp√©s pour v√©rifier la m√©t√©o, l'heure, la base de connaissances, l'ex√©cution de code, la recherche en ligne, et la recherche sur une seule page Web, entre autres.
10. Prise en charge de l'utilisation de LLM en tant que n≈ìud d'outil. Exemple de flux de travail : [LLM poup√©e russe](workflow/LLMÂ•óÂ®É.json)
11. Prise en charge du d√©veloppement rapide de vos propres applications web via API + streamlit.
12. Ajout d'un n≈ìud d'interpr√©teur universel dangereux, permettant au grand mod√®le de faire n'importe quoi.
13. Il est recommand√© d'utiliser les fonctions dans le sous-r√©pertoire du menu contextuel, sous le n≈ìud de texte √† afficher (show_text), comme affichage de sortie pour le n≈ìud LLM.
14. Fonctionnalit√© visuelle prise en charge par GPT-4O ! Exemple de flux de travail : [GPT-4o](workflow/GPT-4o.json)  
15. Un interm√©diaire de flux de travail a √©t√© ajout√©, permettant √† votre flux de travail d'appeler d'autres flux de travail ! Exemple de flux de travail : [Appeler un autre flux de travail](workflow/Ë∞ÉÁî®Âè¶‰∏Ä‰∏™Â∑•‰ΩúÊµÅ.json)  
16. Compatible avec tous les mod√®les ayant une interface similaire √† OpenAI, tels que : Tongyi Qianwen/qwen, Zhipu Qingyan/GLM, deepseek, kimi/moonshot. Veuillez remplir les champs base_url, api_key et model_name des n≈ìuds LLM pour les appeler.  
17. Un chargeur LVM a √©t√© ajout√©, permettant d'appeler localement des mod√®les LVM, prenant en charge le mod√®le [llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf), d'autres mod√®les LVM au format GGUF devraient √©galement fonctionner en th√©orie. Exemple de flux de travail ici : [start_with_LVM.json](workflow/start_with_LVM.json).  
18. Un fichier `fastapi.py` a √©t√© cr√©√©. Si vous l'ex√©cutez directement, vous obtiendrez une interface OpenAI sur `http://127.0.0.1:8817/v1/`, toute application pouvant appeler GPT pourra alors interagir avec votre flux de travail ComfyUI ! Je fournirai un tutoriel d√©taill√© √† ce sujet.  
19. Le chargeur LLM et la cha√Æne LLM ont √©t√© s√©par√©s, permettant de dissocier le chargement du mod√®le et la configuration du mod√®le, facilitant ainsi le partage de mod√®les entre diff√©rents n≈ìuds LLM !  
20. La prise en charge de macOS et des appareils MPS est d√©sormais disponible ! Merci √† [bigcat88](https://github.com/bigcat88) pour cette contribution !  
21. Vous pouvez cr√©er votre propre jeu de roman interactif, avec des fins diff√©rentes selon les choix des utilisateurs ! Exemple de flux de travail : [Roman interactif](workflow/‰∫íÂä®Â∞èËØ¥.json)  
22. Int√©gration des fonctionnalit√©s Whisper et TTS d'OpenAI, permettant l'entr√©e et la sortie vocales. Exemple de flux de travail : [Entr√©e vocale + Sortie vocale](workflow/ËØ≠Èü≥ËæìÂÖ•+ËØ≠Èü≥ËæìÂá∫.json)
23. Compatible avec [Omost](https://github.com/lllyasviel/Omost) ! Veuillez t√©l√©charger [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) pour vivre l'exp√©rience imm√©diatement ! Pour un exemple de flux de travail, veuillez vous r√©f√©rer √† : [start_with_OMOST](workflow/start_with_OMOST.json)  
24. De nouveaux outils LLM ont √©t√© ajout√©s pour envoyer des messages √† WeChat d'entreprise, DingTalk et Feishu, ainsi que des fonctions externes disponibles √† l'appel.  
25. Un nouvel it√©rateur de texte a √©t√© ajout√©, capable de n'afficher qu'une partie des caract√®res √† la fois, en segmentant le texte en toute s√©curit√© par rapport aux caract√®res de retour √† la ligne et √† la taille des morceaux, sans couper le texte au milieu. Le param√®tre chunk_overlap indique combien de caract√®res se chevauchent lors de la segmentation du texte. Cela permet d'entrer en masse des textes tr√®s longs ; il suffit de cliquer sans r√©fl√©chir ou d'activer l'ex√©cution en boucle dans ComfyUI, et tout sera ex√©cut√© automatiquement. N'oubliez pas d'activer l'attribut is_locked, ce qui permettra de verrouiller automatiquement le flux de travail √† la fin de l'entr√©e, emp√™chant toute ex√©cution ult√©rieure. Exemple de flux de travail : [Texte d'it√©ration d'entr√©e](workflow/ÊñáÊú¨Ëø≠‰ª£ËæìÂÖ•.json)  
26. L'attribut model name a √©t√© ajout√© aux chargeurs LLM locaux et LLava locaux ; s'il est vide, divers chemins locaux dans le n≈ìud seront utilis√©s pour le chargement. S'il n'est pas vide, le param√®tre de chemin que vous avez rempli dans `config.ini` sera utilis√© pour le chargement. S'il n'est pas vide et n'est pas dans `config.ini`, le mod√®le sera t√©l√©charg√© depuis Hugging Face ou charg√© depuis le r√©pertoire de mod√®les de Hugging Face. Si vous souhaitez t√©l√©charger depuis Hugging Face, veuillez remplir l'attribut model name au format : `THUDM/glm-4-9b-chat`. Attention ! Les mod√®les charg√©s de cette mani√®re doivent √™tre compatibles avec la biblioth√®que Transformer.  
27. De nouveaux n≈ìuds pour le parsing de fichiers JSON et l'extraction de valeurs JSON ont √©t√© ajout√©s, vous permettant d'obtenir la valeur d'une cl√© depuis un fichier ou un texte. Merci √† [guobalove](https://github.com/guobalove) pour sa contribution !
28. Le code des appels d'outils a √©t√© am√©lior√©, permettant d√©sormais aux LLM sans fonction d'appel d'outils d'activer l'attribut is_tools_in_sys_prompt (par d√©faut, les LLM locaux n'ont pas besoin d'√™tre activ√©s et s'adaptent automatiquement). Une fois activ√©, les informations sur les outils seront ajout√©es √† l'invite syst√®me, permettant ainsi au LLM d'appeler des outils. Un article pertinent sur le principe de mise en ≈ìuvre : [Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning](https://arxiv.org/abs/2407.04997)

29. Un dossier custom_tool a √©t√© cr√©√© pour stocker le code des outils personnalis√©s. Vous pouvez vous r√©f√©rer au code dans le dossier [custom_tool](custom_tool) et y placer votre code d'outil personnalis√© afin de pouvoir l'appeler dans le LLM.

30. Un nouvel outil de graphe de connaissances a √©t√© ajout√©, permettant une interaction parfaite entre le LLM et le graphe de connaissances. Le LLM peut modifier le graphe de connaissances en fonction de vos entr√©es et d√©duire les r√©ponses n√©cessaires √† partir de celui-ci. Pour un exemple de flux de travail, veuillez consulter : [graphRAG_neo4j](workflow/graphRAG_neo4j.json).

31. Une fonctionnalit√© d'IA de personnalit√© a √©t√© ajout√©e, permettant de d√©velopper votre propre IA de petite amie ou d'ami sans code, avec des conversations illimit√©es, une m√©moire permanente et une personnalit√© stable. Pour un exemple de flux de travail, veuillez consulter : [È∫¶Ê¥õËñá‰∫∫Ê†ºAI](workflow/È∫¶Ê¥õËñá‰∫∫Ê†ºAI.json).

32. Vous pouvez utiliser cet outil de fabrication LLM pour g√©n√©rer automatiquement des outils LLM. Sauvegardez le code de l'outil g√©n√©r√© dans un fichier python, puis copiez le code dans le dossier custom_tool, cr√©ant ainsi un nouveau n≈ìud. Exemple de flux de travail : [LLMÂ∑•ÂÖ∑ÁîüÊàêÂô®](workflow/LLMÂ∑•ÂÖ∑Âà∂ÈÄ†Êú∫.json).

33. La recherche avec DuckDuckGo est d√©sormais prise en charge, mais avec de grandes limitations ; il semble que seuls des mots-cl√©s en anglais puissent √™tre saisis et qu'un seul concept soit autoris√© dans les mots-cl√©s. L'avantage r√©side dans l'absence de toute restriction d'API key.

34. La fonctionnalit√© d'appel s√©par√© de plusieurs bases de connaissances a √©t√© ajout√©e, permettant de pr√©ciser dans l'invite quelle base de connaissances doit √™tre utilis√©e pour r√©pondre aux questions. Exemple de flux de travail : [Â§öÁü•ËØÜÂ∫ìÂàÜÂà´Ë∞ÉÁî®](workflow/Â§öÁü•ËØÜÂ∫ìÂàÜÂà´Ë∞ÉÁî®.json).

35. La prise en charge de param√®tres suppl√©mentaires pour le LLM est d√©sormais possible, y compris des param√®tres avanc√©s tels que json out. Exemple de flux de travail : [LLMËæìÂÖ•È¢ùÂ§ñÂèÇÊï∞](workflow/LLMÈ¢ùÂ§ñÂèÇÊï∞eg_JSON_OUT.json). [Áî®json_outÂàÜÁ¶ªÊèêÁ§∫ËØç](workflow/Áî®json_outÂàÜÁ¶ªÊèêÁ§∫ËØç.json).
36. Ajout de la fonctionnalit√© permettant de connecter l'agent √† Discord. (Encore en test)
37. Ajout de la fonctionnalit√© permettant de connecter l'agent √† Feishu, un grand merci √† [guobalove](https://github.com/guobalove) pour sa contribution ! R√©f√©rencer le workflow [Robot Feishu](workflow/È£û‰π¶Êú∫Âô®‰∫∫.json).
38. Ajout d'un n≈ìud d'appel API universel ainsi que de nombreux n≈ìuds auxiliaires pour construire le corps de la requ√™te et extraire des informations de la r√©ponse.
39. Ajout d'un n≈ìud de videur de mod√®le, permettant de d√©charger le LLM de la m√©moire vid√©o √† tout moment !
40. Le n≈ìud [chatTTS](https://github.com/2noise/ChatTTS) a √©t√© ajout√©, un grand merci √† [guobalove](https://github.com/guobalove) pour sa contribution ! Le param√®tre `model_path` peut √™tre vide ! Il est recommand√© d'utiliser le mode HF pour charger le mod√®le, qui sera automatiquement t√©l√©charg√© depuis Hugging Face, sans besoin de t√©l√©chargement manuel ; si vous utilisez le mode local, placez les dossiers `asset` et `config` du mod√®le √† la racine. [Adresse Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), code d'extraction : qyhu ; si vous utilisez le mode `custom`, placez les dossiers `asset` et `config` du mod√®le sous `model_path`.
2. Mise √† jour d'une s√©rie de n≈ìuds de conversion : markdown en HTML, svg en image, HTML en image, mermaid en image, markdown en Excel.
1. Compatible avec le mod√®le llama3.2 vision, prend en charge les dialogues multi-tours, les fonctions visuelles. Adresse du mod√®le : [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct). Exemple de flux de travail : [llama3.2_vision](https://github.com/heshengtao/comfyui_LLM_party/blob/main/workflow_tutorial/LLM_Party%20for%20Llama3.2%20-Vision%EF%BC%88%E5%B8%A6%E8%AE%B0%E5%BF%86%EF%BC%89.json).
1. Adapt√© GOT-OCR2, prend en charge les r√©sultats de sortie format√©s, prend en charge la reconnaissance fine du texte √† l'aide de bo√Ætes de position et de couleurs. Adresse du mod√®le : [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR2_0). Exemple de flux de travail convertit une capture d'√©cran d'une page Web en code HTML, puis ouvre le navigateur pour afficher cette page Web : [img2web](workflow/ÂõæÁâáËΩ¨ÁΩëÈ°µ.json).
2. Les n≈ìuds de chargeur LLM locaux ont √©t√© consid√©rablement ajust√©s, vous n'avez donc plus besoin de choisir vous-m√™me le type de mod√®le. Le n≈ìud de chargeur llava et le n≈ìud de chargeur GGUF ont √©t√© r√©ajout√©s. Le type de mod√®le sur le n≈ìud de cha√Æne de mod√®les LLM local a √©t√© chang√© en LLM, VLM-GGUF et LLM-GGUF, correspondant au chargement direct des mod√®les LLM, au chargement des mod√®les VLM et au chargement des mod√®les LLM au format GGUF. Les mod√®les VLM et les mod√®les LLM au format GGUF sont √† nouveau pris en charge. Les appels locaux peuvent d√©sormais √™tre compatibles avec plus de mod√®les ! Exemples de flux de travail : [LLM_local](workflow/start_with_LLM_local.json), [llava](workflow/start_with_llava.json), [GGUF](workflow/start_with_GGUF.json)
2. Ajout du n≈ìud EasyOCR pour reconna√Ætre le texte et les positions dans les images. Il peut g√©n√©rer des masques correspondants et renvoyer une cha√Æne JSON pour que LLM puisse la consulter. Des versions standard et premium sont disponibles pour tout le monde!
2. Lors de la f√™te comfyui LLM, le syst√®me de fraises du mod√®le de la s√©rie chatgpt-o1 a √©t√© reproduit, en se r√©f√©rant aux invites de [Llamaberry](https://huggingface.co/spaces/martinbowling/Llamaberry/blob/main/app.py). Exemple de flux de travail : [Syst√®me de fraises compar√© √† o1](workflow/ËçâËéìÁ≥ªÁªü‰∏éo1ÂØπÊØî.json).
2. Un nouveau n≈ìud GPT-sovits a √©t√© ajout√©, vous permettant d'appeler le mod√®le GPT-sovits pour convertir du texte en parole en fonction de votre audio de r√©f√©rence. Vous pouvez √©galement remplir le chemin de votre mod√®le affin√© (si non rempli, le mod√®le de base sera utilis√© pour l'inf√©rence) pour obtenir la voix souhait√©e. Pour l'utiliser, vous devez t√©l√©charger le projet [GPT-sovits](https://github.com/RVC-Boss/GPT-SoVITS) et le mod√®le de base correspondant localement, puis d√©marrer le service API avec `runtime\python.exe api_v2.py` dans le dossier du projet GPT-sovits. De plus, le n≈ìud chatTTS a √©t√© d√©plac√© vers [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia). La raison est que chatTTS a de nombreuses d√©pendances, et sa licence sur PyPi est CC BY-NC 4.0, qui est une licence non commerciale. M√™me si le projet chatTTS sur GitHub est sous licence AGPL, nous avons d√©plac√© le n≈ìud chatTTS vers comfyui LLM mafia pour √©viter des probl√®mes inutiles. Nous esp√©rons que tout le monde comprendra!
3. Prend d√©sormais en charge le dernier mod√®le d‚ÄôOpenAI, la s√©rie o1 !
4. Ajout d‚Äôun outil de contr√¥le des fichiers locaux permettant au LLM de contr√¥ler les fichiers dans le dossier sp√©cifi√©, tels que la lecture, l‚Äô√©criture, l‚Äôajout, la suppression, le renommage, le d√©placement et la copie de fichiers.En raison du danger potentiel de ce n≈ìud, il est inclus dans [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia).
5. De nouveaux outils SQL permettent √† LLM d‚Äôinterroger des bases de donn√©es SQL.
6. Mise √† jour de la version multilingue du README. Flux de travail pour traduire le document README : [translate_readme](workflow/ÊñáÊ°£Ëá™Âä®ÁøªËØëÊú∫.json)
7. Quatre n≈ìuds d'it√©rateur ont √©t√© mis √† jour (it√©rateur de texte, it√©rateur d'image, it√©rateur de tableau, it√©rateur json), avec trois modes d'it√©rateur : s√©quentiel, al√©atoire et infini. Le mode s√©quentiel produira des sorties dans l'ordre jusqu'√† d√©passer la limite d'index, moment o√π le processus s'arr√™te automatiquement et la valeur d'index est r√©initialis√©e √† 0 ; le mode al√©atoire choisira un index al√©atoire pour la sortie, et le mode infini effectuera une sortie en boucle infinie.
8. Un nouveau n≈ìud de chargeur d'API Gemini a √©t√© ajout√©, d√©sormais compatible avec l'API officielle de Gemini ! Si vous √™tes dans un environnement r√©seau domestique et que vous rencontrez des probl√®mes de restriction de r√©gion pour l'API, veuillez passer le n≈ìud sur les √âtats-Unis et utiliser le mode TUN. √âtant donn√© que Gemini g√©n√®re une erreur avec un code retour 500 si des caract√®res chinois sont pr√©sents dans les param√®tres retourn√©s lors de l'appel d'outils, certains n≈ìuds d'outils peuvent ne pas √™tre disponibles. Flux de travail d'exemple : [start_with_gemini](workflow/start_with_gemini.json)
9. Un n≈ìud de livre de lore a √©t√© ajout√©, permettant d'ins√©rer vos param√®tres de fond lors des dialogues avec LLM, flux de travail d'exemple : [lorebook](workflow/lorebook.json)
10. Un n≈ìud de g√©n√©rateur de mots-cl√©s FLUX a √©t√© ajout√©, capable de g√©n√©rer des mots-cl√©s dans divers styles, tels que des cartes Hearthstone, des cartes Yu-Gi-Oh, des affiches, des bandes dessin√©es, etc., facilitant la sortie directe du mod√®le FLUX. Flux de travail de r√©f√©rence : [FLUXÊèêÁ§∫ËØç](https://openart.ai/workflows/comfyui_llm_party/flux-by-llm-party/sjME541i68Kfw6Ib0EAD)

## Prochaines √©tapes :
1. Plus d'adaptations de mod√®les;
2. Plus de fa√ßons de construire des agents;
3. Plus de fonctionnalit√©s d'automatisation;
4. Plus de fonctionnalit√©s de gestion de la base de connaissances;
5. Plus d'outils, plus de personas.

## Avertissement :
Ce projet open source et son contenu (ci-apr√®s d√©nomm√© "projet") sont fournis uniquement √† titre de r√©f√©rence et ne constituent en aucun cas une garantie expresse ou implicite. Les contributeurs du projet ne sauraient √™tre tenus responsables de l'int√©grit√©, de l'exactitude, de la fiabilit√© ou de l'applicabilit√© du projet. Toute action reposant sur le contenu du projet se fait √† vos propres risques. En aucun cas, les contributeurs du projet ne sauraient √™tre tenus responsables des pertes ou dommages indirects, sp√©ciaux ou cons√©cutifs r√©sultant de l'utilisation du contenu du projet.
## Remerciements sp√©ciaux
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/HuangYuChuh">
  <img src="https://avatars.githubusercontent.com/u/167663109?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/SpenserCai">
  <img src="https://avatars.githubusercontent.com/u/25168945?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Liste des emprunts
Certaines des fonctionnalit√©s de ce projet s'inspirent des projets suivants, nous les remercions pour leur contribution √† la communaut√© open source !
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)

## Support :

### Rejoignez la communaut√©
Si vous rencontrez des probl√®mes avec le plugin ou si vous avez d'autres questions, n'h√©sitez pas √† rejoindre notre communaut√©.

1. Groupe QQ : `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/QÁæ§.jpg" style="width: 48%;" />
</div>

2. Groupe WeChat : `Choo-Yong` (ajoutez le petit assistant WeChat pour rejoindre le groupe)

3. discord : [lien discord](https://discord.gg/hbMQDH7J)

### Suivez-nous
1. Si vous souhaitez rester inform√© des derni√®res fonctionnalit√©s de ce projet, n'h√©sitez pas √† suivre notre compte Bilibili : [Ê¥æÂØπ‰∏ªÊåÅBBÊú∫](https://space.bilibili.com/26978344)
2. Le compte OpenArt est r√©guli√®rement mis √† jour avec les workflows de party les plus utiles : [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

### Soutien par don
Si mon travail vous apporte de la valeur, veuillez envisager de m'offrir un caf√© ! Votre soutien insuffle non seulement de la vitalit√© au projet, mais r√©chauffe √©galement le c≈ìur des cr√©ateurs.‚òïüíñ Chaque tasse compte !
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## Historique des √©toiles

[![Star History Chart](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
