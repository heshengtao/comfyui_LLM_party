![Image](img/Â∞ÅÈù¢.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">bilibili</a> ¬∑
  <a href="https://www.youtube.com/@comfyui-LLM-party">youtube</a> ¬∑
  <a href="https://github.com/heshengtao/Let-LLM-party">Tutoriel √©crit</a> ¬∑
  <a href="https://pan.quark.cn/s/190b41f3bbdb">Adresse du disque cloud</a> ¬∑
  <a href="img/QÁæ§.jpg">Groupe QQ</a> ¬∑
  <a href="https://discord.gg/f2dsAKKr2V">Discord</a> ¬∑
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">√Ä propos de nous</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/–†—É—Å—Å–∫–∏–π-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Fran√ßais-d9d9d9"></a> 
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/ÿßŸÑÿπÿ±ÿ®Ÿäÿ©-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Espa√±ol-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Portugu√™s-d9d9d9"></a>
</div>

####

Comfyui_llm_party aspire √† d√©velopper une biblioth√®que compl√®te de n≈ìuds pour la construction de workflows LLM, bas√©e sur l'interface utilisateur extr√™mement simplifi√©e de [comfyui](https://github.com/comfyanonymous/ComfyUI). Cela permettra aux utilisateurs de construire plus facilement et rapidement leurs propres workflows LLM et de les int√©grer de mani√®re plus pratique dans leurs workflows d'images.

## D√©monstration des effets
https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6

## Aper√ßu du projet
ComfyUI LLM Party permet de construire rapidement votre propre assistant AI personnalis√©, allant des appels multi-outils de LLM aux configurations de r√¥le, en passant par la gestion localis√©e des bases de connaissances du secteur gr√¢ce aux vecteurs de mots RAG et GraphRAG. De la cha√Æne d'agents intelligents unique √† la construction de modes d'interaction complexes entre agents intelligents, y compris les modes d'interaction radiaux et circulaires ; des besoins des utilisateurs individuels qui souhaitent int√©grer leurs applications sociales (QQ, Feishu, Discord) √† un flux de travail tout-en-un LLM+TTS+ComfyUI pour les travailleurs des m√©dias en continu ; des premi√®res applications LLM simples pour les √©tudiants ordinaires aux diverses interfaces de r√©glage de param√®tres utilis√©es par les chercheurs, ainsi que l'adaptation des mod√®les. Tout cela peut √™tre d√©couvert lors de ComfyUI LLM Party.

## D√©marrage rapide
0. Si vous n'avez jamais utilis√© ComfyUI et que vous rencontrez des probl√®mes de d√©pendance lors de l'installation de LLM Party dans ComfyUI, cliquez [ici](https://drive.google.com/file/d/1T9C7gEbd-w_zf9GqZO1VeI3z8ek8clpX/view?usp=sharing) pour t√©l√©charger le package portable **Windows** contenant LLM Party. Attention ! Ce package portable ne comprend que les deux plugins Party et Manager, et il est uniquement compatible avec le syst√®me Windows.(Si vous devez installer LLM party sur un ComfyUI existant, cette √©tape peut √™tre ignor√©e.)
1. Faites glisser les workflows suivants dans votre comfyui, puis utilisez [comfyui-Manager](https://github.com/ltdrdata/ComfyUI-Manager) pour installer les n≈ìuds manquants.
  - Utilisez l'API pour appeler LLM : [start_with_LLM_api](workflow/start_with_LLM_api.json)
  - Utilisation de aisuite pour appeler LLM : [start_with_aisuite](workflow/start_with_aisuite.json)
  - G√©rez les LLM locaux avec ollama : [start_with_Ollama](workflow/ollama.json)
  - Utilisez des LLM locaux au format distribu√© : [start_with_LLM_local](workflow/start_with_LLM_local.json)
  - Utilisez des LLM locaux au format GGUF : [start_with_LLM_GGUF](workflow/start_with_GGUF.json)
  - Utilisez des VLM locaux au format distribu√© : [start_with_VLM_local](workflow/start_with_VLM_local.json) ÔºàActuellement, le support est disponible pour [Llama-3.2-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)/[Qwen/Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)/[deepseek-ai/Janus-Pro](https://huggingface.co/deepseek-ai/Janus-Pro-1B)Ôºâ
  - Utilisez des VLM locaux au format GGUF : [start_with_VLM_GGUF](workflow/start_with_llava.json)
  - Utiliser l'API pour appeler LLM afin de g√©n√©rer des mots-cl√©s SD et produire des images : [commencer_avec_VLM_API_pour_SD](workflow/start_with_VLM_API_for_SD.json)
  - Utiliser ollama pour appeler minicpm afin de g√©n√©rer des mots-cl√©s SD et produire des images : [commencer_avec_ollama_minicpm_pour_SD](workflow/start_with_ollama_minicpm_for_SD.json)
  - Utiliser le qwen-vl local pour g√©n√©rer des mots-cl√©s SD et produire des images : [commencer_avec_qwen_vl_local_pour_SD](workflow/start_with_qwen_vl_local_for_SD.json)
2. Si vous utilisez l'API, remplissez votre `base_url` (cela peut √™tre une API relais, assurez-vous qu'elle se termine par `/v1/`) et `api_key` dans le n≈ìud de chargement de l'API LLM. Exemple : `https://api.openai.com/v1/`
3. Si vous utilisez ollama, activez l'option `is_ollama` dans le n≈ìud de chargement de l'API LLM, sans remplir `base_url` et `api_key`.
4. Si vous utilisez un mod√®le local, remplissez le chemin de votre mod√®le dans le n≈ìud de chargement du mod√®le local, par exemple : `E:\model\Llama-3.2-1B-Instruct`. Vous pouvez √©galement remplir l'ID du d√©p√¥t du mod√®le Huggingface dans le n≈ìud de chargement du mod√®le local, par exemple : `lllyasviel/omost-llama-3-8b-4bits`.
5. En raison du seuil d'utilisation √©lev√© de ce projet, m√™me si vous choisissez le d√©marrage rapide, j'esp√®re que vous pourrez lire attentivement la page d'accueil du projet.

## Derni√®res mises √† jour
1. Le n≈ìud API LLM prend d√©sormais en charge le mode de sortie en flux, affichant en temps r√©el le texte retourn√© par l'API dans la console, vous permettant ainsi de voir la sortie de l'API sans avoir √† attendre que l'ensemble de la requ√™te soit compl√©t√©.
2. Le n≈ìud API LLM a √©t√© enrichi d'une sortie de reasoning_content, capable de s√©parer automatiquement le raisonnement et la r√©ponse du mod√®le R1.
3. Une nouvelle branche de d√©p√¥t, uniquement_api, a √©t√© ajout√©e. Cette branche ne contient que la partie appelant l'API, facilitant ainsi l'utilisation pour les utilisateurs qui n'ont besoin que des appels API. Il vous suffit d'utiliser la commande dans le dossier `custom tool` de `comfyui` : `git clone -b only_api https://github.com/heshengtao/comfyui_LLM_party.git`, puis de suivre le sch√©ma de d√©ploiement environnemental du site principal de ce projet pour utiliser cette branche. Attention ! Si vous souhaitez vous assurer qu'il n'existe pas d'autre dossier nomm√© `comfyui_LLM_party` dans le dossier `custom tool`.
1. Le n≈ìud de chargeur local VLM prend d√©sormais en charge [deepseek-ai/Janus-Pro](https://huggingface.co/deepseek-ai/Janus-Pro-1B), exemple de flux de travail : [Janus-Pro](workflow/deepseek-janus-pro.json)  
1. Le n≈ìud de chargeur local VLM prend d√©sormais en charge [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), cependant, vous devez mettre √† jour le transformateur vers la derni√®re version (```pip install -U transformers```), flux de travail exemple : [qwen-vl](workflow/qwen-vl.json)
1. Un nouveau n≈ìud de stockage d'images a √©t√© ajout√©, prenant en charge le service d'h√©bergement d'images https://sm.ms (le domaine pour la Chine est https://smms.app) ainsi que https://imgbb.com. D'autres services d'h√©bergement d'images seront pris en charge √† l'avenir. Exemple de flux de travail : [H√©bergement d'images](workflow/ÂõæÂ∫ä.json)  
1. ~~L'h√©bergement d'images compatible par d√©faut pour party a √©t√© mis √† jour vers le domaine [imgbb](https://imgbb.io). L'ancien h√©bergement n'√©tait pas convivial pour les utilisateurs de la Chine continentale, il a donc √©t√© remplac√©.~~ Je suis d√©sol√©, le service API d'h√©bergement d'images de https://imgbb.io semble avoir √©t√© interrompu, donc le code a √©t√© r√©tabli √† l'original https://imgbb.com. Merci pour votre compr√©hension. √Ä l'avenir, je vais mettre √† jour un n≈ìud qui supportera davantage d'h√©bergements d'images.
1. L'outil [MCP](https://modelcontextprotocol.io/introduction) a √©t√© mis √† jour, vous pouvez modifier la configuration dans le fichier '[mcp_config.json](mcp_config.json)' situ√© dans le dossier du projet party pour ajuster la connexion au serveur MCP souhait√©. Vous pouvez trouver ici divers param√®tres de configuration des serveurs MCP que vous souhaitez ajouter : [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers). La configuration par d√©faut de ce projet est le serveur Everything, qui sert √† tester si le serveur MCP fonctionne correctement. Workflow de r√©f√©rence : [start_with_MCP](workflow/start_with_MCP.json). Note pour les d√©veloppeurs : le n≈ìud d'outil MCP peut se connecter au serveur MCP que vous avez configur√©, puis transformer les outils du serveur en outils directement utilisables par LLM. En configurant diff√©rents serveurs locaux ou cloud, vous pouvez exp√©rimenter tous les outils LLM disponibles dans le monde.

## Instructions d'utilisation
1. Pour les instructions d'utilisation des n≈ìuds, veuillez consulter : [ÊÄé‰πà‰ΩøÁî®ËäÇÁÇπ](https://github.com/heshengtao/Let-LLM-party)

2. Si vous rencontrez des probl√®mes avec le plugin ou si vous avez d'autres questions, n'h√©sitez pas √† rejoindre le groupe QQ : [931057213](img/QÁæ§.jpg) |discordÔºö[discord](https://discord.gg/f2dsAKKr2V).

4. Pour plus de flux de travail, veuillez consulter le dossier [workflow](workflow).

## Tutoriels vid√©o
<a href="https://space.bilibili.com/26978344">
  <img src="img/B.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://www.youtube.com/@comfyui-LLM-party">
  <img src="img/YT.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>

## Support du mod√®le
1. Prise en charge de tous les appels API au format OpenAI (en combinaison avec [oneapi](https://github.com/songquanpeng/one-api), il est possible d'appeler presque toutes les API LLM, ainsi que toutes les API de transit). Pour le choix de base_url, veuillez vous r√©f√©rer √† [config.ini.example](config.ini.example). Actuellement, les API test√©es incluent :
* [openai](https://platform.openai.com/docs/api-reference/chat/create) (Parfaitement compatible avec tous les mod√®les OpenAI, y compris les s√©ries 4o et o1!)
* [ollama](https://github.com/ollama/ollama) (Recommand√©! Si vous appelez localement, il est fortement recommand√© d'utiliser la m√©thode ollama pour h√©berger votre mod√®le local!)
* [Azure OpenAI](https://azure.microsoft.com/zh-cn/products/ai-services/openai-service/)
* [llama.cpp](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#web-server) (Recommand√©! Si vous souhaitez utiliser le mod√®le au format gguf local, vous pouvez utiliser l'API du projet llama.cpp pour acc√©der √† ce projet!)
* [Grok](https://x.ai/api)
* [ÈÄö‰πâÂçÉÈóÆ/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [Êô∫Ë∞±Ê∏ÖË®Ä/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [doubao](https://www.volcengine.com/docs/82379/1263482)
* [ËÆØÈ£ûÊòüÁÅ´/spark](https://xinghuo.xfyun.cn/sparkapi?scr=price)
* [Gemini](https://developers.googleblog.com/zh-hans/gemini-is-now-accessible-from-the-openai-library/) (Le n≈ìud de chargeur API LLM d'origine Gemini a √©t√© abandonn√© dans la nouvelle version, veuillez utiliser le n≈ìud de chargeur API LLM avec l'URL de base choisie : https://generativelanguage.googleapis.com/v1beta/openai/)

2. Support de tous les appels API compatibles avec [aisuite](https://github.com/andrewyng/aisuite) :
* [anthropic/claude](https://www.anthropic.com/) 
* [aws](https://docs.aws.amazon.com/solutions/latest/generative-ai-application-builder-on-aws/api-reference.html)
* [vertex](https://cloud.google.com/vertex-ai/docs/reference/rest)
* [huggingface](https://huggingface.co/)  

3. Compatible avec la plupart des mod√®les locaux dans la biblioth√®que transformer (le type de mod√®le sur le n≈ìud de cha√Æne de mod√®les LLM local a √©t√© chang√© en LLM, VLM-GGUF et LLM-GGUF, correspondant au chargement direct des mod√®les LLM, au chargement des mod√®les VLM et au chargement des mod√®les LLM au format GGUF). Si votre mod√®le LLM au format VLM ou GGUF signale une erreur, veuillez t√©l√©charger la derni√®re version de llama-cpp-python depuis [llama-cpp-python](https://github.com/abetlen/llama-cpp-python/releases). Les mod√®les actuellement test√©s incluent :
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (recommand√© ! Mod√®le de jeu de r√¥le)
* [lllyasviel/omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (recommand√© ! Mod√®le avec des invites riches)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [openbmb/MiniCPM-V-2_6-gguf](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf/tree/main)
* [lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main)
* [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)
* [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
* [deepseek-ai/Janus-Pro](https://huggingface.co/deepseek-ai/Janus-Pro-1B)

4. T√©l√©chargement des mod√®les :
* [Adresse du cloud Quark](https://pan.quark.cn/s/190b41f3bbdb)  
* [Adresse Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), code d'extraction : qyhu

## T√©l√©chargement
Installez en utilisant l'une des m√©thodes suivantes
### M√©thode 1 :
1. Recherchez `comfyui_LLM_party` dans le [gestionnaire ComfyUI](https://github.com/ltdrdata/ComfyUI-Manager) et installez-le d'un seul clic
2. Red√©marrez ComfyUI
### M√©thode deux :
1. Naviguez vers le sous-dossier `custom_nodes` dans le dossier racine de ComfyUI.
2. Clonez ce d√©p√¥t. `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### M√©thode trois :
1. Cliquez sur `CODE` en haut √† droite.
2. Cliquez sur `t√©l√©charger zip`.
3. D√©compressez le fichier zip t√©l√©charg√© dans le sous-dossier `custom_nodes` du dossier racine de ComfyUI.

## D√©ploiement de l'environnement
1. Naviguez vers le dossier du projet `comfyui_LLM_party`.
2. Dans le terminal, saisissez `pip install -r requirements.txt` pour d√©ployer les biblioth√®ques tierces n√©cessaires √† ce projet dans l'environnement de comfyui. Veuillez v√©rifier que vous installez dans l'environnement de comfyui et faites attention aux erreurs `pip` dans le terminal.
3. Si vous utilisez le lanceur comfyui, vous devez entrer dans le terminal `chemin dans la configuration du lanceur\python_embeded\python.exe -m pip install -r requirements.txt` pour proc√©der √† l'installation. Le dossier `python_embeded` est g√©n√©ralement au m√™me niveau que votre dossier `ComfyUI`.
4. Si vous rencontrez des probl√®mes de configuration de l'environnement, vous pouvez essayer d'utiliser les d√©pendances dans `requirements_fixed.txt`.
## Configuration
* Vous pouvez configurer la langue dans le fichier `config.ini`, actuellement seules le chinois (zh_CN) et l'anglais (en_US) sont disponibles, la langue par d√©faut √©tant celle de votre syst√®me.
* Vous pouvez configurer l'installation rapide dans le fichier `config.ini`, `fast_installed` est par d√©faut `False`. Si vous n'avez pas besoin d'utiliser le mod√®le GGUF, vous pouvez le r√©gler sur `True`.
* Vous pouvez configurer l'APIKEY en utilisant l'une des m√©thodes suivantes :
### M√©thode 1 :
1. Ouvrez le fichier `config.ini` dans le dossier du projet `comfyui_LLM_party`.
2. Dans `config.ini`, saisissez votre `openai_api_key` et `base_url`.
3. Si vous utilisez le mod√®le ollama, entrez `http://127.0.0.1:11434/v1/` dans `base_url`, `ollama` dans `openai_api_key`, et le nom de votre mod√®le dans `model_name`, par exemple : llama3.
4. Si vous souhaitez utiliser les outils de recherche Google ou Bing, saisissez votre `google_api_key`, `cse_id` ou `bing_api_key` dans `config.ini`.
5. Si vous d√©sirez utiliser des entr√©es d'images pour LLM, il est recommand√© d'utiliser le service d'h√©bergement d'images imgbb, et d'indiquer votre `imgbb_api` dans `config.ini`.
6. Chaque mod√®le peut √™tre configur√© individuellement dans le fichier `config.ini`, vous pouvez vous r√©f√©rer au fichier `config.ini.example` pour vous aider. Une fois que vous avez termin√© la configuration, il vous suffit de saisir `model_name` dans le n≈ìud.

### M√©thode 2 :
1. Ouvrez l'interface comfyui.
2. Cr√©ez un n≈ìud de grand mod√®le de langage (LLM) et saisissez directement votre `openai_api_key` et `base_url` dans le n≈ìud.
3. Si vous utilisez le mod√®le ollama, veuillez utiliser le n≈ìud LLM_api, entrer `http://127.0.0.1:11434/v1/` dans `base_url`, `ollama` dans `api_key`, et le nom de votre mod√®le dans `model_name`, par exemple : llama3.
4. Si vous souhaitez utiliser des entr√©es d'images pour LLM, il est recommand√© d'utiliser le service d'h√©bergement d'images imgbb et d'indiquer votre `imgbb_api_key` dans le n≈ìud.

## Journal des mises √† jour
**[Click here](change_log.md)**

## Prochaines √©tapes :
1. Plus d'adaptations de mod√®les;
2. Plus de fa√ßons de construire des agents;
3. Plus de fonctionnalit√©s d'automatisation;
4. Plus de fonctionnalit√©s de gestion de la base de connaissances;
5. Plus d'outils, plus de personas.

## Avertissement :
Ce projet open source et son contenu (ci-apr√®s d√©nomm√© "projet") sont fournis uniquement √† titre de r√©f√©rence et ne constituent en aucun cas une garantie expresse ou implicite. Les contributeurs du projet ne sauraient √™tre tenus responsables de l'int√©grit√©, de l'exactitude, de la fiabilit√© ou de l'applicabilit√© du projet. Toute action reposant sur le contenu du projet se fait √† vos propres risques. En aucun cas, les contributeurs du projet ne sauraient √™tre tenus responsables des pertes ou dommages indirects, sp√©ciaux ou cons√©cutifs r√©sultant de l'utilisation du contenu du projet.
## Remerciements sp√©ciaux
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/SpenserCai">
  <img src="https://avatars.githubusercontent.com/u/25168945?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Liste des emprunts
Certaines des fonctionnalit√©s de ce projet s'inspirent des projets suivants, nous les remercions pour leur contribution √† la communaut√© open source !
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)

## Support :

### Rejoignez la communaut√©
Si vous rencontrez des probl√®mes avec le plugin ou si vous avez d'autres questions, n'h√©sitez pas √† rejoindre notre communaut√©.

1. Groupe QQ : `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/QÁæ§.jpg" style="width: 48%;" />
</div>

2. Groupe WeChat : `we_glm` (ajoutez le petit assistant WeChat pour rejoindre le groupe)

3. discord : [lien discord](https://discord.gg/f2dsAKKr2V)

### Suivez-nous
1. Si vous souhaitez rester inform√© des derni√®res fonctionnalit√©s de ce projet, n'h√©sitez pas √† suivre notre compte Bilibili : [Ê¥æÈÖ±](https://space.bilibili.com/26978344)
2. [youtube@comfyui-LLM-party](https://www.youtube.com/@comfyui-LLM-party)

### Soutien par don
Si mon travail vous apporte de la valeur, veuillez envisager de m'offrir un caf√© ! Votre soutien insuffle non seulement de la vitalit√© au projet, mais r√©chauffe √©galement le c≈ìur des cr√©ateurs.‚òïüíñ Chaque tasse compte !
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## Historique des √©toiles

[![Star History Chart](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
