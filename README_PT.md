![Imagem](img/Â∞ÅÈù¢.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">Tutorial em v√≠deo</a> ¬∑
  <a href="how_to_use_nodes_ZH.md">Tutorial em texto</a> ¬∑
  <a href="workflow_tutorial/">Tutorial de fluxo de trabalho</a> ¬∑
  <a href="https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu">Link do Baidu Pan</a> ¬∑
  <a href="img/QÁæ§.jpg">Grupo do QQ</a> ¬∑
  <a href="https://discord.gg/gxrQAYy6">Discord</a> ¬∑
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">Sobre n√≥s</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/–†—É—Å—Å–∫–∏–π-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Fran√ßais-d9d9d9"></a> 
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/ÿßŸÑÿπÿ±ÿ®Ÿäÿ©-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Espa√±ol-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Portugu√™s-d9d9d9"></a>
</div>

####

Comfyui_llm_party tem como objetivo, com base na [comfyui](https://github.com/comfyanonymous/ComfyUI), uma interface de usu√°rio extremamente minimalista, desenvolver um conjunto completo de biblioteca de n√≥s para a constru√ß√£o de fluxos de trabalho de LLM. Isso permitir√° que os usu√°rios construam seus pr√≥prios fluxos de trabalho de LLM de forma mais r√°pida e conveniente, al√©m de integrar facilmente em seus pr√≥prios fluxos de trabalho de imagem.

## Demonstra√ß√£o de resultados
https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6

## Vis√£o Geral do Projeto
ComfyUI LLM Party permite desde a chamada de m√∫ltiplas ferramentas LLM, configura√ß√£o r√°pida de personagens para construir seu assistente de IA personalizado, at√© a aplica√ß√£o de Word Vector RAG e GraphRAG para gerenciamento local de bancos de dados de conhecimento da ind√∫stria; desde um pipeline de agentes inteligentes simples, at√© a constru√ß√£o de modos de intera√ß√£o complexos entre agentes inteligentes em padr√£o radial e circular; desde a necessidade de usu√°rios individuais de integrar seus aplicativos sociais (QQ, Feishu, Discord), at√© o fluxo de trabalho tudo-em-um LLM+TTS+ComfyUI que os trabalhadores de streaming precisam; desde o primeiro aplicativo LLM que alunos comuns precisam para um f√°cil in√≠cio, at√© as diversas interfaces de ajuste de par√¢metros frequentemente utilizadas por pesquisadores e a adapta√ß√£o de modelos. Tudo isso, voc√™ pode encontrar respostas no ComfyUI LLM Party.

## Atualiza√ß√µes Recentes
1. Na festa comfyui LLM, o sistema de morango do modelo da s√©rie chatgpt-o1 foi reproduzido, referindo-se aos prompts de [Llamaberry](https://huggingface.co/spaces/martinbowling/Llamaberry/blob/main/app.py). Exemplo de fluxo de trabalho: [Sistema de morango comparado ao o1](workflow\ËçâËéìÁ≥ªÁªü‰∏éo1ÂØπÊØî.json).
2. Foi adicionado um novo n√≥ GPT-sovits, permitindo chamar o modelo GPT-sovits para converter texto em fala com base no seu √°udio de refer√™ncia. Voc√™ tamb√©m pode preencher o caminho do seu modelo ajustado (se n√£o preenchido, o modelo base ser√° usado para infer√™ncia) para obter qualquer voz desejada. Para us√°-lo, voc√™ precisa baixar o projeto [GPT-sovits](https://github.com/RVC-Boss/GPT-SoVITS) e o modelo base correspondente localmente, depois iniciar o servi√ßo API com `runtime\python.exe api_v2.py` na pasta do projeto GPT-sovits. Al√©m disso, o n√≥ chatTTS foi movido para [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia). A raz√£o √© que o chatTTS tem muitas depend√™ncias, e sua licen√ßa no PyPi √© CC BY-NC 4.0, que √© uma licen√ßa n√£o comercial. Embora o projeto chatTTS no GitHub esteja sob a licen√ßa AGPL, movemos o n√≥ chatTTS para comfyui LLM mafia para evitar problemas desnecess√°rios. Esperamos que todos entendam!
3. Agora suporta o modelo mais recente da OpenAI, a s√©rie o1!
4. Foi adicionada uma ferramenta de controle de arquivos locais que permite ao LLM controlar arquivos na pasta especificada, como ler, escrever, adicionar, excluir, renomear, mover e copiar arquivos.Devido ao perigo potencial deste n√≥, ele est√° inclu√≠do em [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia).
5. Novas ferramentas SQL permitem que o LLM consulte bancos de dados SQL.
6. Atualizada a vers√£o multil√≠ngue do README. Fluxo de trabalho para traduzir o documento README: [translate_readme](workflow/ÊñáÊ°£Ëá™Âä®ÁøªËØëÊú∫.json)
7. Atualizados quatro n√≥s iteradores (iterador de texto, iterador de imagem, iterador de tabela, iterador JSON), onde os modos de itera√ß√£o s√£o: sequencial, aleat√≥rio e infinito. O modo sequencial ir√° gerar sa√≠das em ordem, at√© que exceda o limite do √≠ndice e interrompa automaticamente o processo, reiniciando o valor do √≠ndice para 0; o modo aleat√≥rio escolher√° um √≠ndice aleat√≥rio para sa√≠da; o modo infinito ir√° gerar sa√≠das em um loop infinito.
8. Adicionado o n√≥ carregador de API Gemini, agora compat√≠vel com a API oficial do Gemini! Se voc√™ estiver em um ambiente de rede nacional e encontrar problemas de restri√ß√£o geogr√°fica da API, por favor, altere o n√≥ para os Estados Unidos e use o modo TUN. Devido ao fato de que, ao chamar ferramentas, se os par√¢metros retornados contiverem caracteres chineses, ocorrer√° um erro com c√≥digo de retorno 500, portanto, alguns n√≥s de ferramentas podem n√£o estar dispon√≠veis. Fluxo de trabalho de exemplo: [start_with_gemini](workflow/start_with_gemini.json)
9. Adicionado o n√≥ de livro de lore, que permite inserir seu cen√°rio de fundo durante a conversa com o LLM, fluxo de trabalho de exemplo: [lorebook](workflow/lorebook.json)
10. Adicionado o n√≥ gerador de prompts FLUX, que pode gerar prompts em diversos estilos, como cartas de Hearthstone, cartas de Yu-Gi-Oh, p√¥steres, quadrinhos, etc., permitindo que o modelo FLUX gere diretamente. Fluxo de trabalho de refer√™ncia: [FLUXÊèêÁ§∫ËØç](https://openart.ai/workflows/comfyui_llm_party/flux-by-llm-party/sjME541i68Kfw6Ib0EAD)

## Instru√ß√µes de Uso
1. Para instru√ß√µes sobre o uso dos n√≥s, consulte: [ÊÄé‰πà‰ΩøÁî®ËäÇÁÇπ](how_to_use_nodes.md)

2. Se houver problemas com o plugin ou se voc√™ tiver outras d√∫vidas, sinta-se √† vontade para entrar no grupo QQ: [931057213](img/QÁæ§.jpg)
3. Para o tutorial de fluxo de trabalho, por favor, consulte: [Â∑•‰ΩúÊµÅÊïôÁ®ã](workflow_tutorial/)Ôºåagradecemos a contribui√ß√£o de [HuangYuChuh](https://github.com/HuangYuChuh)!

4. Conta para o uso avan√ßado do fluxo de trabalho: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

4. Para mais fluxos de trabalho, voc√™ pode consultar a pasta [workflow](workflow).

## Tutoriais em v√≠deo
1. [Um guia passo a passo sobre como construir agentes modulares (super f√°cil!)](https://www.bilibili.com/video/BV1JZ421v7Tw/?vd_source=f229e378448918b84afab7c430c6a75b)

2. [Como conectar o GPT-4o ao comfyui | Permitir que um fluxo de trabalho chame outro fluxo de trabalho | Transformar LLM em uma ferramenta](https://www.bilibili.com/video/BV1JJ4m1A789/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)

3. [Como disfar√ßar seu fluxo de trabalho como GPT conectado ao WeChat | Compat√≠vel com Omost! Crie seu pr√≥prio dalle3 de forma flex√≠vel](https://www.bilibili.com/video/BV1DT421a7KY/?spm_id_from=333.999.0.0)

4. [Como jogar jogos de novela interativa no comfyui](https://www.bilibili.com/video/BV15y411q78L/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)

5. [Namorada de IA, e na sua forma | Implementa√ß√£o de graphRAG no comfyui, interagindo com neoa4j | Integra√ß√£o do fluxo de trabalho comfyui com o front-end streamlit](https://www.bilibili.com/video/BV1dS421R7Au/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)
## Suporte ao Modelo
1. Suporte a todas as chamadas de API no formato openai (combinado com [oneapi](https://github.com/songquanpeng/one-api), √© poss√≠vel chamar quase todas as APIs LLM, al√©m de suportar todas as APIs de encaminhamento), a escolha do base_url deve seguir o exemplo em [config.ini.example](config.ini.example), atualmente testados incluem:
* [ollama](https://github.com/ollama/ollama) (recomendado! Se voc√™ est√° chamando localmente, √© altamente aconselh√°vel usar o m√©todo ollama para hospedar seu modelo local!)
* [ÈÄö‰πâÂçÉÈóÆ/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [Êô∫Ë∞±Ê∏ÖË®Ä/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [Ë±ÜÂåÖ](https://www.volcengine.com/docs/82379/1263482)

2. Suporte a chamadas de API no formato Gemini:
* [Gemini](https://aistudio.google.com/app/prompts/new_chat)
3. Compat√≠vel com a maioria dos modelos locais suportados pela classe AutoModelForCausalLM da biblioteca transformer (se voc√™ n√£o souber qual tipo de modelo escolher no n√≥ do modelo local, opte por llama, que provavelmente ser√° compat√≠vel), os modelos testados at√© o momento incluem:
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (recomendado! modelo de interpreta√ß√£o de pap√©is)
* [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (recomendado! modelo de sugest√µes ricas)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)
* [THUDM/chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b) (devido √† nova formata√ß√£o de chamada do GLM4, os desenvolvedores n√£o conseguem manter todas as chamadas de grandes modelos locais, portanto, recomenda-se que todos utilizem o m√©todo ollama para chamadas locais!)

4. Download do modelo:
* [Link do Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), c√≥digo de extra√ß√£o: qyhu

## Download
Utilize um dos m√©todos abaixo para instalar
### M√©todo 1:
1. No [gerenciador comfyui](https://github.com/ltdrdata/ComfyUI-Manager), pesquise por `comfyui_LLM_party` e instale com um clique
2. Reinicie o comfyui
### M√©todo Dois:
1. Navegue at√© a pasta raiz do ComfyUI e acesse a subpasta `custom_nodes`.
2. Clone este reposit√≥rio: `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### M√©todo Tr√™s:
1. Clique em `CODE` no canto superior direito.
2. Clique em `download zip`.
3. Extraia o arquivo compactado baixado na subpasta `custom_nodes` da pasta raiz do ComfyUI.

## Implanta√ß√£o do Ambiente
1. Navegue at√© a pasta do projeto `comfyui_LLM_party`.
2. No terminal, digite `pip install -r requirements.txt` para implantar as bibliotecas de terceiros necess√°rias para este projeto no ambiente do ComfyUI. Verifique se voc√™ est√° instalando no ambiente do ComfyUI e fique atento a erros do `pip` no terminal.
3. Se voc√™ estiver usando o lan√ßador do ComfyUI, voc√™ precisar√° digitar no terminal `caminho na configura√ß√£o do lan√ßador\python_embeded\python.exe -m pip install -r requirements.txt` para realizar a instala√ß√£o. A pasta `python_embeded` geralmente est√° no mesmo n√≠vel da sua pasta `ComfyUI`.
4. Se voc√™ encontrar problemas de configura√ß√£o do ambiente, pode tentar usar as depend√™ncias do `requirements_fixed.txt`.
## Configura√ß√£o
* A linguagem pode ser configurada no `config.ini`, atualmente dispon√≠veis apenas o Chin√™s (zh_CN) e o Ingl√™s (en_US), sendo o padr√£o a l√≠ngua do seu sistema.
* Voc√™ pode configurar o APIKEY utilizando um dos m√©todos a seguir:
### M√©todo Um:
1. Abra o arquivo `config.ini` na pasta do projeto `comfyui_LLM_party`.
2. Insira seu `openai_api_key` e `base_url` no `config.ini`.
3. Se voc√™ estiver utilizando o modelo ollama, insira `http://127.0.0.1:11434/v1/` em `base_url`, coloque `ollama` em `openai_api_key` e insira o nome do seu modelo em `model_name`, por exemplo: llama3.
4. Se voc√™ deseja utilizar ferramentas de busca do Google ou Bing, insira seu `google_api_key`, `cse_id` ou `bing_api_key` no `config.ini`.
5. Se voc√™ pretende usar entrada de imagem para LLM, recomenda-se o uso do servi√ßo de hospedagem de imagens imgbb, insira seu `imgbb_api` no `config.ini`.
6. Cada modelo pode ser configurado separadamente no arquivo `config.ini`, utilizando como refer√™ncia o arquivo `config.ini.example`. Ap√≥s a configura√ß√£o, basta inserir `model_name` no n√≥.

### M√©todo Dois:
1. Abra a interface do comfyui.
2. Crie um n√≥ de modelo de linguagem grande (LLM) e insira diretamente seu `openai_api_key` e `base_url` no n√≥.
3. Se voc√™ estiver utilizando o modelo ollama, utilize o n√≥ LLM_api, insira `http://127.0.0.1:11434/v1/` em `base_url`, coloque `ollama` em `api_key` e insira o nome do seu modelo em `model_name`, por exemplo: llama3.
4. Se voc√™ deseja usar entrada de imagem para LLM, recomenda-se o uso do servi√ßo de hospedagem de imagens imgbb, insira seu `imgbb_api_key` no n√≥.
## Registro de Atualiza√ß√µes
1. Voc√™ pode clicar com o bot√£o direito na interface do comfyui e selecionar `llm` no menu do bot√£o direito para encontrar os n√≥s deste projeto. [Como usar os n√≥s](how_to_use_nodes_ZH.md)
2. Suporte para integra√ß√£o de API ou integra√ß√£o de grandes modelos locais. Implementa√ß√£o modular da funcionalidade de chamada de ferramentas. Ao preencher o base_url, insira um URL que termine com `/v1/`. Voc√™ pode usar [ollama](https://github.com/ollama/ollama) para gerenciar seus modelos e, em seguida, preencher `http://127.0.0.1:11434/v1/` no base_url, colocar ollama em api_key e o nome do seu modelo em model_name, por exemplo: llama3.
- Exemplo de fluxo de trabalho para integra√ß√£o de API: [start_with_LLM_api](workflow/start_with_LLM_api.json)
- Exemplo de fluxo de trabalho para integra√ß√£o de modelo local: [start_with_LLM_local](workflow/start_with_LLM_local.json)
- Exemplo de fluxo de trabalho para integra√ß√£o de ollama: [ollama](workflow/ollama.json)
3. Integra√ß√£o de reposit√≥rio local de conhecimento, suporte a RAG. Exemplo de fluxo de trabalho: [Busca RAG no Reposit√≥rio](workflow/Áü•ËØÜÂ∫ìRAGÊêúÁ¥¢.json)
4. Capacidade de chamar um interpretador de c√≥digo.
5. Capacidade de consulta online, suporte a pesquisa no Google. Exemplo de fluxo de trabalho: [Fluxo de Trabalho de Consulta de Filmes](workflow/ÁîµÂΩ±Êü•ËØ¢Â∑•‰ΩúÊµÅ.json)
6. Possibilidade de implementar declara√ß√µes condicionais no comfyui, permitindo classificar perguntas dos usu√°rios antes de responder de forma direcionada. Exemplo de fluxo de trabalho: [Atendimento ao Cliente Inteligente](workflow/Êô∫ËÉΩÂÆ¢Êúç.json)
7. Suporte para links de feedback de grandes modelos, permitindo que dois grandes modelos debate. Exemplo de fluxo de trabalho: [Debate sobre o Dilema do Trem](workflow/ÁîµËΩ¶ÈöæÈ¢òËæ©ËÆ∫Ëµõ.json)
8. Suporte para anexar qualquer m√°scara de personalidade, permitindo a personaliza√ß√£o de modelos de prompt.
9. Suporte para diversas chamadas de ferramentas; atualmente, foram desenvolvidas funcionalidades para verificar clima, hora, reposit√≥rio de conhecimento, execu√ß√£o de c√≥digo, pesquisa online e pesquisa em uma √∫nica p√°gina da web, entre outras.
10. Suporte para usar o LLM como um n√≥ de ferramenta. Exemplo de fluxo de trabalho: [LLM Matryoshka](workflow/LLMÂ•óÂ®É.json)
11. Suporte para desenvolvimento r√°pido de aplica√ß√µes web pr√≥prias atrav√©s de API + streamlit.
12. Adi√ß√£o de um n√≥ interpretador universal perigoso, permitindo que o grande modelo fa√ßa qualquer coisa.
13. Recomenda-se o uso do n√≥ de exibi√ß√£o de texto (show_text) no subdiret√≥rio de fun√ß√µes (function) do menu do bot√£o direito como sa√≠da do n√≥ LLM.
14. Suporte para a funcionalidade visual do GPT-4O! Exemplo de fluxo de trabalho: [GPT-4o](workflow/GPT-4o.json)  
15. Adicionado um intermedi√°rio de fluxo de trabalho, que permite que seu fluxo de trabalho chame outros fluxos de trabalho! Exemplo de fluxo de trabalho: [Chamar outro fluxo de trabalho](workflow/Ë∞ÉÁî®Âè¶‰∏Ä‰∏™Â∑•‰ΩúÊµÅ.json)  
16. Compat√≠vel com todos os modelos que possuem interfaces semelhantes √† openai, como: Tongyi Qianwen/qwen, Zhiyu Qingyan/GLM, deepseek, kimi/moonshot. Preencha o base_url, api_key e model_name desses modelos no n√≥ LLM para cham√°-los.  
17. Adicionado um carregador LVM, agora √© poss√≠vel chamar modelos LVM localmente, suportando o modelo [llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf); outros modelos LVM no formato GGUF devem teoricamente tamb√©m funcionar. O exemplo de fluxo de trabalho est√° aqui: [start_with_LVM.json](workflow/start_with_LVM.json).  
18. Foi escrito um arquivo `fastapi.py`, se voc√™ execut√°-lo diretamente, voc√™ ter√° uma interface openai em `http://127.0.0.1:8817/v1/`, qualquer aplicativo que possa chamar o GPT poder√° acessar seu fluxo de trabalho comfyui! Irei preparar um tutorial detalhado para demonstrar como operar isso~  
19. O carregador LLM e a cadeia LLM foram separados, permitindo que o carregamento do modelo e a configura√ß√£o do modelo sejam distintos, o que possibilita o compartilhamento de modelos entre diferentes n√≥s LLM!  
20. Atualmente, j√° s√£o suportados dispositivos macOS e mps! Agradecimentos a [bigcat88](https://github.com/bigcat88) por sua contribui√ß√£o!  
21. Agora √© poss√≠vel criar seu pr√≥prio jogo de novela interativa, com diferentes finais dependendo das escolhas dos usu√°rios! Exemplo de fluxo de trabalho: [Novela Interativa](workflow/‰∫íÂä®Â∞èËØ¥.json)  
22. Compat√≠vel com as funcionalidades de whisper e tts da openai, permitindo entrada e sa√≠da de voz. Exemplo de fluxo de trabalho: [Entrada de voz + Sa√≠da de voz](workflow/ËØ≠Èü≥ËæìÂÖ•+ËØ≠Èü≥ËæìÂá∫.json)  
23. Compat√≠vel com [Omost](https://github.com/lllyasviel/Omost)!!! Por favor, baixe [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) e experimente agora mesmo! Para refer√™ncia de fluxo de trabalho, consulte: [start_with_OMOST](workflow/start_with_OMOST.json)  
24. Adicionadas ferramentas LLM para enviar mensagens ao WeChat empresarial, DingTalk e Feishu, al√©m de fun√ß√µes externas que podem ser chamadas.  
25. Um novo iterador de texto foi adicionado, que pode gerar apenas uma parte dos caracteres a cada vez, dividindo o texto de forma segura com base no caractere de nova linha e no tamanho do bloco (chunk size), sem cortar o texto no meio. O chunk_overlap refere-se √† quantidade de caracteres que se sobrep√µem na divis√£o do texto. Isso permite a entrada em massa de textos muito longos, bastando clicar sem pensar, ou ativar a execu√ß√£o em loop no comfyui para que a execu√ß√£o seja feita automaticamente. Lembre-se de ativar o atributo is_locked para que, ao final da entrada, o fluxo de trabalho seja automaticamente bloqueado e n√£o continue a execu√ß√£o. Fluxo de trabalho de exemplo: [ÊñáÊú¨Ëø≠‰ª£ËæìÂÖ•](workflow/ÊñáÊú¨Ëø≠‰ª£ËæìÂÖ•.json)  
26. Adicionado o atributo model name no carregador LLM local e no carregador llava local; se estiver vazio, ser√° utilizado o caminho local de diversos tipos contido no n√≥. Se n√£o estiver vazio, ser√° utilizado o par√¢metro de caminho que voc√™ preencheu no `config.ini`. Se n√£o estiver vazio e n√£o estiver no `config.ini`, ser√° feito o download do Hugging Face ou carregado do diret√≥rio de modelos do Hugging Face. Se voc√™ deseja baixar do Hugging Face, preencha o atributo model name no formato: `THUDM/glm-4-9b-chat`. Aten√ß√£o! O modelo carregado dessa forma deve ser compat√≠vel com a biblioteca transformer.  
27. Adicionados n√≥s de an√°lise de arquivos JSON e n√≥s de obten√ß√£o de valores JSON, que permitem obter o valor de uma chave espec√≠fica de um arquivo ou texto. Agradecimentos a [guobalove](https://github.com/guobalove) pela contribui√ß√£o!
28. O c√≥digo de chamada de ferramentas foi aprimorado, agora LLMs que n√£o possuem a funcionalidade de chamada de ferramentas tamb√©m podem ativar o atributo is_tools_in_sys_prompt (LLM local n√£o precisa ser ativado por padr√£o, se adapta automaticamente). Quando ativado, as informa√ß√µes da ferramenta ser√£o adicionadas ao prompt do sistema, permitindo que o LLM fa√ßa chamadas de ferramentas. O artigo relacionado ao princ√≠pio de implementa√ß√£o: [Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning](https://arxiv.org/abs/2407.04997)
29. Foi criada a pasta custom_tool para armazenar o c√≥digo das ferramentas personalizadas, podendo ser consultado o c√≥digo na pasta [custom_tool](custom_tool). Basta colocar o c√≥digo da ferramenta personalizada na pasta custom_tool para que ela possa ser chamada no LLM.
30. Foi adicionada a ferramenta de gr√°fico de conhecimento, permitindo uma intera√ß√£o perfeita entre o LLM e o gr√°fico de conhecimento. O LLM pode modificar o gr√°fico de conhecimento com base na sua entrada e inferir para obter as respostas que voc√™ precisa. Para um fluxo de trabalho exemplo, consulte: [graphRAG_neo4j](workflow/graphRAG_neo4j.json)
31. Foi introduzida a funcionalidade de AI de personalidade, permitindo o desenvolvimento de sua pr√≥pria AI de namorada ou namorado sem necessidade de c√≥digo, com di√°logos infinitos, mem√≥ria permanente e personagens est√°veis. Para um fluxo de trabalho exemplo, consulte: [È∫¶Ê¥õËñá‰∫∫Ê†ºAI](workflow/È∫¶Ê¥õËñá‰∫∫Ê†ºAI.json)
32. Voc√™ pode usar esta m√°quina de ferramentas LLM para gerar automaticamente ferramentas LLM, salvando o c√≥digo da ferramenta gerada em um arquivo python e copiando o c√≥digo para a pasta custom_tool, criando assim um novo n√≥. Fluxo de trabalho exemplo: [LLMÂ∑•ÂÖ∑ÁîüÊàêÂô®](workflow/LLMÂ∑•ÂÖ∑Âà∂ÈÄ†Êú∫.json).
33. O suporte √† busca no duckduckgo foi adicionado, mas com grandes limita√ß√µes; parece que s√≥ √© poss√≠vel inserir palavras-chave em ingl√™s, e as palavras-chave n√£o podem conter m√∫ltiplos conceitos, com a vantagem de n√£o haver restri√ß√µes de qualquer API key.
34. Foi implementada a funcionalidade de chamada separada de m√∫ltiplos reposit√≥rios de conhecimento, permitindo especificar qual reposit√≥rio de conhecimento utilizar para responder perguntas dentro do prompt. Para um fluxo de trabalho exemplo, consulte: [Â§öÁü•ËØÜÂ∫ìÂàÜÂà´Ë∞ÉÁî®](workflow/Â§öÁü•ËØÜÂ∫ìÂàÜÂà´Ë∞ÉÁî®.json).
35. Suporte para que o LLM receba par√¢metros adicionais, incluindo json out e outros par√¢metros avan√ßados. Para um fluxo de trabalho exemplo, consulte: [LLMËæìÂÖ•È¢ùÂ§ñÂèÇÊï∞](workflow/LLMÈ¢ùÂ§ñÂèÇÊï∞eg_JSON_OUT.json). [Áî®json_outÂàÜÁ¶ªÊèêÁ§∫ËØç](workflow/Áî®json_outÂàÜÁ¶ªÊèêÁ§∫ËØç.json).
36. Foi adicionada a funcionalidade de conectar agentes ao Discord. (Ainda em teste)
37. Foi adicionada a funcionalidade de conectar agentes ao Feishu, agradecemos imensamente a contribui√ß√£o de [guobalove](https://github.com/guobalove)! Consulte o fluxo de trabalho [Rob√¥ Feishu](workflow/È£û‰π¶Êú∫Âô®‰∫∫.json).
38. Foi adicionado um n√≥ de chamada de API universal e uma grande quantidade de n√≥s auxiliares, utilizados para construir o corpo da solicita√ß√£o e capturar informa√ß√µes da resposta.
39. Foi adicionado um n√≥ de limpeza de modelo, que permite descarregar o LLM da mem√≥ria a qualquer momento!
40. Foi adicionado o n√≥ [chatTTS](https://github.com/2noise/ChatTTS), agradecemos imensamente a contribui√ß√£o de [guobalove](https://github.com/guobalove)! O par√¢metro `model_path` pode ser deixado vazio! Recomenda-se usar o modo HF para carregar o modelo, que ser√° baixado automaticamente do Hugging Face, sem necessidade de download manual; se usar o modo local, coloque as pastas `asset` e `config` do modelo no diret√≥rio raiz. [Endere√ßo do Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), c√≥digo de extra√ß√£o: qyhu; se usar o modo `custom`, coloque as pastas `asset` e `config` do modelo dentro de `model_path`.
## Pr√≥ximos passos:
1. Maior adapta√ß√£o de modelos, cobrindo pelo menos as principais interfaces de API de grandes modelos e chamadas locais de modelos de c√≥digo aberto populares, al√©m de mais adapta√ß√µes de modelos LVM. Atualmente, s√≥ adaptei a funcionalidade visual do GPT-4.
2. Mais maneiras de construir agentes inteligentes. At√© agora, completei o trabalho de importar um LLM como uma ferramenta para outro LLM, permitindo a constru√ß√£o radial de fluxos de trabalho LLM, onde um fluxo de trabalho √© importado como um n√≥ para outro fluxo de trabalho. No futuro, posso desenvolver funcionalidades ainda mais interessantes nessa √°rea.
3. Mais funcionalidades de automa√ß√£o. No futuro, lan√ßarei mais n√≥s que automaticamente enviam imagens, texto, v√≠deo e √°udio para outros aplicativos, al√©m de implementar n√≥s de escuta para permitir respostas autom√°ticas em redes sociais e f√≥runs populares.
4. Mais funcionalidades de gerenciamento de banco de dados de conhecimento. Atualmente, este projeto j√° suporta busca em arquivos locais e busca na web. No futuro, lan√ßarei busca em gr√°ficos de conhecimento e busca em mem√≥ria de longo prazo, permitindo que os agentes realizem um racioc√≠nio l√≥gico sobre conhecimentos especializados e retenham informa√ß√µes-chave em conversas com os usu√°rios.
5. Mais ferramentas e mais personas. Esta √°rea √© a mais f√°cil de desenvolver, mas tamb√©m a que mais precisa de acumula√ß√£o. Espero que, no futuro, este projeto possa ter, assim como o comfyui, uma variedade de n√≥s personaliz√°veis, oferecendo numerosas ferramentas e personas.

## Isen√ß√£o de responsabilidade:
Este projeto de c√≥digo aberto e seu conte√∫do (doravante denominado "projeto") s√£o fornecidos apenas para fins de refer√™ncia e n√£o implicam em qualquer garantia expressa ou impl√≠cita. Os contribuintes do projeto n√£o assumem qualquer responsabilidade pela integridade, precis√£o, confiabilidade ou adequa√ß√£o do projeto. Qualquer a√ß√£o que dependa do conte√∫do do projeto deve ser realizada por sua pr√≥pria conta e risco. Em nenhuma circunst√¢ncia os contribuintes do projeto ser√£o respons√°veis por quaisquer perdas ou danos indiretos, especiais ou consequenciais decorrentes do uso do conte√∫do do projeto.
## Agradecimentos Especiais
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/HuangYuChuh">
  <img src="https://avatars.githubusercontent.com/u/167663109?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Agradecimentos
Alguns n√≥s deste projeto foram inspirados pelos seguintes projetos, agradecemos suas contribui√ß√µes √† comunidade de c√≥digo aberto!
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)

## Suporte:

### Junte-se √† Comunidade
Se houver problemas com o plugin ou se voc√™ tiver outras d√∫vidas, sinta-se √† vontade para se juntar √† nossa comunidade.

1. Grupo QQ: `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/QÁæ§.jpg" style="width: 48%;" />
</div>

2. Grupo WeChat: `Choo-Yong` (adicione o assistente no WeChat antes de entrar no grupo)

3. Discord: [discordÈìæÊé•](https://discord.gg/gxrQAYy6)

### Siga-nos
1. Se desejar acompanhar as √∫ltimas funcionalidades deste projeto, fique √† vontade para seguir nossa conta no Bilibili: [Ê¥æÂØπ‰∏ªÊåÅBBÊú∫](https://space.bilibili.com/26978344)
2. A conta OpenArt atualiza continuamente os workflows de festa mais √∫teis: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

### Apoio √† doa√ß√£o
Se meu trabalho lhe trouxe valor, considere me convidar para um caf√©! Seu apoio n√£o apenas energiza o projeto, mas tamb√©m aquece o cora√ß√£o do criador.‚òïüíñ Cada x√≠cara conta!
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## Hist√≥rico de Estrelas

[![Star History Chart](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
