![Imagem](img/Â∞ÅÈù¢.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">bilibili</a> ¬∑
  <a href="https://www.youtube.com/@comfyui-LLM-party">youtube</a> ¬∑
  <a href="https://github.com/heshengtao/Let-LLM-party">Tutorial em texto</a> ¬∑
  <a href="https://pan.quark.cn/s/190b41f3bbdb">Endere√ßo do disco em nuvem</a> ¬∑
  <a href="img/QÁæ§.jpg">Grupo do QQ</a> ¬∑
  <a href="https://discord.gg/f2dsAKKr2V">Discord</a> ¬∑
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">Sobre n√≥s</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/–†—É—Å—Å–∫–∏–π-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Fran√ßais-d9d9d9"></a> 
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/ÿßŸÑÿπÿ±ÿ®Ÿäÿ©-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Espa√±ol-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Portugu√™s-d9d9d9"></a>
</div>

####

Comfyui_llm_party tem como objetivo, com base na [comfyui](https://github.com/comfyanonymous/ComfyUI), uma interface de usu√°rio extremamente minimalista, desenvolver um conjunto completo de biblioteca de n√≥s para a constru√ß√£o de fluxos de trabalho de LLM. Isso permitir√° que os usu√°rios construam seus pr√≥prios fluxos de trabalho de LLM de forma mais r√°pida e conveniente, al√©m de integrar facilmente em seus pr√≥prios fluxos de trabalho de imagem.

## Demonstra√ß√£o de resultados
https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6

## Vis√£o Geral do Projeto
ComfyUI LLM Party permite desde a chamada de m√∫ltiplas ferramentas LLM, configura√ß√£o r√°pida de personagens para construir seu assistente de IA personalizado, at√© a aplica√ß√£o de Word Vector RAG e GraphRAG para gerenciamento local de bancos de dados de conhecimento da ind√∫stria; desde um pipeline de agentes inteligentes simples, at√© a constru√ß√£o de modos de intera√ß√£o complexos entre agentes inteligentes em padr√£o radial e circular; desde a necessidade de usu√°rios individuais de integrar seus aplicativos sociais (QQ, Feishu, Discord), at√© o fluxo de trabalho tudo-em-um LLM+TTS+ComfyUI que os trabalhadores de streaming precisam; desde o primeiro aplicativo LLM que alunos comuns precisam para um f√°cil in√≠cio, at√© as diversas interfaces de ajuste de par√¢metros frequentemente utilizadas por pesquisadores e a adapta√ß√£o de modelos. Tudo isso, voc√™ pode encontrar respostas no ComfyUI LLM Party.

## In√≠cio R√°pido
0. Se voc√™ nunca usou o ComfyUI e encontrou alguns problemas de depend√™ncia ao instalar o LLM party no ComfyUI, clique [aqui](https://drive.google.com/file/d/1T9C7gEbd-w_zf9GqZO1VeI3z8ek8clpX/view?usp=sharing) para baixar o pacote port√°til do ComfyUI **windows** que inclui o LLM party. Aten√ß√£o! Este pacote port√°til cont√©m apenas os dois plugins: party e manager, e √© exclusivo para sistemas Windows.(Se voc√™ precisar instalar o LLM party em um comfyui existente, esta etapa pode ser pulada.)
1. Arraste os seguintes fluxos de trabalho para o seu comfyui e use [comfyui-Manager](https://github.com/ltdrdata/ComfyUI-Manager) para instalar os n√≥s ausentes.
  - Use a API para chamar LLM: [start_with_LLM_api](workflow/start_with_LLM_api.json)
  - Chamando o LLM com aisuite: [iniciar_com_aisuite](workflow/start_with_aisuite.json)
  - Gerencie LLM local com ollama: [start_with_Ollama](workflow/ollama.json)
  - Use LLM local em formato distribu√≠do: [start_with_LLM_local](workflow/start_with_LLM_local.json)
  - Use LLM local em formato GGUF: [start_with_LLM_GGUF](workflow/start_with_GGUF.json)
  - Use VLM local em formato distribu√≠do: [start_with_VLM_local](workflow/start_with_VLM_local.json) Atualmente, j√° √© suportado [Llama-3.2-Vision](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)/[Qwen/Qwen2.5-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)/[deepseek-ai/Janus-Pro](https://huggingface.co/deepseek-ai/Janus-Pro-1B)
  - Use VLM local em formato GGUF: [start_with_VLM_GGUF](workflow/start_with_llava.json)
  - Utilizar a API para invocar LLM e gerar palavras-chave para SD, bem como criar imagens: [start_with_VLM_API_for_SD](workflow/start_with_VLM_API_for_SD.json)
  - Utilizar ollama para invocar minicpm e gerar palavras-chave para SD, bem como criar imagens: [start_with_ollama_minicpm_for_SD](workflow/start_with_ollama_minicpm_for_SD.json)
  - Utilizar o qwen-vl local para gerar palavras-chave para SD e criar imagens: [start_with_qwen_vl_local_for_SD](workflow/start_with_qwen_vl_local_for_SD.json)
2. Se voc√™ estiver usando a API, preencha seu `base_url` (pode ser uma API de retransmiss√£o, certifique-se de que termine com `/v1/`) e `api_key` no n√≥ de carregamento da API LLM. Exemplo: `https://api.openai.com/v1/`
3. Se voc√™ estiver usando ollama, ative a op√ß√£o `is_ollama` no n√≥ de carregamento da API LLM, n√£o √© necess√°rio preencher `base_url` e `api_key`.
4. Se voc√™ estiver usando um modelo local, preencha o caminho do seu modelo no n√≥ de carregamento do modelo local, por exemplo: `E:\model\Llama-3.2-1B-Instruct`. Voc√™ tamb√©m pode preencher o ID do reposit√≥rio do modelo no Huggingface no n√≥ de carregamento do modelo local, por exemplo: `lllyasviel/omost-llama-3-8b-4bits`.
5. Devido ao alto limiar de uso deste projeto, mesmo que voc√™ escolha o in√≠cio r√°pido, espero que possa ler pacientemente a p√°gina inicial do projeto.

## Atualiza√ß√µes Recentes
1. O n√≥do LLM API agora suporta o modo de sa√≠da em streaming, que exibir√° na consola o texto retornado pela API em tempo real, permitindo que voc√™ veja a sa√≠da da API sem precisar esperar que toda a solicita√ß√£o seja conclu√≠da.
2. Foi adicionada a sa√≠da reasoning_content ao n√≥do LLM API, que permite separar automaticamente o racioc√≠nio e a resposta do modelo R1.
3. Uma nova ramifica√ß√£o do reposit√≥rio chamada only_api foi criada, a qual cont√©m apenas a parte da chamada √† API. Isso facilita para os usu√°rios que precisam apenas fazer chamadas √† API, basta usar o comando `git clone -b only_api https://github.com/heshengtao/comfyui_LLM_party.git` na pasta `custom tool` do `comfyui`, e em seguida seguir o plano de implementa√ß√£o do ambiente na p√°gina principal deste projeto para que possam utilizar essa ramifica√ß√£o. Aten√ß√£o! Se precisar garantir que n√£o haja outra pasta chamada `comfyui_LLM_party` na pasta `custom tool`.
1. O n√≥ de carregador local do VLM j√° suporta [deepseek-ai/Janus-Pro](https://huggingface.co/deepseek-ai/Janus-Pro-1B), fluxo de trabalho de exemplo: [Janus-Pro](workflow/deepseek-janus-pro.json)
1. O n√≥ de carregador local VLM j√° √© compat√≠vel com [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct), mas voc√™ precisa atualizar o transformer para a vers√£o mais recente (```pip install -U transformers```), fluxo de trabalho de exemplo: [qwen-vl](workflow/qwen-vl.json)  
1. Foi adicionado um novo n√≥ de hospedagem de imagens, que atualmente suporta a hospedagem de imagens de https://sm.ms (o dom√≠nio na regi√£o da China √© https://smms.app) e https://imgbb.com. No futuro, mais servi√ßos de hospedagem de imagens ser√£o suportados. Exemplo de fluxo de trabalho: [Hospedagem de Imagens](workflow/ÂõæÂ∫ä.json)
1. ~~O servi√ßo de hospedagem de imagens imgbb, que √© compat√≠vel por padr√£o com party, foi atualizado para o dom√≠nio [imgbb](https://imgbb.io). O servi√ßo anterior n√£o era amig√°vel para usu√°rios da China continental, por isso foi alterado.~~ Sinto muito informar que o servi√ßo API de hospedagem de imagens em https://imgbb.io parece ter sido descontinuado, portanto, o c√≥digo voltou para o original https://imgbb.com. Agrade√ßo a compreens√£o de todos. No futuro, atualizarei um n√≥ que suporte mais servi√ßos de hospedagem de imagens.
1. A ferramenta [MCP](https://modelcontextprotocol.io/introduction) foi atualizada, voc√™ pode modificar a configura√ß√£o no arquivo '[mcp_config.json](mcp_config.json)' na pasta do projeto party para ajustar o servidor MCP ao qual deseja se conectar. Voc√™ pode encontrar aqui v√°rios par√¢metros de configura√ß√£o dos servidores MCP que deseja adicionar: [modelcontextprotocol/servers](https://github.com/modelcontextprotocol/servers). A configura√ß√£o padr√£o deste projeto √© o servidor Everything, um servidor utilizado para testar se o servidor MCP opera corretamente. Fluxo de trabalho de refer√™ncia: [start_with_MCP](workflow/start_with_MCP.json). Nota para desenvolvedores: o n√≥ da ferramenta MCP pode se conectar ao servidor MCP configurado e, em seguida, converter as ferramentas do servidor em ferramentas que o LLM pode usar diretamente. Ao configurar diferentes servidores locais ou em nuvem, voc√™ pode experimentar todas as ferramentas LLM dispon√≠veis no mundo.

## Instru√ß√µes de Uso
1. Para instru√ß√µes sobre o uso dos n√≥s, consulte: [ÊÄé‰πà‰ΩøÁî®ËäÇÁÇπ](https://github.com/heshengtao/Let-LLM-party)

2. Se houver problemas com o plugin ou se voc√™ tiver outras d√∫vidas, sinta-se √† vontade para entrar no grupo QQ: [931057213](img/QÁæ§.jpg) |discordÔºö[discord](https://discord.gg/f2dsAKKr2V).

4. Para mais fluxos de trabalho, voc√™ pode consultar a pasta [workflow](workflow).

## Tutoriais em v√≠deo
<a href="https://space.bilibili.com/26978344">
  <img src="img/B.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://www.youtube.com/@comfyui-LLM-party">
  <img src="img/YT.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>

## Suporte ao Modelo
1. Suporte a todas as chamadas de API no formato openai (combinado com [oneapi](https://github.com/songquanpeng/one-api), √© poss√≠vel chamar quase todas as APIs LLM, al√©m de suportar todas as APIs de encaminhamento), a escolha do base_url deve seguir o exemplo em [config.ini.example](config.ini.example), atualmente testados incluem:
* [openai](https://platform.openai.com/docs/api-reference/chat/create) (Perfeitamente compat√≠vel com todos os modelos OpenAI, incluindo as s√©ries 4o e o1!)
* [ollama](https://github.com/ollama/ollama) (Recomendado! Se voc√™ estiver chamando localmente, √© altamente recomend√°vel usar o m√©todo ollama para hospedar seu modelo local!)
* [Azure OpenAI](https://azure.microsoft.com/zh-cn/products/ai-services/openai-service/)
* [llama.cpp](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#web-server) (Recomendado! Se voc√™ quiser usar o modelo no formato gguf local, pode usar a API do projeto llama.cpp para acessar este projeto!)
* [Grok](https://x.ai/api)
* [ÈÄö‰πâÂçÉÈóÆ/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [Êô∫Ë∞±Ê∏ÖË®Ä/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [doubao](https://www.volcengine.com/docs/82379/1263482)
* [ËÆØÈ£ûÊòüÁÅ´/spark](https://xinghuo.xfyun.cn/sparkapi?scr=price)
* [G√™meos](https://developers.googleblog.com/zh-hans/gemini-is-now-accessible-from-the-openai-library/)(O antigo n√≥ do carregador de API do G√™meos foi descontinuado na nova vers√£o, por favor utilize o n√≥ do carregador de API LLM, selecionando o base_url: https://generativelanguage.googleapis.com/v1beta/openai/)

2. Suporte a todas as chamadas de API compat√≠veis com [aisuite](https://github.com/andrewyng/aisuite):
* [anthropic/claude](https://www.anthropic.com/)
* [aws](https://docs.aws.amazon.com/solutions/latest/generative-ai-application-builder-on-aws/api-reference.html)
* [v√©rtice](https://cloud.google.com/vertex-ai/docs/reference/rest)
* [huggingface](https://huggingface.co/)

3. Compat√≠vel com a maioria dos modelos locais na biblioteca transformer (o tipo de modelo no n√≥ da cadeia de modelos LLM local foi alterado para LLM, VLM-GGUF e LLM-GGUF, correspondendo ao carregamento direto de modelos LLM, carregamento de modelos VLM e carregamento de modelos LLM no formato GGUF). Se o seu modelo LLM no formato VLM ou GGUF relatar um erro, fa√ßa o download da vers√£o mais recente do llama-cpp-python em [llama-cpp-python](https://github.com/abetlen/llama-cpp-python/releases). Os modelos atualmente testados incluem:
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (recomendado! modelo de interpreta√ß√£o de pap√©is)
* [lllyasviel/omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (recomendado! modelo de sugest√µes ricas)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [openbmb/MiniCPM-V-2_6-gguf](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf/tree/main)
* [lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main)
* [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)
* [Qwen/Qwen2.5-VL-3B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)
* [deepseek-ai/Janus-Pro](https://huggingface.co/deepseek-ai/Janus-Pro-1B)

4. Download do modelo:
* [Endere√ßo da nuvem Quark](https://pan.quark.cn/s/190b41f3bbdb)
* [Link do Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), c√≥digo de extra√ß√£o: qyhu

## Download
Utilize um dos m√©todos abaixo para instalar
### M√©todo 1:
1. No [gerenciador comfyui](https://github.com/ltdrdata/ComfyUI-Manager), pesquise por `comfyui_LLM_party` e instale com um clique
2. Reinicie o comfyui
### M√©todo Dois:
1. Navegue at√© a pasta raiz do ComfyUI e acesse a subpasta `custom_nodes`.
2. Clone este reposit√≥rio: `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### M√©todo Tr√™s:
1. Clique em `CODE` no canto superior direito.
2. Clique em `download zip`.
3. Extraia o arquivo compactado baixado na subpasta `custom_nodes` da pasta raiz do ComfyUI.

## Implanta√ß√£o do Ambiente
1. Navegue at√© a pasta do projeto `comfyui_LLM_party`.
2. No terminal, digite `pip install -r requirements.txt` para implantar as bibliotecas de terceiros necess√°rias para este projeto no ambiente do ComfyUI. Verifique se voc√™ est√° instalando no ambiente do ComfyUI e fique atento a erros do `pip` no terminal.
3. Se voc√™ estiver usando o lan√ßador do ComfyUI, voc√™ precisar√° digitar no terminal `caminho na configura√ß√£o do lan√ßador\python_embeded\python.exe -m pip install -r requirements.txt` para realizar a instala√ß√£o. A pasta `python_embeded` geralmente est√° no mesmo n√≠vel da sua pasta `ComfyUI`.
4. Se voc√™ encontrar problemas de configura√ß√£o do ambiente, pode tentar usar as depend√™ncias do `requirements_fixed.txt`.
## Configura√ß√£o
* √â poss√≠vel configurar o idioma no `config.ini`, atualmente existem apenas duas op√ß√µes: chin√™s (zh_CN) e ingl√™s (en_US), sendo o idioma padr√£o o do seu sistema.
* √â poss√≠vel configurar no `config.ini` se deseja uma instala√ß√£o r√°pida; `fast_installed` est√° configurado por padr√£o como `False`. Se voc√™ n√£o precisa utilizar o modelo GGUF, pode defini-lo como `True`.
* Voc√™ pode configurar o APIKEY utilizando um dos m√©todos a seguir:
### M√©todo Um:
1. Abra o arquivo `config.ini` na pasta do projeto `comfyui_LLM_party`.
2. Insira seu `openai_api_key` e `base_url` no `config.ini`.
3. Se voc√™ estiver utilizando o modelo ollama, insira `http://127.0.0.1:11434/v1/` em `base_url`, coloque `ollama` em `openai_api_key` e insira o nome do seu modelo em `model_name`, por exemplo: llama3.
4. Se voc√™ deseja utilizar ferramentas de busca do Google ou Bing, insira seu `google_api_key`, `cse_id` ou `bing_api_key` no `config.ini`.
5. Se voc√™ pretende usar entrada de imagem para LLM, recomenda-se o uso do servi√ßo de hospedagem de imagens imgbb, insira seu `imgbb_api` no `config.ini`.
6. Cada modelo pode ser configurado separadamente no arquivo `config.ini`, utilizando como refer√™ncia o arquivo `config.ini.example`. Ap√≥s a configura√ß√£o, basta inserir `model_name` no n√≥.

### M√©todo Dois:
1. Abra a interface do comfyui.
2. Crie um n√≥ de modelo de linguagem grande (LLM) e insira diretamente seu `openai_api_key` e `base_url` no n√≥.
3. Se voc√™ estiver utilizando o modelo ollama, utilize o n√≥ LLM_api, insira `http://127.0.0.1:11434/v1/` em `base_url`, coloque `ollama` em `api_key` e insira o nome do seu modelo em `model_name`, por exemplo: llama3.
4. Se voc√™ deseja usar entrada de imagem para LLM, recomenda-se o uso do servi√ßo de hospedagem de imagens imgbb, insira seu `imgbb_api_key` no n√≥.

## Registro de Atualiza√ß√µes
**[Click here](change_log.md)**

## Pr√≥ximos passos:
1. Mais adapta√ß√µes de modelos;
2. Mais formas de construir agentes;
3. Mais recursos de automa√ß√£o;
4. Mais recursos de gest√£o de base de conhecimento;
5. Mais ferramentas, mais personas.

## Isen√ß√£o de responsabilidade:
Este projeto de c√≥digo aberto e seu conte√∫do (doravante denominado "projeto") s√£o fornecidos apenas para fins de refer√™ncia e n√£o implicam em qualquer garantia expressa ou impl√≠cita. Os contribuintes do projeto n√£o assumem qualquer responsabilidade pela integridade, precis√£o, confiabilidade ou adequa√ß√£o do projeto. Qualquer a√ß√£o que dependa do conte√∫do do projeto deve ser realizada por sua pr√≥pria conta e risco. Em nenhuma circunst√¢ncia os contribuintes do projeto ser√£o respons√°veis por quaisquer perdas ou danos indiretos, especiais ou consequenciais decorrentes do uso do conte√∫do do projeto.
## Agradecimentos Especiais
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/SpenserCai">
  <img src="https://avatars.githubusercontent.com/u/25168945?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Agradecimentos
Alguns n√≥s deste projeto foram inspirados pelos seguintes projetos, agradecemos suas contribui√ß√µes √† comunidade de c√≥digo aberto!
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)

## Suporte:

### Junte-se √† Comunidade
Se houver problemas com o plugin ou se voc√™ tiver outras d√∫vidas, sinta-se √† vontade para se juntar √† nossa comunidade.

1. Grupo QQ: `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/QÁæ§.jpg" style="width: 48%;" />
</div>

2. Grupo WeChat: `we_glm` (adicione o assistente no WeChat antes de entrar no grupo)

3. Discord: [discordÈìæÊé•](https://discord.gg/f2dsAKKr2V)

### Siga-nos
1. Se desejar acompanhar as √∫ltimas funcionalidades deste projeto, fique √† vontade para seguir nossa conta no Bilibili: [Ê¥æÈÖ±](https://space.bilibili.com/26978344)
2. [youtube@comfyui-LLM-party](https://www.youtube.com/@comfyui-LLM-party)

### Apoio √† doa√ß√£o
Se meu trabalho lhe trouxe valor, considere me convidar para um caf√©! Seu apoio n√£o apenas energiza o projeto, mas tamb√©m aquece o cora√ß√£o do criador.‚òïüíñ Cada x√≠cara conta!
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## Hist√≥rico de Estrelas

[![Star History Chart](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
