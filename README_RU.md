![图片](img/封面.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">bilibili</a> ·
  <a href="https://www.youtube.com/@comfyui-LLM-party">youtube</a> ·
  <a href="https://github.com/heshengtao/Let-LLM-party">Текстовые инструкции</a> ·
  <a href="workflow_tutorial/">Инструкции по работе с потоками</a> ·
  <a href="https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu">Ссылка на Baidu Pan</a> ·
  <a href="img/Q群.jpg">QQ группа</a> ·
  <a href="https://discord.gg/f2dsAKKr2V">Discord</a> ·
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">О нас</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/简体中文-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/Русский-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Français-d9d9d9"></a> 
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/日本語-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/한국어-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/العربية-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Español-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Português-d9d9d9"></a>
</div>

####

Comfyui_llm_party стремится на основе [comfyui](https://github.com/comfyanonymous/ComfyUI) создать полностью укомплектованную библиотеку узлов для построения рабочих потоков LLM, используя исключительно простой интерфейс. Это позволит пользователям быстрее и удобнее создавать свои рабочие потоки LLM и интегрировать их в собственные графические рабочие процессы.

## Демонстрация эффектов
https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6

## Обзор проекта
ComfyUI LLM Party предлагает от самых основ LLM, включая многопрофильное использование инструментов, быструю настройку индивидуального AI-ассистента, до внедрения в отрасль векторных представлений слов (RAG) и GraphRAG для локального управления знаниями в отрасли. От простых потоков агентов до построения сложных моделей взаимодействия между агентами, включая радиальные и кольцевые модели взаимодействия; от необходимости индивидуальных пользователей интегрировать свои социальные приложения (QQ, Feishu, Discord), до комплексного рабочего процесса для стримеров, который объединяет LLM, TTS и ComfyUI; от простого первого опыта работы с LLM, необходимого обычным студентам, до интерфейсов настройки параметров, часто используемых исследователями и адаптации моделей. Все это вы сможете найти в ComfyUI LLM Party.

## Быстрый старт
0. Если вы никогда не использовали ComfyUI и столкнулись с некоторыми проблемами зависимости при установке LLM party в ComfyUI, пожалуйста, нажмите [здесь](https://drive.google.com/file/d/1NJSpwEL3FqroKVv5UsrVY3YbCG-9YWmt/view?usp=sharing), чтобы загрузить **Windows** портативный пакет, который включает в себя LLM party. Обратите внимание, что этот портативный пакет содержит только плагины party и manager и исключительно совместим с операционной системой Windows.
1. Перетащите следующие рабочие процессы в ваш comfyui, затем используйте [comfyui-Manager](https://github.com/ltdrdata/ComfyUI-Manager) для установки недостающих узлов.
  - Используйте API для вызова LLM: [start_with_LLM_api](workflow/start_with_LLM_api.json)
  - Управляйте локальным LLM с помощью ollama: [start_with_Ollama](workflow/ollama.json)
  - Используйте локальный LLM в распределенном формате: [start_with_LLM_local](workflow/start_with_LLM_local.json)
  - Используйте локальный LLM в формате GGUF: [start_with_LLM_GGUF](workflow/start_with_GGUF.json)
  - Используйте локальный VLM в распределенном формате: [start_with_VLM_local](https://github.com/heshengtao/comfyui_LLM_party/blob/main/workflow_tutorial/LLM_Party%20for%20Llama3.2%20-Vision%EF%BC%88%E5%B8%A6%E8%AE%B0%E5%BF%86%EF%BC%89.json) (тестирование, в настоящее время поддерживает только [Llama-3.2-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct))
  - Используйте локальный VLM в формате GGUF: [start_with_VLM_GGUF](workflow/start_with_llava.json)
2. Если вы используете API, заполните `base_url` (это может быть промежуточный API, убедитесь, что он заканчивается на `/v1/`), например: `https://api.openai.com/v1/` и `api_key` в узле загрузчика API LLM.
3. Если вы используете ollama, включите опцию `is_ollama` в узле загрузчика API LLM, не нужно заполнять `base_url` и `api_key`.
4. Если вы используете локальную модель, заполните путь к вашей модели в узле загрузчика локальной модели, например: `E:\model\Llama-3.2-1B-Instruct`. Вы также можете заполнить идентификатор репозитория модели Huggingface в узле загрузчика локальной модели, например: `lllyasviel/omost-llama-3-8b-4bits`.
5. Из-за высокого порога использования этого проекта, даже если вы выбрали быстрый старт, я надеюсь, что вы терпеливо прочитаете домашнюю страницу проекта.

## Последние обновления
1. Добавлен инструмент для чтения локальных файлов. В отличие от предыдущего инструмента управления локальными файлами в ComfyUI LLM Mafia, этот инструмент может читать только файлы или дерево файлов в определенной папке, что обеспечивает большую безопасность.
1. Создал ветвь от [chatgpt-on-wechat](https://github.com/zhayujie/chatgpt-on-wechat), создав новый репозиторий [party-on-wechat](https://github.com/heshengtao/party-on-wechat). Методы установки и использования совпадают с оригинальным проектом, не требуется дополнительная настройка, достаточно запустить FastAPI для party. По умолчанию используется рабочий процесс wx_api и поддерживается вывод изображений. Проект будет постепенно обновляться для обеспечения плавного опыта использования party в WeChat.
2. Добавлен узел маски In-Context-LoRA, используемый для создания последовательных [In-Context-LoRA](https://github.com/ali-vilab/In-Context-LoRA/tree/main) подсказок.
1. Мы добавили фронтенд компонент, функции которого располагаются слева направо следующим образом:
  - Сохраняет ваш ключ API и Base URL в файл `config.ini`. Когда вы используете функцию `fix node` для загрузчика API LLM, он автоматически читает измененные вами ключ API и Base URL из файла `config.ini`.
  - Запускает сервис FastAPI, который может использоваться для вызова ваших рабочих процессов ComfyUI. Если вы запустите его напрямую, вы получите интерфейс OpenAI на `http://127.0.0.1:8817/v1/`. Вам нужно подключить начало и конец вашего рабочего процесса к "началу рабочего процесса" и "концу рабочего процесса", а затем сохранить его в формате API в папке `workflow_api`. Затем в любом фронтенде, который может вызывать интерфейс OpenAI, вы должны ввести `model name=<название вашего рабочего процесса без расширения .json>`, `Base URL=http://127.0.0.1:8817/v1/`, ключ API можно заполнить любым значением.
  - Запускает приложение Streamlit, процесс сохранения рабочего процесса аналогичен вышеописанному. Вы можете выбрать сохраненный рабочий процесс в разделе "настройки" приложения Streamlit и взаимодействовать с агентом вашего рабочего процесса в разделе "чат".
  - "О нас", где представлено описание проекта.
2. Узел автоматического получения списка имен моделей был удален и заменен простым узлом загрузчика API LLM, который автоматически получает список имен моделей из конфигурации в вашем файле config.ini. Вам нужно просто выбрать имя, чтобы загрузить модель. Кроме того, были обновлены простые узлы загрузчика LLM, LLM-GGUF, VLM, VLM-GGUF и LLM lora. Все они автоматически читают пути к моделям из папки model в папке party, что упрощает загрузку различных локальных моделей.
1. Теперь LLM могут динамически загружать lora, как SD и FLUX. Вы можете объединить несколько lora, чтобы загрузить больше lora на тот же LLM. Пример рабочего процесса: [start_with_LLM_LORA](workflow/LLM_lora.json).
1. Добавлен инструмент [searxng](https://github.com/searxng/searxng), который может агрегировать поиск по всему интернету. Perplexica также зависит от этого агрегированного поискового инструмента, что означает, что вы можете установить Perplexica на вечеринке. Вы можете развернуть публичный образ searxng/searxng в Docker, затем использовать команду `docker run -d -p 8080:8080 searxng/searxng` для его запуска, и затем использовать `http://localhost:8080` для его доступа. Вы можете ввести URL `http://localhost:8080` в инструмент searxng в party, и тогда searxng будет использоваться как один из инструментов LLM.
1. **Масштабное обновление!!!** Теперь вы можете инкапсулировать любой рабочий процесс ComfyUI в узел инструмента LLM. Вы можете заставить ваш LLM одновременно управлять несколькими рабочими процессами ComfyUI. Когда вы хотите, чтобы он выполнил некоторые задачи, он может выбрать соответствующий рабочий процесс ComfyUI на основе вашего запроса, выполнить вашу задачу и вернуть вам результат. Пример рабочего процесса: [comfyui_workflows_tool](workflow/把任意workflow当作LLM_tool.json). Конкретные шаги следующие:
   - Сначала подключите интерфейс ввода текста рабочего процесса, который вы хотите инкапсулировать как инструмент, к выходу "user_prompt" узла "Начать рабочий процесс". Это место, куда передается запрос при вызове инструмента LLM.
   - Подключите места, где вы хотите выводить текст и изображения, к соответствующим входным позициям узла "Завершить рабочий процесс".
   - Сохраните этот рабочий процесс как API (вам нужно включить режим разработчика в настройках, чтобы увидеть эту кнопку).
   - Сохраните этот рабочий процесс в папку workflow_api этого проекта.
   - Перезапустите ComfyUI и создайте простой рабочий процесс LLM, например: [start_with_LLM_api](workflow/start_with_LLM_api.json).
   - Добавьте узел "Инструмент рабочего процесса" к этому узлу LLM и подключите его к входу инструмента узла LLM.
   - В узле "Инструмент рабочего процесса" напишите имя файла рабочего процесса, который вы хотите вызвать, в первое поле ввода, например: draw.json. Вы можете написать несколько имен файлов рабочего процесса. Во втором поле ввода напишите функцию каждого рабочего процесса, чтобы LLM понял, как использовать эти рабочие процессы.
   - Запустите его, чтобы увидеть, как LLM вызывает ваш инкапсулированный рабочий процесс и возвращает вам результат. Если возвращается изображение, подключите узел "Предварительный просмотр изображения" к выходу изображения узла LLM, чтобы просмотреть сгенерированное изображение. Внимание! Этот метод вызывает новый ComfyUI на вашем порту 8190, пожалуйста, не занимайте этот порт. На системах Windows и Mac будет открыт новый терминал, пожалуйста, не закрывайте его. В системе Linux используется процесс screen для достижения этого, когда вам не нужно его использовать, закройте этот процесс screen, иначе он всегда будет занимать ваш порт.

![workflow_tool](img/workflowtool.png)

## Инструкция по использованию
1. Пожалуйста, обратитесь к инструкции по использованию узлов: [怎么使用节点](https://github.com/heshengtao/Let-LLM-party)

2. Если у вас возникли проблемы с плагином или у вас есть другие вопросы, присоединяйтесь к нашей группе QQ: [931057213](img/Q群.jpg) |discord：[discord](https://discord.gg/f2dsAKKr2V).
3. Пожалуйста, обратитесь к [учебнику по рабочим процессам](workflow_tutorial/), благодарим за вклад [HuangYuChuh](https://github.com/HuangYuChuh)!

4. Учётная запись для высокоуровневых игровых рабочих процессов: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

4. Дополнительные рабочие процессы можно найти в папке [workflow](workflow).

## Видеоуроки
<a href="https://space.bilibili.com/26978344">
  <img src="img/B.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://www.youtube.com/@comfyui-LLM-party">
  <img src="img/YT.png" width="100" height="100" style="border-radius: 80%; overflow: hidden;" alt="octocat"/>
</a>

## Поддержка моделей
1. Поддержка всех API-вызовов формата OpenAI (в сочетании с [oneapi](https://github.com/songquanpeng/one-api) можно вызывать практически все API LLM, также поддерживаются все промежуточные API). Выбор base_url можно найти в [config.ini.example](config.ini.example), на данный момент протестированы следующие:
* [openai](https://platform.openai.com/docs/api-reference/chat/create) (Идеально совместим со всеми моделями OpenAI, включая серии 4o и o1!)
* [ollama](https://github.com/ollama/ollama) (Рекомендуется! Если вы вызываете локально, настоятельно рекомендуется использовать метод ollama для размещения вашей локальной модели!)
* [Azure OpenAI](https://azure.microsoft.com/zh-cn/products/ai-services/openai-service/)
* [llama.cpp](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#web-server) (Рекомендуется! Если вы хотите использовать локальную модель формата gguf, вы можете использовать API проекта llama.cpp для доступа к этому проекту!)
* [Grok](https://x.ai/api)
* [通义千问/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [智谱清言/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [doubao](https://www.volcengine.com/docs/82379/1263482)
* [讯飞星火/spark](https://xinghuo.xfyun.cn/sparkapi?scr=price)

2. Поддержка API-вызовов формата Gemini:
* [Gemini](https://aistudio.google.com/app/prompts/new_chat)

3. Совместим с большинством локальных моделей в библиотеке transformer (тип модели на узле цепочки локальных моделей LLM изменен на LLM, VLM-GGUF и LLM-GGUF, что соответствует прямой загрузке моделей LLM, загрузке моделей VLM и загрузке моделей LLM в формате GGUF). Если ваша модель LLM в формате VLM или GGUF выдает ошибку, пожалуйста, загрузите последнюю версию llama-cpp-python с [llama-cpp-python](https://github.com/abetlen/llama-cpp-python/releases). В настоящее время протестированные модели включают:
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (рекомендуется! Модель для ролевых игр)
* [lllyasviel/omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (рекомендуется! Модель с богатым набором подсказок)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [openbmb/MiniCPM-V-2_6-gguf](https://huggingface.co/openbmb/MiniCPM-V-2_6-gguf/tree/main)
* [lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF](https://huggingface.co/lmstudio-community/Meta-Llama-3.1-8B-Instruct-GGUF/tree/main)
* [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct)

4. Загрузка моделей:
* [Ссылка на Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), код для извлечения: qyhu

## Загрузка
Используйте один из следующих методов для установки
### Метод 1:
1. В [менеджере comfyui](https://github.com/ltdrdata/ComfyUI-Manager) найдите `comfyui_LLM_party` и установите одним нажатием
2. Перезагрузите comfyui
### Способ второй:
1. Перейдите в подпапку `custom_nodes` в корневой папке ComfyUI.
2. Используйте команду для клонирования этого репозитория: `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### Способ третий:
1. Нажмите на кнопку `CODE` в правом верхнем углу.
2. Нажмите `download zip`.
3. Распакуйте загруженный архив в подпапку `custom_nodes` в корневой папке ComfyUI.

## Развертывание окружения
1. Перейдите в папку проекта `comfyui_LLM_party`.
2. В терминале введите команду `pip install -r requirements.txt`, чтобы установить необходимые сторонние библиотеки для проекта в окружение comfyui. Обратите внимание на то, что вы находитесь в окружении comfyui, и следите за ошибками `pip` в терминале.
3. Если вы используете запускатор comfyui, вам нужно ввести в терминале команду `путь к запускатору\python_embeded\python.exe -m pip install -r requirements.txt` для установки. Папка `python_embeded` обычно находится на одном уровне с папкой `ComfyUI`.
4. Если у вас возникли проблемы с конфигурацией окружения, вы можете попробовать использовать зависимости из файла `requirements_fixed.txt`.
## Конфигурация
* Язык можно настроить в файле `config.ini`, в настоящее время доступны только китайский (zh_CN) и английский (en_US), по умолчанию используется язык вашей системы.
* Для настройки APIKEY можно использовать один из следующих методов.
### Метод 1:
1. Откройте файл `config.ini`, находящийся в папке проекта `comfyui_LLM_party`.
2. Введите ваш `openai_api_key` и `base_url` в `config.ini`.
3. Если вы используете модель ollama, введите `http://127.0.0.1:11434/v1/` в `base_url`, `ollama` в `openai_api_key`, а в `model_name` укажите название вашей модели, например: llama3.
4. Если вы хотите использовать инструменты поиска Google или Bing, введите ваш `google_api_key`, `cse_id` или `bing_api_key` в `config.ini`.
5. Если вы хотите использовать ввод изображений в LLM, рекомендуется использовать хостинг изображений imgbb, введите ваш `imgbb_api` в `config.ini`.
6. Каждая модель может быть настроена отдельно в файле `config.ini`, вы можете обратиться к файлу `config.ini.example` для справки. После настройки вам нужно просто ввести `model_name` на узле.

### Метод 2:
1. Откройте интерфейс comfyui.
2. Создайте узел большого языкового моделирования (LLM), введите ваш `openai_api_key` и `base_url` непосредственно в узле.
3. Если вы используете модель ollama, используйте узел LLM_api, введите `http://127.0.0.1:11434/v1/` в `base_url`, `ollama` в `api_key`, а в `model_name` укажите название вашей модели, например: llama3.
4. Если вы хотите использовать ввод изображений в LLM, рекомендуется использовать хостинг изображений imgbb, введите ваш `imgbb_api_key` на узле.
## Журнал обновлений
1. Вы можете щелкнуть правой кнопкой мыши в интерфейсе comfyui и выбрать в контекстном меню `llm`, чтобы найти узел данного проекта. [Как использовать узлы](https://github.com/heshengtao/Let-LLM-party)
2. Поддерживается подключение API или локальной большой модели. Модульная реализация функции вызова инструментов. При заполнении base_url укажите адрес, заканчивающийся на `/v1/`. Вы можете использовать [ollama](https://github.com/ollama/ollama) для управления вашими моделями, затем в base_url введите `http://127.0.0.1:11434/v1/`, в api_key укажите ollama, а в model_name - название вашей модели, например: llama3.
- Пример рабочего процесса для API подключения: [start_with_LLM_api](workflow/start_with_LLM_api.json)
- Пример рабочего процесса для локальной модели: [start_with_LLM_local](workflow/start_with_LLM_local.json)
- Пример рабочего процесса для подключения ollama: [ollama](workflow/ollama.json)
3. Подключение локальной базы знаний с поддержкой RAG. Пример рабочего процесса: [Поиск в базе знаний RAG.json](workflow/知识库RAG搜索.json)
4. Возможность вызова интерпретатора кода.
5. Возможность подключения к сети для запросов, поддерживается поиск в Google. Пример рабочего процесса: [Рабочий процесс поиска фильмов](workflow/电影查询工作流.json)
6. Возможность реализации условных операторов в comfyui, позволяющая классифицировать вопросы пользователей и отвечать на них целенаправленно. Пример рабочего процесса: [Умный сервис поддержки](workflow/智能客服.json)
7. Поддержка связи между большими моделями, позволяющая организовать дебаты между двумя большими моделями. Пример рабочего процесса: [Дебаты по проблеме трамваев](workflow/电车难题辩论赛.json)
8. Поддержка подключения любых личностей, возможность настройки шаблонов подсказок.
9. Поддержка вызова различных инструментов, в настоящее время разработаны функции для проверки погоды, времени, базы знаний, выполнения кода, сетевого поиска и поиска по отдельным веб-страницам.
10. Поддержка использования LLM в качестве узла инструмента. Пример рабочего процесса: [LLM матрешка](workflow/LLM套娃.json)
11. Поддержка быстрого разработки собственного веб-приложения с помощью API+streamlit.
12. Добавлен опасный универсальный интерпретатор узлов, позволяющий большой модели выполнять любые задачи.
13. Рекомендуется использовать узел отображения текста (show_text) в подкаталоге функций (function) контекстного меню в качестве вывода узла LLM.
14. Поддержка визуальных функций GPT-4O! Пример рабочего процесса: [GPT-4o](workflow/GPT-4o.json)  
15. Добавлен маршрутизатор рабочих процессов, который позволяет вашему рабочему процессу вызывать другие рабочие процессы! Пример рабочего процесса: [Вызов другого рабочего процесса](workflow/调用另一个工作流.json)  
16. Адаптация всех моделей с интерфейсом, аналогичным openai, таких как: 通义千问/qwen, 智谱清言/GLM, deepseek, kimi/moonshot. Пожалуйста, заполните base_url, api_key и model_name этих моделей в узле LLM для их вызова.  
17. Добавлен загрузчик LVM, теперь можно локально вызывать модели LVM, поддерживающий [llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf) модель, другие модели LVM в формате GGUF теоретически также должны работать. Пример рабочего процесса здесь: [start_with_LVM.json](workflow/start_with_LVM.json).  
18. Написан файл `fastapi.py`, если вы запустите его, вы получите интерфейс openai на `http://127.0.0.1:8817/v1/`, любое приложение, которое может вызывать GPT, сможет вызвать ваш рабочий процесс comfyui! Подробности о том, как это сделать, я подготовлю в следующем учебном пособии~  
19. Разделены загрузчик LLM и цепочка LLM, модель загрузки и настройки модели теперь разделены, что позволяет делиться моделями между различными узлами LLM!  
20. В настоящее время поддерживаются устройства macOS и mps! Благодарим [bigcat88](https://github.com/bigcat88) за этот вклад!  
21. Теперь можно создать свою интерактивную игру-роман, которая будет вести к различным концовкам в зависимости от выбора пользователя! Пример рабочего процесса: [Интерактивный роман](workflow/互动小说.json)  
22. Адаптированы функции whisper и tts от openai, что позволяет реализовать ввод и вывод голоса. Пример рабочего процесса: [Ввод голоса + вывод голоса](workflow/语音输入+语音输出.json)  
23. Совместимость с [Omost](https://github.com/lllyasviel/Omost) достигнута!!! Пожалуйста, скачайте [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) и начните использовать прямо сейчас! Пример рабочего процесса: [start_with_OMOST](workflow/start_with_OMOST.json)  
24. Добавлены инструменты LLM для отправки сообщений в корпоративный WeChat, DingTalk и Feishu, а также доступные для вызова внешние функции.  
25. Добавлен текстовый итератор, который может выводить только часть символов за раз, безопасно разделяя текст по символу переноса строки и размеру блока, не разрывая текст посередине. Параметр chunk_overlap указывает, сколько символов будет перекрываться при разделении текста. Это позволяет вводить очень длинные тексты пакетами: достаточно нажимать кнопку или включить циклическое выполнение в comfyui, чтобы автоматизировать процесс. Не забудьте включить свойство is_locked, чтобы автоматически заблокировать рабочий процесс по окончании ввода, предотвращая дальнейшее выполнение. Пример рабочего процесса: [文本迭代输入](workflow/文本迭代输入.json)  
26. В локальном загрузчике LLM и локальном загрузчике llava добавлен атрибут model name; если он пуст, используются различные локальные пути из узла. Если он не пуст, загружаются параметры пути, указанные вами в `config.ini`. Если он не пуст и не указан в `config.ini`, модель будет загружена с huggingface или из директории сохранения моделей huggingface. Если вы хотите загрузить с huggingface, укажите атрибут model name в формате, например: `THUDM/glm-4-9b-chat`. Внимание! Модели, загружаемые таким образом, должны быть совместимы с библиотекой transformer.  
27. Добавлены узлы для анализа JSON-файлов и получения значений из JSON, которые позволяют извлекать значение определенного ключа из файла или текста. Спасибо [guobalove](https://github.com/guobalove) за вклад!
28. Улучшен код вызова инструментов, теперь LLM, не обладающие функцией вызова инструментов, могут активировать атрибут is_tools_in_sys_prompt (локальные LLM по умолчанию не требуют активации, автоматически адаптируются). После активации информация о инструментах будет добавлена в системные подсказки, что позволит LLM вызывать инструменты. Связанная статья о принципах реализации: [Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning](https://arxiv.org/abs/2407.04997)
29. Создана папка custom_tool для хранения кода пользовательских инструментов. Вы можете обратиться к коду в папке [custom_tool](custom_tool) и поместить код своих пользовательских инструментов в папку custom_tool, чтобы использовать их в LLM.
30. Добавлен инструмент графа знаний, который позволяет LLM идеально взаимодействовать с графами знаний. LLM может изменять граф знаний на основе вашего ввода и проводить выводы для получения необходимых ответов. Пример рабочего процесса: [graphRAG_neo4j](workflow/graphRAG_neo4j.json)
31. Добавлена функция персонального AI, позволяющая без программирования создать собственного AI для девушки или парня, с неограниченным общением, постоянной памятью и стабильным характером. Пример рабочего процесса: [麦洛薇人格AI](workflow/麦洛薇人格AI.json)
32. Вы можете использовать этот LLM-генератор инструментов для автоматической генерации инструментов LLM, сохранив сгенерированный код инструмента в файл python, а затем скопировав код в папку custom_tool, после чего вы создадите новый узел. Пример рабочего процесса: [LLM工具生成器](workflow/LLM工具制造机.json).
33. Поддерживается поиск через duckduckgo, но с большими ограничениями, похоже, что можно вводить только английские ключевые слова, и ключевые слова не могут содержать несколько концепций. Преимущество заключается в отсутствии ограничений по API-ключам.
34. Поддерживается функция отдельного вызова нескольких баз знаний, позволяющая в подсказках четко указывать, какая база знаний используется для ответа на вопрос. Пример рабочего процесса: [多知识库分别调用](workflow/多知识库分别调用.json).
35. Поддерживается ввод дополнительных параметров для LLM, включая json out и другие продвинутые параметры. Пример рабочего процесса: [LLM输入额外参数](workflow/LLM额外参数eg_JSON_OUT.json). [用json_out分离提示词](workflow/用json_out分离提示词.json).
36. Добавлена функция подключения агента к Discord. (Все еще в тестировании)
37. Добавлена функция подключения агента к Feishu, огромная благодарность [guobalove](https://github.com/guobalove) за вклад! Ссылка на рабочий процесс [Feishu Robot](workflow/飞书机器人.json).
38. Добавлены универсальные узлы вызова API и множество вспомогательных узлов для формирования тела запроса и извлечения информации из ответа.
39. Добавлен узел для очистки модели, который позволяет выгружать LLM из видеопамяти в любом месте!
40. Добавлен узел [chatTTS](https://github.com/2noise/ChatTTS), огромная благодарность [guobalove](https://github.com/guobalove) за вклад! Параметр `model_path` может быть пустым! Рекомендуется использовать режим HF для загрузки модели, модель будет автоматически загружена с hugging face, без необходимости ручной загрузки; если используется загрузка local, поместите папки `asset` и `config` модели в корневую директорию. [Ссылка на Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), код для извлечения: qyhu; если используется режим `custom`, поместите папки `asset` и `config` модели в папку `model_path`.
2. Обновлены серии узлов преобразования: markdown в HTML, svg в изображение, HTML в изображение, mermaid в изображение, markdown в Excel.
1. Совместим с моделью llama3.2 vision, поддерживает многократный диалог, визуальные функции. Адрес модели: [meta-llama/Llama-3.2-11B-Vision-Instruct](https://huggingface.co/meta-llama/Llama-3.2-11B-Vision-Instruct). Пример рабочего процесса: [llama3.2_vision](https://github.com/heshengtao/comfyui_LLM_party/blob/main/workflow_tutorial/LLM_Party%20for%20Llama3.2%20-Vision%EF%BC%88%E5%B8%A6%E8%AE%B0%E5%BF%86%EF%BC%89.json).
1. Адаптированный GOT-OCR2, поддерживает форматированный вывод результатов, поддерживает точное распознавание текста с использованием позиционных блоков и цветов. Адрес модели: [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR2_0). Пример рабочего процесса преобразует снимок экрана веб-страницы в HTML-код, а затем открывает браузер для отображения этой веб-страницы: [img2web](workflow/图片转网页.json).
2. Значительно изменены узлы загрузчика локальных LLM, теперь вам не нужно выбирать тип модели самостоятельно. Повторно добавлены узлы загрузчика llava и GGUF. Тип модели на узле цепочки локальных моделей LLM изменен на LLM, VLM-GGUF и LLM-GGUF, что соответствует прямой загрузке моделей LLM, загрузке моделей VLM и загрузке моделей LLM в формате GGUF. Теперь снова поддерживаются модели VLM и модели LLM в формате GGUF. Локальные вызовы теперь могут быть совместимы с большим количеством моделей! Примеры рабочих процессов: [LLM_local](workflow/start_with_LLM_local.json), [llava](workflow/start_with_llava.json), [GGUF](workflow/start_with_GGUF.json)
2. Добавлен узел EasyOCR для распознавания текста и позиций на изображениях. Он может создавать соответствующие маски и возвращать строку JSON для просмотра LLM. Доступны стандартная и премиум-версии на выбор!
2. На вечеринке comfyui LLM была воспроизведена система клубники модели серии chatgpt-o1, ссылаясь на подсказки [Llamaberry](https://huggingface.co/spaces/martinbowling/Llamaberry/blob/main/app.py). Пример рабочего процесса: [Система клубники по сравнению с o1](workflow/草莓系统与o1对比.json).
2. Добавлен новый узел GPT-sovits, позволяющий вызывать модель GPT-sovits для преобразования текста в речь на основе вашего эталонного аудио. Вы также можете указать путь к вашей доработанной модели (если не указано, будет использоваться базовая модель) для получения любого желаемого голоса. Для использования необходимо скачать проект [GPT-sovits](https://github.com/RVC-Boss/GPT-SoVITS) и соответствующую базовую модель локально, затем запустить API-сервис с помощью `runtime\python.exe api_v2.py` в папке проекта GPT-sovits. Кроме того, узел chatTTS был перемещен в [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia). Причина в том, что у chatTTS много зависимостей, и его лицензия на PyPi - CC BY-NC 4.0, что является некоммерческой лицензией. Несмотря на то, что проект chatTTS на GitHub находится под лицензией AGPL, мы переместили узел chatTTS в comfyui LLM mafia, чтобы избежать ненужных проблем. Надеемся на ваше понимание!
3. Теперь поддерживает новейшую модель OpenAI, серию o1!
4. Добавлен инструмент управления локальными файлами, который позволяет LLM управлять файлами в указанной вами папке, например, читать, записывать, добавлять, удалять, переименовывать, перемещать и копировать файлы.Из-за потенциальной опасности этого узла он включен в [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia).
5. Новые инструменты SQL позволяют LLM выполнять запросы к базам данных SQL.
6. Обновлена многоязычная версия README. Рабочий процесс для перевода документа README: [translate_readme](workflow/文档自动翻译机.json)
7. Обновлено четыре узла итераторов (текстовый итератор, изображенческий итератор, табличный итератор, json-итератор). Режимы итераторов включают: последовательный, случайный и бесконечный. Последовательный режим выводит данные по порядку, пока не превышает предел индекса, после чего процесс автоматически прекращается, а индекс сбрасывается на 0. Случайный режим будет выбирать случайный индекс для вывода, бесконечный режим будет циклически повторять вывод.
8. Добавлен узел загрузчика API Gemini, который теперь совместим с официальным API Gemini! Если вы находитесь в сети в Китае и сталкиваетесь с проблемами ограничения API по региону, переключите узел на США и используйте режим TUN. Из-за того, что при вызове инструмента Gemini, если возвращаемые параметры содержат китайские символы, может возникать ошибка с кодом 500, некоторые узлы инструментов могут быть недоступны. Пример рабочего процесса: [start_with_gemini](workflow/start_with_gemini.json)
9. Добавлен узел lore book, который позволяет вставлять ваши фоновые настройки во время диалога с LLM, пример рабочего процесса: [lorebook](workflow/lorebook.json)
10. Добавлен узел генератора масок подсказок FLUX, который может генерировать подсказки в различных стилях, таких как карты Hearthstone, карты Yu-Gi-Oh!, постеры, комиксы и другие, позволяя модели FLUX генерировать выходные данные. Пример рабочего процесса: [FLUX提示词](https://openart.ai/workflows/comfyui_llm_party/flux-by-llm-party/sjME541i68Kfw6Ib0EAD)

## Следующий план:
1. Больше адаптаций моделей;
2. Больше способов создания агентов;
3. Больше функций автоматизации;
4. Больше функций управления базой знаний;
5. Больше инструментов, больше персон.

## Отказ от ответственности:
Данный открытый проект и его содержание (далее именуемые "проект") предназначены только для справочных целей и не подразумевают никаких явных или подразумеваемых гарантий. Участники проекта не несут никакой ответственности за полноту, точность, надежность или применимость проекта. Любые действия, основанные на содержании проекта, осуществляются на ваш собственный риск. В любых случаях участники проекта не несут ответственности за любые косвенные, специальные или побочные убытки или повреждения, возникшие в результате использования содержания проекта.

## Особая благодарность
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/HuangYuChuh">
  <img src="https://avatars.githubusercontent.com/u/167663109?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/SpenserCai">
  <img src="https://avatars.githubusercontent.com/u/25168945?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Список заимствований
Некоторые узлы в этом проекте заимствованы из следующих проектов, благодарим их за вклад в сообщество с открытым исходным кодом!
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)

## Поддержка:

### Присоединяйтесь к сообществу
Если у вас возникли проблемы с плагином или есть другие вопросы, приглашаем вас присоединиться к нашему сообществу.

1. QQ группа: `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/Q群.jpg" style="width: 48%;" />
</div>

2. 微信群：`Choo-Yong`（添加小助手微信后进群）

3. discord:[дискорд-ссылка](https://discord.gg/f2dsAKKr2V)

### Следите за нами
1. Если вы хотите оставаться в курсе последних функций этого проекта, пожалуйста, подпишитесь на аккаунт B站: [派对主持BB机](https://space.bilibili.com/26978344)
2. Аккаунт OpenArt постоянно обновляет самые полезные рабочие процессы для вечеринок: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

### Поддержка пожертвований
Если моя работа принесла вам ценность, пожалуйста, подумайте о том, чтобы угостить меня чашечкой кофе! Ваша поддержка не только придаёт проекту жизнь, но и согревает сердце создателя.☕💖 Каждая чашка имеет значение!
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## История звёзд

[![График истории звёзд](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
