![图片](img/封面.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">Видеоуроки</a> ·
  <a href="how_to_use_nodes_ZH.md">Текстовые инструкции</a> ·
  <a href="workflow_tutorial/">Инструкции по работе с потоками</a> ·
  <a href="https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu">Ссылка на Baidu Pan</a> ·
  <a href="img/Q群.jpg">QQ группа</a> ·
  <a href="https://discord.gg/gxrQAYy6">Discord</a> ·
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">О нас</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/简体中文-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/Русский-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Français-d9d9d9"></a>
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/日本語-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/한국어-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/العربية-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Español-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Português-d9d9d9"></a>
</div>

####

Comfyui_llm_party стремится на основе [comfyui](https://github.com/comfyanonymous/ComfyUI) создать полностью укомплектованную библиотеку узлов для построения рабочих потоков LLM, используя исключительно простой интерфейс. Это позволит пользователям быстрее и удобнее создавать свои рабочие потоки LLM и интегрировать их в собственные графические рабочие процессы.

## Демонстрация эффектов
https://github.com/user-attachments/assets/9e627204-4626-479e-8806-cb06cd6157a6
## Обзор проекта
ComfyUI LLM Party предлагает от самых основ LLM, включая многопрофильное использование инструментов, быструю настройку индивидуального AI-ассистента, до внедрения в отрасль векторных представлений слов (RAG) и GraphRAG для локального управления знаниями в отрасли. От простых потоков агентов до построения сложных моделей взаимодействия между агентами, включая радиальные и кольцевые модели взаимодействия; от необходимости индивидуальных пользователей интегрировать свои социальные приложения (QQ, Feishu, Discord), до комплексного рабочего процесса для стримеров, который объединяет LLM, TTS и ComfyUI; от простого первого опыта работы с LLM, необходимого обычным студентам, до интерфейсов настройки параметров, часто используемых исследователями и адаптации моделей. Все это вы сможете найти в ComfyUI LLM Party.

## Последние обновления
1. Обновлена многоязычная версия README. Рабочий процесс для перевода документа README: [translate_readme](workflow/文档自动翻译机.json)
2. Обновлено четыре узла итераторов (текстовый итератор, изображенческий итератор, табличный итератор, json-итератор). Режимы итераторов включают: последовательный, случайный и бесконечный. Последовательный режим выводит данные по порядку, пока не превышает предел индекса, после чего процесс автоматически прекращается, а индекс сбрасывается на 0. Случайный режим будет выбирать случайный индекс для вывода, бесконечный режим будет циклически повторять вывод.
3. Добавлен узел загрузчика API Gemini, который теперь совместим с официальным API Gemini! Если вы находитесь в сети в Китае и сталкиваетесь с проблемами ограничения API по региону, переключите узел на США и используйте режим TUN. Из-за того, что при вызове инструмента Gemini, если возвращаемые параметры содержат китайские символы, может возникать ошибка с кодом 500, некоторые узлы инструментов могут быть недоступны. Пример рабочего процесса: [start_with_gemini](workflow/start_with_gemini.json)
4. Добавлен узел lore book, который позволяет вставлять ваши фоновые настройки во время диалога с LLM, пример рабочего процесса: [lorebook](workflow/lorebook.json)
5. Добавлен узел генератора масок подсказок FLUX, который может генерировать подсказки в различных стилях, таких как карты Hearthstone, карты Yu-Gi-Oh!, постеры, комиксы и другие, позволяя модели FLUX генерировать выходные данные. Пример рабочего процесса: [FLUX提示词](https://openart.ai/workflows/comfyui_llm_party/flux-by-llm-party/sjME541i68Kfw6Ib0EAD)

## Инструкция по использованию
1. Пожалуйста, обратитесь к инструкции по использованию узлов: [怎么使用节点](how_to_use_nodes.md)

2. Если у вас возникли проблемы с плагином или у вас есть другие вопросы, присоединяйтесь к нашей группе QQ: [931057213](img/Q群.jpg)
3. Пожалуйста, обратитесь к [учебнику по рабочим процессам](workflow_tutorial/), благодарим за вклад [HuangYuChuh](https://github.com/HuangYuChuh)!

4. Учётная запись для высокоуровневых игровых рабочих процессов: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

4. Дополнительные рабочие процессы можно найти в папке [workflow](workflow).

## Видеоуроки
1. [Пошаговое руководство по созданию модульного интеллекта (очень просто!)](https://www.bilibili.com/video/BV1JZ421v7Tw/?vd_source=f229e378448918b84afab7c430c6a75b)

2. [Как подключить GPT-4o к comfyui | Позвольте рабочему процессу вызывать другой рабочий процесс | Превратите LLM в инструмент](https://www.bilibili.com/video/BV1JJ4m1A789/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)

3. [Как замаскировать ваш рабочий процесс под GPT и подключить к WeChat | Совместимость с Omost! Гибко создавайте свой dalle3](https://www.bilibili.com/video/BV1DT421a7KY/?spm_id_from=333.999.0.0)

4. [Как играть в интерактивные новеллы в comfyui](https://www.bilibili.com/video/BV15y411q78L/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)

5. [AI подруга, и она в твоем облике | Реализация graphRAG на comfyui, взаимодействие с neo4j | Подключение рабочего процесса comfyui к фронтенду streamlit](https://www.bilibili.com/video/BV1dS421R7Au/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)
## Поддержка моделей
1. Поддержка всех API-вызовов формата OpenAI (в сочетании с [oneapi](https://github.com/songquanpeng/one-api) можно вызывать практически все API LLM, также поддерживаются все промежуточные API). Выбор base_url можно найти в [config.ini.example](config.ini.example), на данный момент протестированы следующие:
* [ollama](https://github.com/ollama/ollama) (рекомендуется! Если вы вызываете локально, настоятельно рекомендуем использовать ollama для размещения вашей локальной модели!)
* [通义千问/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [智谱清言/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [豆包](https://www.volcengine.com/docs/82379/1263482)

2. Поддержка API-вызовов формата Gemini:
* [Gemini](https://aistudio.google.com/app/prompts/new_chat)
3. Совместимость с большинством локальных моделей, поддерживаемых классом AutoModelForCausalLM библиотеки transformer (если не знаете, какой выбрать тип модели на локальном узле, выберите llama, вероятно, подойдет). В настоящее время протестированы следующие модели:
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (рекомендуется! Модель для ролевых игр)
* [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (рекомендуется! Модель с богатым набором подсказок)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)
* [THUDM/chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b) (в связи с тем, что GLM4 изменил формат вызова, разработчики не в состоянии поддерживать вызовы всех локальных крупных моделей, поэтому рекомендуется использовать метод локального вызова ollama!)

4. Загрузка моделей:
* [Ссылка на Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), код для извлечения: qyhu

## Загрузка
Используйте один из следующих методов для установки
### Метод 1:
1. В [менеджере comfyui](https://github.com/ltdrdata/ComfyUI-Manager) найдите `comfyui_LLM_party` и установите одним нажатием
2. Перезагрузите comfyui
### Способ второй:
1. Перейдите в подпапку `custom_nodes` в корневой папке ComfyUI.
2. Используйте команду для клонирования этого репозитория: `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### Способ третий:
1. Нажмите на кнопку `CODE` в правом верхнем углу.
2. Нажмите `download zip`.
3. Распакуйте загруженный архив в подпапку `custom_nodes` в корневой папке ComfyUI.

## Развертывание окружения
1. Перейдите в папку проекта `comfyui_LLM_party`.
2. В терминале введите команду `pip install -r requirements.txt`, чтобы установить необходимые сторонние библиотеки для проекта в окружение comfyui. Обратите внимание на то, что вы находитесь в окружении comfyui, и следите за ошибками `pip` в терминале.
3. Если вы используете запускатор comfyui, вам нужно ввести в терминале команду `путь к запускатору\python_embeded\python.exe -m pip install -r requirements.txt` для установки. Папка `python_embeded` обычно находится на одном уровне с папкой `ComfyUI`.
4. Если у вас возникли проблемы с конфигурацией окружения, вы можете попробовать использовать зависимости из файла `requirements_fixed.txt`.
## Конфигурация
* Язык можно настроить в файле `config.ini`, в настоящее время доступны только китайский (zh_CN) и английский (en_US), по умолчанию используется язык вашей системы.
* Для настройки APIKEY можно использовать один из следующих методов.
### Метод 1:
1. Откройте файл `config.ini`, находящийся в папке проекта `comfyui_LLM_party`.
2. Введите ваш `openai_api_key` и `base_url` в `config.ini`.
3. Если вы используете модель ollama, введите `http://127.0.0.1:11434/v1/` в `base_url`, `ollama` в `openai_api_key`, а в `model_name` укажите название вашей модели, например: llama3.
4. Если вы хотите использовать инструменты поиска Google или Bing, введите ваш `google_api_key`, `cse_id` или `bing_api_key` в `config.ini`.
5. Если вы хотите использовать ввод изображений в LLM, рекомендуется использовать хостинг изображений imgbb, введите ваш `imgbb_api` в `config.ini`.
6. Каждая модель может быть настроена отдельно в файле `config.ini`, вы можете обратиться к файлу `config.ini.example` для справки. После настройки вам нужно просто ввести `model_name` на узле.

### Метод 2:
1. Откройте интерфейс comfyui.
2. Создайте узел большого языкового моделирования (LLM), введите ваш `openai_api_key` и `base_url` непосредственно в узле.
3. Если вы используете модель ollama, используйте узел LLM_api, введите `http://127.0.0.1:11434/v1/` в `base_url`, `ollama` в `api_key`, а в `model_name` укажите название вашей модели, например: llama3.
4. Если вы хотите использовать ввод изображений в LLM, рекомендуется использовать хостинг изображений imgbb, введите ваш `imgbb_api_key` на узле.
## Журнал обновлений
1. Вы можете щелкнуть правой кнопкой мыши в интерфейсе comfyui и выбрать в контекстном меню `llm`, чтобы найти узел данного проекта. [Как использовать узлы](how_to_use_nodes_ZH.md)
2. Поддерживается подключение API или локальной большой модели. Модульная реализация функции вызова инструментов. При заполнении base_url укажите адрес, заканчивающийся на `/v1/`. Вы можете использовать [ollama](https://github.com/ollama/ollama) для управления вашими моделями, затем в base_url введите `http://127.0.0.1:11434/v1/`, в api_key укажите ollama, а в model_name - название вашей модели, например: llama3.
- Пример рабочего процесса для API подключения: [start_with_LLM_api](workflow/start_with_LLM_api.json)
- Пример рабочего процесса для локальной модели: [start_with_LLM_local](workflow/start_with_LLM_local.json)
- Пример рабочего процесса для подключения ollama: [ollama](workflow/ollama.json)
3. Подключение локальной базы знаний с поддержкой RAG. Пример рабочего процесса: [Поиск в базе знаний RAG.json](workflow/知识库RAG搜索.json)
4. Возможность вызова интерпретатора кода.
5. Возможность подключения к сети для запросов, поддерживается поиск в Google. Пример рабочего процесса: [Рабочий процесс поиска фильмов](workflow/电影查询工作流.json)
6. Возможность реализации условных операторов в comfyui, позволяющая классифицировать вопросы пользователей и отвечать на них целенаправленно. Пример рабочего процесса: [Умный сервис поддержки](workflow/智能客服.json)
7. Поддержка связи между большими моделями, позволяющая организовать дебаты между двумя большими моделями. Пример рабочего процесса: [Дебаты по проблеме трамваев](workflow/电车难题辩论赛.json)
8. Поддержка подключения любых личностей, возможность настройки шаблонов подсказок.
9. Поддержка вызова различных инструментов, в настоящее время разработаны функции для проверки погоды, времени, базы знаний, выполнения кода, сетевого поиска и поиска по отдельным веб-страницам.
10. Поддержка использования LLM в качестве узла инструмента. Пример рабочего процесса: [LLM матрешка](workflow/LLM套娃.json)
11. Поддержка быстрого разработки собственного веб-приложения с помощью API+streamlit.
12. Добавлен опасный универсальный интерпретатор узлов, позволяющий большой модели выполнять любые задачи.
13. Рекомендуется использовать узел отображения текста (show_text) в подкаталоге функций (function) контекстного меню в качестве вывода узла LLM.
14. Поддержка визуальных функций GPT-4O! Пример рабочего процесса: [GPT-4o](workflow/GPT-4o.json)
15. Добавлен маршрутизатор рабочих процессов, который позволяет вашему рабочему процессу вызывать другие рабочие процессы! Пример рабочего процесса: [Вызов другого рабочего процесса](workflow/调用另一个工作流.json)
16. Адаптация всех моделей с интерфейсом, аналогичным openai, таких как: 通义千问/qwen, 智谱清言/GLM, deepseek, kimi/moonshot. Пожалуйста, заполните base_url, api_key и model_name этих моделей в узле LLM для их вызова.
17. Добавлен загрузчик LVM, теперь можно локально вызывать модели LVM, поддерживающий [llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf) модель, другие модели LVM в формате GGUF теоретически также должны работать. Пример рабочего процесса здесь: [start_with_LVM.json](workflow/start_with_LVM.json).
18. Написан файл `fastapi.py`, если вы запустите его, вы получите интерфейс openai на `http://127.0.0.1:8817/v1/`, любое приложение, которое может вызывать GPT, сможет вызвать ваш рабочий процесс comfyui! Подробности о том, как это сделать, я подготовлю в следующем учебном пособии~
19. Разделены загрузчик LLM и цепочка LLM, модель загрузки и настройки модели теперь разделены, что позволяет делиться моделями между различными узлами LLM!
20. В настоящее время поддерживаются устройства macOS и mps! Благодарим [bigcat88](https://github.com/bigcat88) за этот вклад!
21. Теперь можно создать свою интерактивную игру-роман, которая будет вести к различным концовкам в зависимости от выбора пользователя! Пример рабочего процесса: [Интерактивный роман](workflow/互动小说.json)
22. Адаптированы функции whisper и tts от openai, что позволяет реализовать ввод и вывод голоса. Пример рабочего процесса: [Ввод голоса + вывод голоса](workflow/语音输入+语音输出.json)
23. Совместимость с [Omost](https://github.com/lllyasviel/Omost) достигнута!!! Пожалуйста, скачайте [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) и начните использовать прямо сейчас! Пример рабочего процесса: [start_with_OMOST](workflow/start_with_OMOST.json)
24. Добавлены инструменты LLM для отправки сообщений в корпоративный WeChat, DingTalk и Feishu, а также доступные для вызова внешние функции.
25. Добавлен текстовый итератор, который может выводить только часть символов за раз, безопасно разделяя текст по символу переноса строки и размеру блока, не разрывая текст посередине. Параметр chunk_overlap указывает, сколько символов будет перекрываться при разделении текста. Это позволяет вводить очень длинные тексты пакетами: достаточно нажимать кнопку или включить циклическое выполнение в comfyui, чтобы автоматизировать процесс. Не забудьте включить свойство is_locked, чтобы автоматически заблокировать рабочий процесс по окончании ввода, предотвращая дальнейшее выполнение. Пример рабочего процесса: [文本迭代输入](workflow/文本迭代输入.json)
26. В локальном загрузчике LLM и локальном загрузчике llava добавлен атрибут model name; если он пуст, используются различные локальные пути из узла. Если он не пуст, загружаются параметры пути, указанные вами в `config.ini`. Если он не пуст и не указан в `config.ini`, модель будет загружена с huggingface или из директории сохранения моделей huggingface. Если вы хотите загрузить с huggingface, укажите атрибут model name в формате, например: `THUDM/glm-4-9b-chat`. Внимание! Модели, загружаемые таким образом, должны быть совместимы с библиотекой transformer.
27. Добавлены узлы для анализа JSON-файлов и получения значений из JSON, которые позволяют извлекать значение определенного ключа из файла или текста. Спасибо [guobalove](https://github.com/guobalove) за вклад!
28. Улучшен код вызова инструментов, теперь LLM, не обладающие функцией вызова инструментов, могут активировать атрибут is_tools_in_sys_prompt (локальные LLM по умолчанию не требуют активации, автоматически адаптируются). После активации информация о инструментах будет добавлена в системные подсказки, что позволит LLM вызывать инструменты. Связанная статья о принципах реализации: [Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning](https://arxiv.org/abs/2407.04997)
29. Создана папка custom_tool для хранения кода пользовательских инструментов. Вы можете обратиться к коду в папке [custom_tool](custom_tool) и поместить код своих пользовательских инструментов в папку custom_tool, чтобы использовать их в LLM.
30. Добавлен инструмент графа знаний, который позволяет LLM идеально взаимодействовать с графами знаний. LLM может изменять граф знаний на основе вашего ввода и проводить выводы для получения необходимых ответов. Пример рабочего процесса: [graphRAG_neo4j](workflow/graphRAG_neo4j.json)
31. Добавлена функция персонального AI, позволяющая без программирования создать собственного AI для девушки или парня, с неограниченным общением, постоянной памятью и стабильным характером. Пример рабочего процесса: [麦洛薇人格AI](workflow/麦洛薇人格AI.json)
32. Вы можете использовать этот LLM-генератор инструментов для автоматической генерации инструментов LLM, сохранив сгенерированный код инструмента в файл python, а затем скопировав код в папку custom_tool, после чего вы создадите новый узел. Пример рабочего процесса: [LLM工具生成器](workflow/LLM工具制造机.json).
33. Поддерживается поиск через duckduckgo, но с большими ограничениями, похоже, что можно вводить только английские ключевые слова, и ключевые слова не могут содержать несколько концепций. Преимущество заключается в отсутствии ограничений по API-ключам.
34. Поддерживается функция отдельного вызова нескольких баз знаний, позволяющая в подсказках четко указывать, какая база знаний используется для ответа на вопрос. Пример рабочего процесса: [多知识库分别调用](workflow/多知识库分别调用.json).
35. Поддерживается ввод дополнительных параметров для LLM, включая json out и другие продвинутые параметры. Пример рабочего процесса: [LLM输入额外参数](workflow/LLM额外参数eg_JSON_OUT.json). [用json_out分离提示词](workflow/用json_out分离提示词.json).
36. Добавлена функция подключения агента к Discord. (Все еще в тестировании)
37. Добавлена функция подключения агента к Feishu, огромная благодарность [guobalove](https://github.com/guobalove) за вклад! Ссылка на рабочий процесс [Feishu Robot](workflow/飞书机器人.json).
38. Добавлены универсальные узлы вызова API и множество вспомогательных узлов для формирования тела запроса и извлечения информации из ответа.
39. Добавлен узел для очистки модели, который позволяет выгружать LLM из видеопамяти в любом месте!
40. Добавлен узел [chatTTS](https://github.com/2noise/ChatTTS), огромная благодарность [guobalove](https://github.com/guobalove) за вклад! Параметр `model_path` может быть пустым! Рекомендуется использовать режим HF для загрузки модели, модель будет автоматически загружена с hugging face, без необходимости ручной загрузки; если используется загрузка local, поместите папки `asset` и `config` модели в корневую директорию. [Ссылка на Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), код для извлечения: qyhu; если используется режим `custom`, поместите папки `asset` и `config` модели в папку `model_path`.
## Следующий план:
1. Более широкая адаптация моделей, которая охватывает основные API интерфейсы крупных моделей и локальные вызовы популярных открытых моделей, а также большее количество адаптаций LVM моделей. На данный момент я адаптировал только вызов визуальных функций GPT-4;
2. Больше способов построения агентов. В этой области я уже завершил работу по импорту LLM как инструмента в другой LLM, что позволяет создавать радиальные рабочие процессы LLM, где один рабочий процесс импортируется как узел в другой. В будущем я, возможно, разработаю некоторые более интересные функции в этом направлении;
3. Больше автоматизированных функций. В будущем я представлю больше узлов для автоматической отправки изображений, текста, видео и аудио в другие приложения, а также запущу узлы для мониторинга, которые обеспечивают автоматические ответы на основные социальные сети и форумы;
4. Более продвинутые функции управления базами знаний. В настоящее время проект уже поддерживает поиск по локальным файлам и в интернете, в будущем я представлю поиск по графам знаний и долговременной памяти. Это позволит агентам логически обрабатывать специализированные знания и запоминать определенную ключевую информацию при взаимодействии с пользователями;
5. Больше инструментов и больше масок личностей. Эта часть проще в реализации, но требует накопления. Я надеюсь, что в будущем мой проект сможет иметь множество настраиваемых узлов, как comfyui, с разнообразными инструментами и масками личностей.

## Отказ от ответственности:
Данный открытый проект и его содержание (далее именуемые "проект") предназначены только для справочных целей и не подразумевают никаких явных или подразумеваемых гарантий. Участники проекта не несут никакой ответственности за полноту, точность, надежность или применимость проекта. Любые действия, основанные на содержании проекта, осуществляются на ваш собственный риск. В любых случаях участники проекта не несут ответственности за любые косвенные, специальные или побочные убытки или повреждения, возникшие в результате использования содержания проекта.
## Особая благодарность
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/HuangYuChuh">
  <img src="https://avatars.githubusercontent.com/u/167663109?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Список заимствований
Некоторые узлы в этом проекте заимствованы из следующих проектов, благодарим их за вклад в сообщество с открытым исходным кодом!
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)
3. [2noise/ChatTTS](https://github.com/2noise/ChatTTS)

## Поддержка:

### Присоединяйтесь к сообществу
Если у вас возникли проблемы с плагином или есть другие вопросы, приглашаем вас присоединиться к нашему сообществу.

1. QQ группа: `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/Q群.jpg" style="width: 48%;" />
</div>

2. 微信群：`Choo-Yong`（添加小助手微信后进群）

3. discord:[дискорд-ссылка](https://discord.gg/gxrQAYy6)

### Следите за нами
1. Если вы хотите оставаться в курсе последних функций этого проекта, пожалуйста, подпишитесь на аккаунт B站: [派对主持BB机](https://space.bilibili.com/26978344)
2. Аккаунт OpenArt постоянно обновляет самые полезные рабочие процессы для вечеринок: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

### Поддержка пожертвований
Если моя работа принесла вам ценность, пожалуйста, подумайте о том, чтобы угостить меня чашечкой кофе! Ваша поддержка не только придаёт проекту жизнь, но и согревает сердце создателя.☕💖 Каждая чашка имеет значение!
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## История звёзд

[![График истории звёзд](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
