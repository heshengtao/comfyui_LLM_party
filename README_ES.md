![Imagen](img/Â∞ÅÈù¢.png)

<div align="center">
  <a href="https://space.bilibili.com/26978344">Tutorial en video</a> ¬∑
  <a href="how_to_use_nodes_ZH.md">Tutorial escrito</a> ¬∑
  <a href="workflow_tutorial/">Tutorial de flujo de trabajo</a> ¬∑
  <a href="https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu">Enlace de Baidu</a> ¬∑
  <a href="img/QÁæ§.jpg">Grupo QQ</a> ¬∑
  <a href="https://discord.gg/gxrQAYy6">Discord</a> ¬∑
  <a href="https://dcnsxxvm4zeq.feishu.cn/wiki/IyUowXNj9iH0vzk68cpcLnZXnYf">Sobre nosotros</a>
</div>

####

<div align="center">
  <a href="./README_ZH.md"><img src="https://img.shields.io/badge/ÁÆÄ‰Ωì‰∏≠Êñá-d9d9d9"></a>
  <a href="./README.md"><img src="https://img.shields.io/badge/English-d9d9d9"></a>
  <a href="./README_RU.md"><img src="https://img.shields.io/badge/–†—É—Å—Å–∫–∏–π-d9d9d9"></a>
  <a href="./README_FR.md"><img src="https://img.shields.io/badge/Fran√ßais-d9d9d9"></a> 
  <a href="./README_DE.md"><img src="https://img.shields.io/badge/Deutsch-d9d9d9"></a>
  <a href="./README_JA.md"><img src="https://img.shields.io/badge/Êó•Êú¨Ë™û-d9d9d9"></a>
  <a href="./README_KO.md"><img src="https://img.shields.io/badge/ÌïúÍµ≠Ïñ¥-d9d9d9"></a>
  <a href="./README_AR.md"><img src="https://img.shields.io/badge/ÿßŸÑÿπÿ±ÿ®Ÿäÿ©-d9d9d9"></a>
  <a href="./README_ES.md"><img src="https://img.shields.io/badge/Espa√±ol-d9d9d9"></a>
  <a href="./README_PT.md"><img src="https://img.shields.io/badge/Portugu√™s-d9d9d9"></a>
</div>

####

Comfyui_llm_party tiene la intenci√≥n de desarrollar una biblioteca completa de nodos para la construcci√≥n de flujos de trabajo de LLM, basada en la interfaz de usuario extremadamente simple de [comfyui](https://github.com/comfyanonymous/ComfyUI) como frontend. Esto permitir√° a los usuarios construir sus flujos de trabajo de LLM de manera m√°s r√°pida y conveniente, adem√°s de facilitar la integraci√≥n de sus flujos de trabajo de im√°genes.

## Demostraci√≥n de resultados
https://github.com/user-attachments/assets/945493c0-92b3-4244-ba8f-0c4b2ad4eba6

## Resumen del Proyecto
ComfyUI LLM Party permite desde la llamada a m√∫ltiples herramientas LLM desde la base, la r√°pida configuraci√≥n de un asistente AI personalizado, hasta la implementaci√≥n de vectores de palabras RAG y GraphRAG para la gesti√≥n local de bases de datos de conocimiento en la industria; desde una simple l√≠nea de agentes inteligentes hasta la construcci√≥n de complejos modos de interacci√≥n radial entre agentes inteligentes y modos de interacci√≥n en c√≠rculo; desde la integraci√≥n de aplicaciones sociales (QQ, Feishu, Discord) para usuarios individuales, hasta un flujo de trabajo integral de LLM+TTS+ComfyUI para trabajadores de streaming; desde el inicio sencillo que necesita un estudiante com√∫n con su primera aplicaci√≥n LLM, hasta las diversas interfaces de ajuste de par√°metros frecuentemente utilizadas por investigadores. Todo esto lo puedes encontrar en ComfyUI LLM Party.

## √öltimas Actualizaciones
1. En la fiesta comfyui LLM, se reprodujo el sistema de fresas del modelo de la serie chatgpt-o1, haciendo referencia a las indicaciones de [Llamaberry](https://huggingface.co/spaces/martinbowling/Llamaberry/blob/main/app.py). Ejemplo de flujo de trabajo: [Sistema de fresas comparado con o1](workflow\ËçâËéìÁ≥ªÁªü‰∏éo1ÂØπÊØî.json).
2. Se ha a√±adido un nuevo nodo GPT-sovits, que permite llamar al modelo GPT-sovits para convertir texto en voz basado en tu audio de referencia. Tambi√©n puedes rellenar la ruta de tu modelo afinado (si no se rellena, se utilizar√° el modelo base para la inferencia) para obtener cualquier voz deseada. Para usarlo, necesitas descargar el proyecto [GPT-sovits](https://github.com/RVC-Boss/GPT-SoVITS) y el modelo base correspondiente localmente, luego iniciar el servicio API con `runtime\python.exe api_v2.py` en la carpeta del proyecto GPT-sovits. Adem√°s, el nodo chatTTS se ha movido a [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia). La raz√≥n es que chatTTS tiene muchas dependencias, y su licencia en PyPi es CC BY-NC 4.0, que es una licencia no comercial. Aunque el proyecto chatTTS en GitHub est√° bajo la licencia AGPL, movimos el nodo chatTTS a comfyui LLM mafia para evitar problemas innecesarios. ¬°Esperamos que todos lo entiendan!
3. ¬°Ahora admite el √∫ltimo modelo de OpenAI, la serie o1!
4. Se ha a√±adido una herramienta de control de archivos locales que permite al LLM controlar los archivos en la carpeta especificada, como leer, escribir, a√±adir, eliminar, renombrar, mover y copiar archivos.Debido al peligro potencial de este nodo, est√° incluido en [comfyui LLM mafia](https://github.com/heshengtao/comfyui_LLM_mafia).
5. Las nuevas herramientas SQL permiten a LLM consultar bases de datos SQL.
6. Actualizada la versi√≥n multiling√ºe del README. Flujo de trabajo para traducir el documento README: [translate_readme](workflow/ÊñáÊ°£Ëá™Âä®ÁøªËØëÊú∫.json)
7. Se han actualizado cuatro nodos iteradores (iterador de texto, iterador de im√°genes, iterador de tablas, iterador json), con tres modos de iteraci√≥n: secuencial, aleatorio e infinito. El modo secuencial emitir√° los resultados en orden, deteni√©ndose autom√°ticamente al superar el l√≠mite de √≠ndice y reiniciando el valor del √≠ndice a 0; el modo aleatorio elegir√° un √≠ndice al azar para emitir; el modo infinito emitir√° indefinidamente en bucle.
8. Se ha a√±adido un nodo cargador de API Gemini, ahora compatible con la API oficial de Gemini. Si est√°s en un entorno de red nacional y enfrentas problemas de restricci√≥n regional de la API, cambia el nodo a Estados Unidos y utiliza el modo TUN. Debido a que Gemini presenta un error con c√≥digo de retorno 500 si los par√°metros devueltos contienen caracteres en chino, algunos nodos de herramientas pueden no estar disponibles. Flujo de trabajo de ejemplo: [start_with_gemini](workflow/start_with_gemini.json)
9. Se ha a√±adido el nodo lore book, que permite insertar tus configuraciones de fondo al dialogar con el LLM. Flujo de trabajo de ejemplo: [lorebook](workflow/lorebook.json)
10. Se ha a√±adido el nodo generador de palabras clave FLUX, que puede generar palabras clave en varios estilos como cartas de Hearthstone, cartas de Yu-Gi-Oh, carteles, c√≥mics, entre otros, permitiendo que el modelo FLUX genere directamente. Flujo de trabajo de referencia: [FLUXÊèêÁ§∫ËØç](https://openart.ai/workflows/comfyui_llm_party/flux-by-llm-party/sjME541i68Kfw6Ib0EAD)

## Instrucciones de Uso
1. Para las instrucciones de uso de los nodos, consulta: [ÊÄé‰πà‰ΩøÁî®ËäÇÁÇπ](how_to_use_nodes.md)

2. Si hay problemas con el plugin o tienes otras dudas, no dudes en unirte a nuestro grupo de QQ: [931057213](img/QÁæ§.jpg)
3. Para el tutorial de flujos de trabajo, consulte: [Â∑•‰ΩúÊµÅÊïôÁ®ã](workflow_tutorial/)Ôºåagradecemos la contribuci√≥n de [HuangYuChuh](https://github.com/HuangYuChuh).

4. Cuenta para funciones avanzadas de flujos de trabajo: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

4. Para m√°s flujos de trabajo, puede consultar la carpeta [workflow](workflow).

## Tutoriales en video
1. [Manos a la obra: c√≥mo construir agentes modulares (¬°muy sencillo!)](https://www.bilibili.com/video/BV1JZ421v7Tw/?vd_source=f229e378448918b84afab7c430c6a75b)

2. [Integrando GPT-4o con comfyui | Permitiendo que un flujo de trabajo llame a otro flujo de trabajo | Transformando LLM en una herramienta](https://www.bilibili.com/video/BV1JJ4m1A789/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)

3. [Disfrazando tu flujo de trabajo como GPT para integrarlo en WeChat | ¬°Compatible con Omost! Crea tu propio dalle3 de manera flexible](https://www.bilibili.com/video/BV1DT421a7KY/?spm_id_from=333.999.0.0)

4. [C√≥mo jugar a juegos de novela interactiva en comfyui](https://www.bilibili.com/video/BV15y411q78L/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)

5. [Novia AI, y adem√°s en tu forma | Implementando graphRAG en comfyui, conectando con neoa4j | Integraci√≥n de flujos de trabajo de comfyui con el frontend de streamlit](https://www.bilibili.com/video/BV1dS421R7Au/?spm_id_from=333.999.0.0&vd_source=f229e378448918b84afab7c430c6a75b)
## Soporte de Modelos
1. Se admiten todas las llamadas a la API en formato OpenAI (en combinaci√≥n con [oneapi](https://github.com/songquanpeng/one-api) se pueden realizar llamadas a casi todas las API de LLM, tambi√©n se admiten todas las API de retransmisi√≥n), para la elecci√≥n de base_url, consulte [config.ini.example](config.ini.example). Actualmente se han probado las siguientes:
* [ollama](https://github.com/ollama/ollama) (¬°recomendado! Si realiza llamadas localmente, se recomienda encarecidamente utilizar ollama para alojar su modelo localmente).
* [ÈÄö‰πâÂçÉÈóÆ/qwen](https://help.aliyun.com/zh/dashscope/developer-reference/compatibility-of-openai-with-dashscope/?spm=a2c4g.11186623.0.0.7b576019xkArPq)
* [Êô∫Ë∞±Ê∏ÖË®Ä/glm](https://open.bigmodel.cn/dev/api#http_auth)
* [deepseek](https://platform.deepseek.com/api-docs/zh-cn/)
* [kimi/moonshot](https://platform.moonshot.cn/docs/api/chat#%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF)
* [Ë±ÜÂåÖ](https://www.volcengine.com/docs/82379/1263482)

2. Se admiten llamadas a la API en formato Gemini:
* [Gemini](https://aistudio.google.com/app/prompts/new_chat)
3. Compatible con la mayor√≠a de los modelos locales soportados por la clase AutoModelForCausalLM de la biblioteca transformer (si no sabe qu√© seleccionar para el tipo de modelo en el nodo local, elija llama, que probablemente se adapte), hasta ahora se han probado los siguientes:
* [ClosedCharacter/Peach-9B-8k-Roleplay](https://huggingface.co/ClosedCharacter/Peach-9B-8k-Roleplay) (¬°Recomendado! Modelo de rol)
* [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) (¬°Recomendado! Modelo de palabras clave ricas)
* [meta-llama/Llama-2-7b-chat-hf](https://huggingface.co/meta-llama/Llama-2-7b-chat-hf)
* [Qwen/Qwen2-7B-Instruct](https://huggingface.co/Qwen/Qwen2-7B-Instruct)
* [xtuner/llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf)
* [THUDM/chatglm3-6b](https://huggingface.co/THUDM/chatglm3-6b) (Debido a que GLM4 ha cambiado a un nuevo formato de llamada, los desarrolladores no pueden mantener todas las llamadas a grandes modelos locales, ya que se recomienda utilizar ollama para las llamadas locales).

4. Descarga del modelo:
* [Direcci√≥n de Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), c√≥digo de extracci√≥n: qyhu

## Descarga
Instale utilizando uno de los siguientes m√©todos:
### M√©todo Uno:
1. Busque `comfyui_LLM_party` en [el administrador de ComfyUI](https://github.com/ltdrdata/ComfyUI-Manager) e inst√°lelo con un solo clic.
2. Reinicie ComfyUI.
### M√©todo dos:
1. Navegue hasta la subcarpeta `custom_nodes` en la carpeta ra√≠z de ComfyUI.
2. Clone este repositorio. `git clone https://github.com/heshengtao/comfyui_LLM_party.git`

### M√©todo tres:
1. Haga clic en `CODE` en la esquina superior derecha.
2. Haga clic en `download zip`.
3. Extraiga el archivo comprimido descargado en la subcarpeta `custom_nodes` de la carpeta ra√≠z de ComfyUI.

## Implementaci√≥n del entorno
1. Navegue hasta la carpeta del proyecto `comfyui_LLM_party`.
2. En la terminal, ingrese `pip install -r requirements.txt` para implementar las bibliotecas de terceros necesarias para este proyecto en el entorno de comfyui. Tenga en cuenta si est√° instalando en el entorno de comfyui y preste atenci√≥n a los errores de `pip` en la terminal.
3. Si est√° utilizando el lanzador de comfyui, deber√° ingresar en la terminal `ruta en la configuraci√≥n del lanzador\python_embeded\python.exe -m pip install -r requirements.txt` para realizar la instalaci√≥n. La carpeta `python_embeded` generalmente est√° al mismo nivel que su carpeta `ComfyUI`.
4. Si encuentra problemas con la configuraci√≥n del entorno, puede intentar usar las dependencias del archivo `requirements_fixed.txt`.
## Configuraci√≥n
* Puede configurar el idioma en `config.ini`, actualmente solo hay dos opciones: chino (zh_CN) e ingl√©s (en_US), siendo el predeterminado el idioma de su sistema.
* Puede utilizar uno de los siguientes m√©todos para configurar el APIKEY.
### M√©todo uno:
1. Abra el archivo `config.ini` en la carpeta del proyecto `comfyui_LLM_party`.
2. Ingrese su `openai_api_key` y `base_url` en `config.ini`.
3. Si utiliza el modelo ollama, ingrese `http://127.0.0.1:11434/v1/` en `base_url`, `ollama` en `openai_api_key`, y el nombre de su modelo en `model_name`, por ejemplo: llama3.
4. Si desea utilizar herramientas de b√∫squeda de Google o Bing, ingrese su `google_api_key`, `cse_id` o `bing_api_key` en `config.ini`.
5. Si desea utilizar entrada de im√°genes para LLM, se recomienda usar el servicio de alojamiento de im√°genes imgbb, ingresando su `imgbb_api` en `config.ini`.
6. Cada modelo se puede configurar individualmente en el archivo `config.ini`, y puede consultar el archivo `config.ini.example` para completar la configuraci√≥n. Una vez que est√© configurado, solo necesita ingresar `model_name` en el nodo.

### M√©todo dos:
1. Abra la interfaz de comfyui.
2. Cree un nuevo nodo de modelo de lenguaje grande (LLM), ingresando directamente su `openai_api_key` y `base_url` en el nodo.
3. Si utiliza el modelo ollama, utilice el nodo LLM_api, ingresando `http://127.0.0.1:11434/v1/` en `base_url`, `ollama` en `api_key`, y el nombre de su modelo en `model_name`, por ejemplo: llama3.
4. Si desea utilizar entrada de im√°genes para LLM, se recomienda usar el servicio de alojamiento de im√°genes imgbb, ingresando su `imgbb_api_key` en el nodo.
## Registro de Actualizaciones
1. Puede hacer clic derecho en la interfaz de comfyui, seleccionar `llm` en el men√∫ contextual y as√≠ encontrar√° el nodo de este proyecto. [C√≥mo usar nodos](how_to_use_nodes_ZH.md)
2. Se admite la integraci√≥n de API o la conexi√≥n de modelos grandes locales. Se ha implementado de manera modular la funcionalidad de llamada de herramientas. Al ingresar el base_url, debe ser una URL que termine en `/v1/`. Puede utilizar [ollama](https://github.com/ollama/ollama) para gestionar sus modelos; en base_url ingrese `http://127.0.0.1:11434/v1/`, en api_key ingrese ollama, y en model_name ingrese el nombre de su modelo, por ejemplo: llama3.
   - Ejemplo de flujo de trabajo de integraci√≥n de API: [start_with_LLM_api](workflow/start_with_LLM_api.json)
   - Ejemplo de flujo de trabajo de integraci√≥n de modelo local: [start_with_LLM_local](workflow/start_with_LLM_local.json)
   - Ejemplo de flujo de trabajo de integraci√≥n de ollama: [ollama](workflow/ollama.json)
3. Integraci√≥n de repositorios de conocimiento local, soporte para RAG. Ejemplo de flujo de trabajo: [B√∫squeda RAG en Repositorio](workflow/Áü•ËØÜÂ∫ìRAGÊêúÁ¥¢.json)
4. Se puede invocar un int√©rprete de c√≥digo.
5. Se permite la consulta en l√≠nea, con soporte para b√∫squeda en Google. Ejemplo de flujo de trabajo: [Flujo de Trabajo de Consulta de Pel√≠culas](workflow/ÁîµÂΩ±Êü•ËØ¢Â∑•‰ΩúÊµÅ.json)
6. Es posible implementar declaraciones condicionales en comfyui, permitiendo clasificar preguntas de los usuarios y responder de manera espec√≠fica. Ejemplo de flujo de trabajo: [Servicio al Cliente Inteligente](workflow/Êô∫ËÉΩÂÆ¢Êúç.json)
7. Soporte para enlaces de retroalimentaci√≥n de modelos grandes, lo que permite que dos modelos grandes participen en un debate. Ejemplo de flujo de trabajo: [Debate sobre el Dilema del Tranv√≠a](workflow/ÁîµËΩ¶ÈöæÈ¢òËæ©ËÆ∫Ëµõ.json)
8. Se admite la conexi√≥n de cualquier m√°scara de personalidad, permitiendo personalizar plantillas de indicaciones.
9. Soporte para diversas llamadas de herramientas, actualmente se han desarrollado funciones como consulta del clima, hora, repositorio de conocimiento, ejecuci√≥n de c√≥digo, b√∫squeda en l√≠nea y b√∫squeda en una √∫nica p√°gina web.
10. Se admite el uso de LLM como un nodo de herramienta. Ejemplo de flujo de trabajo: [LLM Matryoshka](workflow/LLMÂ•óÂ®É.json)
11. Se admite el desarrollo r√°pido de aplicaciones web propias mediante API + streamlit.
12. Se ha a√±adido un nodo de int√©rprete universal peligroso, que permite a los modelos grandes hacer cualquier cosa.
13. Se recomienda utilizar el nodo de mostrar texto (show_text) en el subdirectorio de funciones (function) del men√∫ contextual como salida del nodo LLM.
14. ¬°Se ha habilitado la funci√≥n visual de GPT-4O! Flujo de trabajo de ejemplo: [GPT-4o](workflow/GPT-4o.json)  
15. Se ha a√±adido un conector de flujo de trabajo, que permite que tu flujo de trabajo llame a otros flujos de trabajo. Flujo de trabajo de ejemplo: [Llamar a otro flujo de trabajo](workflow/Ë∞ÉÁî®Âè¶‰∏Ä‰∏™Â∑•‰ΩúÊµÅ.json)  
16. Se han adaptado todos los modelos que tienen interfaces similares a openai, como: Tongyi Qianwen/qwen, Zhipu Qingyan/GLM, deepseek, kimi/moonshot. Por favor, completa los campos base_url, api_key y model_name de los nodos LLM para poder llamarlos.  
17. Se ha a√±adido un cargador LVM, ahora puedes llamar a modelos LVM localmente, soportando el modelo [llava-llama-3-8b-v1_1-gguf](https://huggingface.co/xtuner/llava-llama-3-8b-v1_1-gguf); otros modelos LVM en formato GGUF te√≥ricamente tambi√©n deber√≠an funcionar. El flujo de trabajo de ejemplo est√° aqu√≠: [start_with_LVM.json](workflow/start_with_LVM.json).  
18. Se ha escrito un archivo `fastapi.py`, si lo ejecutas directamente, obtendr√°s una interfaz openai en `http://127.0.0.1:8817/v1/`, cualquier aplicaci√≥n que pueda llamar a GPT podr√° invocar tu flujo de trabajo de comfyui. Detallar√© c√≥mo hacerlo en un tutorial pr√≥ximo.  
19. Se ha separado el cargador LLM y la cadena LLM, dividiendo la carga del modelo y la configuraci√≥n del modelo, lo que permite compartir modelos entre diferentes nodos LLM.  
20. Actualmente se admite macOS y dispositivos mps. ¬°Gracias a [bigcat88](https://github.com/bigcat88) por esta contribuci√≥n!  
21. Ahora puedes crear tu propio juego de novela interactiva, donde las decisiones del usuario conducen a diferentes finales. Flujo de trabajo de ejemplo: [Novela interactiva](workflow/‰∫íÂä®Â∞èËØ¥.json)  
22. Se han adaptado las funciones de whisper y tts de openai, permitiendo la entrada y salida de voz. Flujo de trabajo de ejemplo: [Entrada de voz + Salida de voz](workflow/ËØ≠Èü≥ËæìÂÖ•+ËØ≠Èü≥ËæìÂá∫.json)
23. ¬°Compatibilidad con [Omost](https://github.com/lllyasviel/Omost) ya disponible! Descargue [omost-llama-3-8b-4bits](https://huggingface.co/lllyasviel/omost-llama-3-8b-4bits) y comience a experimentar de inmediato. Referencia de flujo de trabajo: [start_with_OMOST](workflow/start_with_OMOST.json)  
24. Se han a√±adido herramientas LLM para enviar mensajes a WeChat empresarial, DingTalk y Feishu, as√≠ como funciones externas que se pueden invocar.  
25. Se ha a√±adido un iterador de texto que puede emitir solo una parte de los caracteres cada vez. Este iterador divide el texto de manera segura seg√∫n el s√≠mbolo de retorno de carro y el tama√±o del fragmento, sin dividir el texto a la mitad. El par√°metro chunk_overlap indica cu√°ntos caracteres se superponen en el texto dividido. Esto permite la entrada masiva de textos extremadamente largos; solo es necesario hacer clic sin pensar o activar la ejecuci√≥n c√≠clica en ComfyUI para completarlo autom√°ticamente. Recuerde activar la propiedad is_locked para que, al finalizar la entrada, el flujo de trabajo se bloquee autom√°ticamente y no contin√∫e ejecut√°ndose. Ejemplo de flujo de trabajo: [ÊñáÊú¨Ëø≠‰ª£ËæìÂÖ•](workflow/ÊñáÊú¨Ëø≠‰ª£ËæìÂÖ•.json)  
26. Se ha a√±adido el atributo model name en el cargador local de LLM y el cargador local de llava. Si est√° vac√≠o, se utilizar√°n las diversas rutas locales en el nodo. Si no est√° vac√≠o, se cargar√° utilizando el par√°metro de ruta que usted mismo haya especificado en `config.ini`. Si no est√° vac√≠o y no se encuentra en `config.ini`, se descargar√° desde Hugging Face o se cargar√° desde el directorio de modelos guardados de Hugging Face. Si desea descargar desde Hugging Face, complete el atributo model name en el formato: `THUDM/glm-4-9b-chat`. ¬°Atenci√≥n! Los modelos cargados de esta manera deben ser compatibles con la biblioteca transformer.  
27. Se han a√±adido nodos de an√°lisis de archivos JSON y nodos de obtenci√≥n de valores JSON, que permiten obtener el valor de una clave espec√≠fica de un archivo o texto. ¬°Agradecimientos a [guobalove](https://github.com/guobalove) por su contribuci√≥n!
28. Se ha mejorado el c√≥digo para la invocaci√≥n de herramientas, de modo que ahora los LLM sin funci√≥n de llamada a herramientas tambi√©n pueden activar el atributo is_tools_in_sys_prompt (por defecto, no es necesario activarlo en LLM locales, se adapta autom√°ticamente). Una vez activado, la informaci√≥n de las herramientas se a√±adir√° al mensaje del sistema, permitiendo as√≠ que el LLM invoque herramientas. Art√≠culo relacionado sobre el principio de implementaci√≥n: [Achieving Tool Calling Functionality in LLMs Using Only Prompt Engineering Without Fine-Tuning](https://arxiv.org/abs/2407.04997).

29. Se ha creado la carpeta custom_tool para almacenar el c√≥digo de las herramientas personalizadas. Puede hacer referencia al c√≥digo en la carpeta [custom_tool](custom_tool) y colocar el c√≥digo de su herramienta personalizada en la carpeta custom_tool para que pueda ser invocada en el LLM.

30. Se ha a√±adido una herramienta de gr√°fico de conocimiento que permite una interacci√≥n perfecta entre el LLM y el gr√°fico de conocimiento. El LLM puede modificar el gr√°fico de conocimiento seg√∫n su entrada y razonar sobre √©l para obtener las respuestas que necesita. Flujo de trabajo de ejemplo: [graphRAG_neo4j](workflow/graphRAG_neo4j.json).

31. Se ha incorporado la funci√≥n de IA de personalidad, permitiendo desarrollar su propia IA de novia o novio sin necesidad de c√≥digo, con conversaciones ilimitadas, memoria permanente y una personalidad estable. Flujo de trabajo de ejemplo: [È∫¶Ê¥õËñá‰∫∫Ê†ºAI](workflow/È∫¶Ê¥õËñá‰∫∫Ê†ºAI.json).

32. Puede utilizar esta m√°quina generadora de herramientas LLM para crear autom√°ticamente herramientas LLM. Guarde el c√≥digo de las herramientas generadas como un archivo python, luego copie el c√≥digo en la carpeta custom_tool y habr√° creado un nuevo nodo. Flujo de trabajo de ejemplo: [LLMÂ∑•ÂÖ∑ÁîüÊàêÂô®](workflow/LLMÂ∑•ÂÖ∑Âà∂ÈÄ†Êú∫.json).

33. Se ha habilitado la b√∫squeda en duckduckgo, aunque con importantes limitaciones; parece que solo se pueden ingresar palabras clave en ingl√©s y no se pueden incluir m√∫ltiples conceptos. La ventaja es que no hay restricciones de API key.

34. Se ha a√±adido la funci√≥n de invocaci√≥n separada de m√∫ltiples bases de conocimiento, permitiendo especificar en el mensaje cu√°l base de conocimiento se utilizar√° para responder a las preguntas. Flujo de trabajo de ejemplo: [Â§öÁü•ËØÜÂ∫ìÂàÜÂà´Ë∞ÉÁî®](workflow/Â§öÁü•ËØÜÂ∫ìÂàÜÂà´Ë∞ÉÁî®.json).

35. Se admite la entrada de par√°metros adicionales en el LLM, incluidos par√°metros avanzados como json out. Flujo de trabajo de ejemplo: [LLMËæìÂÖ•È¢ùÂ§ñÂèÇÊï∞](workflow/LLMÈ¢ùÂ§ñÂèÇÊï∞eg_JSON_OUT.json). [Áî®json_outÂàÜÁ¶ªÊèêÁ§∫ËØç](workflow/Áî®json_outÂàÜÁ¶ªÊèêÁ§∫ËØç.json).
36. Se ha a√±adido la funci√≥n de conectar el agente a Discord. (A√∫n en fase de prueba)  
37. Se ha a√±adido la funci√≥n de conectar el agente a Feishu. Agradecemos enormemente la contribuci√≥n de [guobalove](https://github.com/guobalove). Consulte el flujo de trabajo [Robot de Feishu](workflow/È£û‰π¶Êú∫Âô®‰∫∫.json).  
38. Se ha a√±adido un nodo de llamada a API universal y numerosos nodos auxiliares para construir el cuerpo de la solicitud y extraer informaci√≥n de la respuesta.  
39. Se ha a√±adido un nodo para limpiar el modelo, que permite descargar el LLM de la memoria en cualquier momento.  
40. Se ha a√±adido el nodo [chatTTS](https://github.com/2noise/ChatTTS). Agradecemos enormemente la contribuci√≥n de [guobalove](https://github.com/guobalove). El par√°metro `model_path` puede dejarse vac√≠o. Se recomienda utilizar el modo HF para cargar el modelo, el cual se descargar√° autom√°ticamente desde Hugging Face, sin necesidad de descarga manual; si se utiliza la carga local, coloque las carpetas `asset` y `config` del modelo en el directorio ra√≠z. [Direcci√≥n de Baidu Cloud](https://pan.baidu.com/share/init?surl=T4aEB4HumdJ7iVbvsv1vzA&pwd=qyhu), c√≥digo de extracci√≥n: qyhu; si se utiliza el modo `custom` para cargar, coloque las carpetas `asset` y `config` del modelo en el directorio `model_path`.
## Plan de acci√≥n siguiente:
1. Adaptaci√≥n de m√°s modelos, cubriendo al menos las interfaces API de los grandes modelos m√°s utilizados y las llamadas locales a los modelos de c√≥digo abierto m√°s relevantes, as√≠ como m√°s adaptaciones de modelos LVM. Hasta ahora, solo he adaptado la funcionalidad visual de GPT-4.
2. M√°s formas de construir agentes inteligentes. En este √°mbito, he completado el trabajo de importar un LLM como herramienta a otro LLM, creando flujos de trabajo LLM radiales y permitiendo que un flujo de trabajo act√∫e como nodo dentro de otro flujo de trabajo. En el futuro, espero desarrollar funciones a√∫n m√°s innovadoras en este aspecto.
3. M√°s funciones de automatizaci√≥n. En el futuro, lanzar m√°s nodos que permitan la transmisi√≥n autom√°tica de im√°genes, texto, video y audio a otras aplicaciones, as√≠ como introducir nodos de escucha para lograr respuestas autom√°ticas en las principales redes sociales y foros.
4. M√°s funciones de gesti√≥n de bases de conocimiento. Actualmente, este proyecto ya soporta b√∫squedas en archivos locales y en internet. En el futuro, lanzar b√∫squedas en gr√°ficos de conocimiento y b√∫squedas de memoria a largo plazo, permitiendo que los agentes realicen un razonamiento l√≥gico sobre conocimientos especializados y mantengan en la memoria informaci√≥n clave durante las conversaciones con los usuarios.
5. M√°s herramientas y m√°s m√°scaras de personalidad. Esta √°rea es f√°cil de desarrollar, pero requiere acumulaci√≥n. Espero que en el futuro este proyecto pueda ofrecer, al igual que comfyui, una amplia variedad de nodos personalizados, herramientas y m√°scaras de personalidad.

## Aviso legal:
Este proyecto de c√≥digo abierto y su contenido (en adelante, "el proyecto") son solo para fines de referencia y no implican ninguna garant√≠a expresa o impl√≠cita. Los contribuyentes del proyecto no asumen ninguna responsabilidad por la integridad, precisi√≥n, fiabilidad o aplicabilidad del proyecto. Cualquier acci√≥n que dependa del contenido del proyecto debe asumir el riesgo por su cuenta. En ning√∫n caso, los contribuyentes del proyecto ser√°n responsables de cualquier p√©rdida o da√±o indirecto, especial o incidental que surja del uso del contenido del proyecto.
## Agradecimientos especiales
<a href="https://github.com/bigcat88">
  <img src="https://avatars.githubusercontent.com/u/13381981?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/guobalove">
  <img src="https://avatars.githubusercontent.com/u/171540731?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>
<a href="https://github.com/HuangYuChuh">
  <img src="https://avatars.githubusercontent.com/u/167663109?v=4" width="50" height="50" style="border-radius: 50%; overflow: hidden;" alt="octocat"/>
</a>

## Lista de proyectos referenciados
Algunos nodos en este proyecto se han inspirado en los siguientes proyectos. ¬°Agradecemos su contribuci√≥n a la comunidad de c√≥digo abierto!
1. [pythongosssss/ComfyUI-Custom-Scripts](https://github.com/pythongosssss/ComfyUI-Custom-Scripts)
2. [lllyasviel/Omost](https://github.com/lllyasviel/Omost)

## Soporte:

### √önete a la comunidad
Si hay problemas con el complemento o si tiene alguna otra pregunta, le damos la bienvenida a unirse a nuestra comunidad.

1. Grupo de QQ: `931057213`
<div style="display: flex; justify-content: center;">
    <img src="img/QÁæ§.jpg" style="width: 48%;" />
</div>

2. Grupo de WeChat: `Choo-Yong` (una vez que a√±ada a la asistente de WeChat, podr√° unirse al grupo)

3. Discord: [enlace de discord](https://discord.gg/gxrQAYy6)

### S√≠guenos
1. Si desea mantenerse informado sobre las √∫ltimas funciones de este proyecto, le invitamos a seguir nuestra cuenta de Bilibili: [‰∏ªÊåÅBBÊú∫](https://space.bilibili.com/26978344)
2. La cuenta de OpenArt se actualiza continuamente con los flujos de trabajo de fiesta m√°s √∫tiles: [openart](https://openart.ai/workflows/profile/comfyui_llm_party?sort=latest&tab=creation)

### Apoyo a donaciones
Si mi trabajo le ha aportado valor, ¬°considere invitarme a un caf√©! Su apoyo no solo revitaliza el proyecto, sino que tambi√©n calienta el coraz√≥n del creador.‚òïüíñ ¬°Cada taza cuenta!
<div style="display:flex; justify-content:space-between;">
    <img src="img/zhifubao.jpg" style="width: 48%;" />
    <img src="img/wechat.jpg" style="width: 48%;" />
</div>

## Historial de estrellas

[![Gr√°fico de Historial de Estrellas](https://api.star-history.com/svg?repos=heshengtao/comfyui_LLM_party&type=Date)](https://star-history.com/#heshengtao/comfyui_LLM_party&Date)
