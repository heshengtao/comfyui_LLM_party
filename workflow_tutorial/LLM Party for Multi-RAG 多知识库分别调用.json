{"last_node_id":98,"last_link_id":117,"nodes":[{"id":76,"type":"load_file","pos":[-30,300],"size":{"0":320,"1":140},"flags":{},"order":0,"mode":0,"outputs":[{"name":"file_content","type":"STRING","links":[105],"slot_index":0,"shape":3,"label":"file_content"}],"properties":{"Node name for S&R":"load_file"},"widgets_values":["","test.txt",true],"shape":1},{"id":79,"type":"advance_ebd_tool","pos":[320,310],"size":{"0":320,"1":250},"flags":{},"order":5,"mode":0,"inputs":[{"name":"ebd_model","type":"EBD_MODEL","link":null,"slot_index":0,"label":"ebd_model"},{"name":"file_content","type":"STRING","link":105,"slot_index":1,"widget":{"name":"file_content"},"label":"file_content"}],"outputs":[{"name":"tool","type":"STRING","links":[112],"slot_index":0,"shape":3,"label":"tool"}],"properties":{"Node name for S&R":"advance_ebd_tool"},"widgets_values":["/hy-tmp/AI_files/models/Embedding_Tools/ChatGLM3","enable",5,"auto",200,50,"test","",""],"shape":1},{"id":82,"type":"advance_ebd_tool","pos":[320,600],"size":{"0":320,"1":290},"flags":{},"order":6,"mode":0,"inputs":[{"name":"ebd_model","type":"EBD_MODEL","link":null,"slot_index":0,"label":"ebd_model"},{"name":"file_content","type":"STRING","link":108,"slot_index":1,"widget":{"name":"file_content"},"label":"file_content"}],"outputs":[{"name":"tool","type":"STRING","links":[113],"slot_index":0,"shape":3,"label":"tool"}],"properties":{"Node name for S&R":"advance_ebd_tool"},"widgets_values":["/hy-tmp/AI_files/models/Embedding_Tools/ChatGLM3","enable",5,"auto",200,50,"readme","",""],"shape":1},{"id":84,"type":"load_file","pos":[-30,490],"size":{"0":310,"1":130},"flags":{},"order":2,"mode":0,"outputs":[{"name":"file_content","type":"STRING","links":[108],"slot_index":0,"shape":3,"label":"file_content"}],"properties":{"Node name for S&R":"load_file"},"widgets_values":["","README_ZH.txt",true],"shape":1},{"id":87,"type":"tool_combine","pos":[670,560],"size":[310,320],"flags":{},"order":7,"mode":0,"inputs":[{"name":"tool1","type":"STRING","link":112,"widget":{"name":"tool1"},"label":"tool1"},{"name":"tool2","type":"STRING","link":113,"widget":{"name":"tool2"},"label":"tool2"},{"name":"tool3","type":"STRING","link":null,"widget":{"name":"tool3"},"label":"tool3"}],"outputs":[{"name":"tools","type":"STRING","links":[116],"slot_index":0,"shape":3,"label":"tools"}],"properties":{"Node name for S&R":"tool_combine"},"widgets_values":["","","",true],"shape":1},{"id":89,"type":"LLM","pos":[1030,300],"size":[440,590],"flags":{},"order":8,"mode":0,"inputs":[{"name":"model","type":"CUSTOM","link":115,"label":"model"},{"name":"images","type":"IMAGE","link":null,"label":"images"},{"name":"extra_parameters","type":"DICT","link":null,"label":"extra_parameters"},{"name":"system_prompt_input","type":"STRING","link":null,"widget":{"name":"system_prompt_input"},"label":"system_prompt_input"},{"name":"user_prompt_input","type":"STRING","link":null,"widget":{"name":"user_prompt_input"},"label":"user_prompt_input"},{"name":"tools","type":"STRING","link":116,"widget":{"name":"tools"},"label":"tools"},{"name":"file_content","type":"STRING","link":null,"widget":{"name":"file_content"},"label":"file_content"}],"outputs":[{"name":"assistant_response","type":"STRING","links":[117],"slot_index":0,"shape":3,"label":"assistant_response"},{"name":"history","type":"STRING","links":null,"shape":3,"label":"history"},{"name":"tool","type":"STRING","links":null,"shape":3,"label":"tool"},{"name":"image","type":"IMAGE","links":null,"shape":3,"label":"image"}],"properties":{"Node name for S&R":"LLM"},"widgets_values":["你一个强大的人工智能助手。","你好，readme中，comfyui llm party是个什么项目",0.7,"disable","enable","disable","enable",1920,"","","","","",100,"",true,true,true],"shape":1},{"id":90,"type":"LLM_api_loader","pos":[670,300],"size":{"0":320,"1":130},"flags":{},"order":1,"mode":0,"outputs":[{"name":"model","type":"CUSTOM","links":[115],"slot_index":0,"shape":3,"label":"model"}],"properties":{"Node name for S&R":"LLM_api_loader"},"widgets_values":["qwen2:latest","","",true],"shape":1},{"id":91,"type":"show_text_party","pos":[1500,310],"size":{"0":380,"1":580},"flags":{},"order":9,"mode":0,"inputs":[{"name":"text","type":"STRING","link":117,"widget":{"name":"text"},"label":"text"}],"outputs":[{"name":"STRING","type":"STRING","links":null,"shape":6,"label":"STRING"}],"properties":{"Node name for S&R":"show_text_party"},"widgets_values":["","你好！首先，你需要了解`comfyui llm party`是一个用于在WebUI平台集成LLM（大型语言模型）系统的项目。通过配置与自定义设置进行优化，它旨在让开发者快速上手，同时提高现有社区对生成文本和对话过程的参与度。\n\n该项目的功能和应用可以包括但不限于：\n1. **文本生成**：基于输入提示或提供的上下文生成与之相关的文本内容。\n2. **问答系统**：提供与复杂或专业主题相关问题的自动解答机制，为用户、开发者以及广大社区成员节省时间资源。\n3. **代码建议/修复**：在程序员开发或者调试代码的时候进行协助，可能包括自动识别语法错误、补全语句，或者重构建议等。\n4. **跨领域知识应用辅助**：不论背景、兴趣还是特定语言的知识点方面为用户提供指导和支持。\n\n`llm party`作为部分（通常指的是多个单独的LLM系统的集合）或在comfyui中的一个具体模块，则旨在通过各种方式融入现有工作流程，提供高效、智能化的数据生成和咨询模式。这不仅能够提升团队效率，还极大地丰富了跨平台的交互体验和服务性能。\n\n如果您需要深度了解项目的详细配置与调用说明，请访问原始项目文档或者开发者社区（例如GitLab, GitHub等专业网站），这些地方通常有项目主页以及相关指南和讨论区，帮助您解答操作细节问题。如果语言沟通出现困难，请尝试在同样的语境或在相关社区平台上提出您的问题。\n\n```markdown\n如果您是英文环境中使用中文提问：\n\"Hello, I need to understand what 'README.md' specifically for the project 'comfyui llm party' refers to?\"\n```\n\n### 另一个请求范例：\n\n如果有需要获取特定用户疑问的详细文件信息，可以尝试使用 `data_base_advance` 选项。例如：\n\n输入示例：\n{ \"type\": \"object\", \n  \"properties\": {\n    \"question\": { \"type\": \"string\",\"description\": `要查询的问题，“comfyui llm party”在什么背景下”，默认使用英文作为输出。}, `\"file_name\"`, `'README.md}' },\n  }\n\n需要获取与用户提问相关的、详细描述 `comfyuidocumentaryllmnt party` 文件的信息，这里可能会返回关于其背景、意图或者功能实现的相关细节概述。\n\n为了更好地向你说明：  \n- 使用问题：\"查阅或分析用户上传的问题描述文档来寻找相关于 “comfyui llmntpart”的重要信息。”\n\n请调用 `data_base_advance api`.\n\n例如:\n```\n{\"type\": \"object\",  \n  \"Properties\": {\"Question\":\"What's purpose 'comFYi Ullmntpart'? Context and implications\",\"file_name\"=README.md,\"k\":3} \n}\n```"],"shape":1},{"id":92,"type":"Note","pos":[-390,260],"size":[340,640],"flags":{},"order":4,"mode":0,"properties":{"text":""},"widgets_values":["#工作流介绍：\n通过将本地文档加载到 ComfyUI 中并结合词嵌入向量模型进行处理，LLM 能够更好地利用这些外部知识库来提供更加精确和丰富的回答。这种方式不仅扩展了 LLM 的知识覆盖面，还提升了其在处理专业化、定制化任务中的能力，特别是在安全性和保密性要求较高的场景中具有明显的优势。\n\n在实际应用中，虽然大多数大型语言模型（LLM）在训练时已经接触了大量的数据集，但由于世界上的数据量庞大且多变，LLM 并不能覆盖所有可能的知识点和背景信息。因此，在某些特定场景下，仅依靠 LLM 的内置知识库可能无法得到满意的回答。此时，结合多知识库的调用将大大提升 LLM 的回答质量，特别是在处理需要专业背景知识的任务时。\n\n在 ComfyUI 中，我们可以通过使用【加载文件】节点，将本地的文档文件加载到系统中，作为知识库或背景知识的一部分。这些文档可能包括技术文档、行业报告、公司内部的指南或任何与问题相关的参考资料。\n\n-----------------------------------------------------\n#应用场景：\n例如，在一个企业环境中，员工希望利用 LLM 来回答有关公司内部政策的问题。虽然 LLM 本身可能已经具备一些通用的政策知识，但具体到某个企业内部的政策细节，LLM 可能无法提供准确的回答。通过将企业的内部政策文件加载到 ComfyUI 中，作为知识库的一部分，LLM 就能够利用这些本地文件中包含的详细信息来生成更精准的回答。\n这种多知识库调用的方式不仅增强了 LLM 的实用性，还提供了更多的灵活性，使其能够根据实际需求动态加载和利用不同的数据源。\n\n-----------------------------------------------------\n#词嵌入向量模型的作用：\n在调用本地文档时，词嵌入向量模型扮演着至关重要的角色。当【加载文件】节点加载了本地的文档后，系统会使用高级词嵌入工具对文档内容进行向量化处理。词嵌入模型能够将文本数据转化为多维向量，这些向量能够表示文本的语义信息和上下文关系。\n\n通过这种向量化的处理，LLM 可以更好地理解和利用本地文档中的内容。词嵌入向量模型可以帮助 LLM：\n\n1. 语义匹配：通过计算问题与文档内容在向量空间上的余弦相似度，LLM 能够更准确地检索出与问题相关的内容，从而生成更加精准的回答。\n\n2. 语境理解：匹配的文本以最接近多个的文字段落的形式返回，LLM 在调用这些文字段落时，能够更好地理解文本的上下文，从而避免因为语境不清导致的回答错误。\n\n3. 处理大规模数据：面对多个知识库时，LLM可以选择性的调用其中一个知识库，提高搜索的精确性和高效性，可以将不同知识库的相关信息放在系统提示词中，用户只需要正常交互，LLM即可自动匹配到最相关的知识库并查询。\n\n------------------------------------------------------\n#写在最后：\n- LLM_Party正在用心经营一片AI时代的后花园，我们希望能够在AI时代下成为众多参与者的一员，我们从开源社区中走来，也希望回到社区中去。\n- 欢迎大家来到我们用心经营的后花园：\n- 项目地址：https://github.com/heshengtao/comfyui_LLM_party\n\n- openart：https://openart.ai/workflows/profile/comfyui_llm_party?tab=workflows&sort=latest\n\n- LibLib：https://www.liblib.art/userpage/4378612c5b3341c79c0deab3101aeabb/publish/workflow\n\n- 哔哩哔哩：https://space.bilibili.com/26978344?spm_id_from=333.337.0.0\n\n- YouTube：https://www.youtube.com/@comfyui-LLM-party\n\n- discord：https://discord.com/invite/gxrQAYy6\n\n- QQ交流群：931057213\n\n- 微信交流群：we_glm（添加小助理微信，统一通过后会添加至交流群）\n"],"color":"#432","bgcolor":"#653","shape":1},{"id":97,"type":"Note","pos":[-30,660],"size":[320,230],"flags":{},"order":3,"mode":0,"title":"Note","properties":{"text":""},"widgets_values":["#工作流参数配置：\n【加载文件】\n- absolute_path: 填写本地文件的路径。填写示例：C:\\Users\\Documents\\Pro\\myfile.txt\n\n- relative_path：将本地的文件放入ComfyUI/custom_nodes/comfyui_LLM_party/file中，重新加载ComfyUI并且刷新前端界面即可自动识别文件。\n\n- Note：absolute_path和relative_path两种加载方式选其一即可，不需要两种方式同时加载文件。\n\n------------------------------------------------\n【词嵌入模型工具/高级词嵌入工具】\n- file_content：可以输入一个字符串，该字符串会被作为词嵌入模型的输入，模型会在这个字符串上进行搜索，根据question来返回最相关的文本内容。\n\n- path：填入向量模型的本地文件位置。示例填写：hy-tmp\\AI_files\\models\\Embedding_Tools\\ChatGLM3\n\n- k：是返回的段落数量，chuck_size为文本分割时，每个文本块的大小，默认为200，chuck_overlap为文本分割时，每个文本块之间的重叠大小，默认为50。\n\n- device：一般默认状态为[auto]，自动选择你的cuda/mps/cpu设备，可根据实际情况进行调整。\n\n- chuck_size：文本分割时，每个文本块的大小，默认为200。\n\n- chuck_overlap：文本分割时，每个文本块之间的重叠大小，默认为50。\n\n- file_name：代表了知识库的名字，在与LLM交互时，可以直接通过file_name来调用对应的知识库。\n\n- base_path：本地向量数据库的加载路径，如果你输入了base_path，数据库将从已经生成过的本地向量库加载，如果没有填入，则会从file_content的内容中生成一个向量数据库。\n"],"color":"#c09430"}],"links":[[105,76,0,79,1,"STRING"],[108,84,0,82,1,"STRING"],[112,79,0,87,0,"STRING"],[113,82,0,87,1,"STRING"],[115,90,0,89,0,"CUSTOM"],[116,87,0,89,5,"STRING"],[117,89,0,91,0,"STRING"]],"groups":[{"title":"词嵌入向量模型（Advanced）","bounding":[310,230,340,670],"color":"#3f789e","font_size":24,"locked":false},{"title":"Files","bounding":[-40,230,340,670],"color":"#3f789e","font_size":24,"locked":false},{"title":"Group","bounding":[660,230,340,214],"color":"#3f789e","font_size":24,"locked":false},{"title":"LLM Party for Multi-RAG 多知识库分别调用","bounding":[-390,60,2276,158],"color":"#3f789e","font_size":118,"locked":false},{"title":"Tools Combine","bounding":[660,460,340,440],"color":"#3f789e","font_size":24,"locked":false},{"title":"LLM Apply ","bounding":[1010,230,467,674],"color":"#3f789e","font_size":24,"locked":false},{"title":"Text OutPut","bounding":[1490,230,400,670],"color":"#3f789e","font_size":24,"locked":false}],"config":{},"extra":{"ds":{"scale":0.7247295000000022,"offset":[617.4979618697474,40.07705833860196]}},"version":0.4}